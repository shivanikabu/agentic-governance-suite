This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.env.sample
.github/workflows/black.yml
.github/workflows/run_pytest.yml
.gitignore
.pre-commit-config.yaml
aisuite/__init__.py
aisuite/client.py
aisuite/framework/__init__.py
aisuite/framework/chat_completion_response.py
aisuite/framework/choice.py
aisuite/framework/message.py
aisuite/framework/provider_interface.py
aisuite/provider.py
aisuite/providers/anthropic_provider.py
aisuite/providers/aws_provider.py
aisuite/providers/azure_provider.py
aisuite/providers/cerebras_provider.py
aisuite/providers/cohere_provider.py
aisuite/providers/deepseek_provider.py
aisuite/providers/fireworks_provider.py
aisuite/providers/google_provider.py
aisuite/providers/groq_provider.py
aisuite/providers/huggingface_provider.py
aisuite/providers/message_converter.py
aisuite/providers/mistral_provider.py
aisuite/providers/nebius_provider.py
aisuite/providers/ollama_provider.py
aisuite/providers/openai_provider.py
aisuite/providers/sambanova_provider.py
aisuite/providers/together_provider.py
aisuite/providers/watsonx_provider.py
aisuite/providers/xai_provider.py
aisuite/utils/tools.py
CONTRIBUTING.md
examples/aisuite_tool_abstraction.ipynb
examples/AISuiteDemo.ipynb
examples/chat-ui/.streamlit/config.toml
examples/chat-ui/chat.py
examples/chat-ui/config.yaml
examples/chat-ui/README.md
examples/client.ipynb
examples/DeepseekPost.ipynb
examples/llm_reasoning.ipynb
examples/QnA_with_pdf.ipynb
examples/simple_tool_calling.ipynb
examples/tool_calling_abstraction.ipynb
guides/anthropic.md
guides/aws.md
guides/azure.md
guides/cerebras.md
guides/cohere.md
guides/deepseek.md
guides/google.md
guides/groq.md
guides/huggingface.md
guides/mistral.md
guides/nebius.md
guides/openai.md
guides/README.md
guides/sambanova.md
guides/watsonx.md
guides/xai.md
LICENSE
pyproject.toml
README.md
tests/client/test_client.py
tests/client/test_prerelease.py
tests/providers/test_anthropic_converter.py
tests/providers/test_aws_converter.py
tests/providers/test_azure_provider.py
tests/providers/test_cerebras_provider.py
tests/providers/test_cohere_provider.py
tests/providers/test_deepseek_provider.py
tests/providers/test_google_converter.py
tests/providers/test_google_provider.py
tests/providers/test_groq_provider.py
tests/providers/test_mistral_provider.py
tests/providers/test_nebius_provider.py
tests/providers/test_ollama_provider.py
tests/providers/test_sambanova_provider.py
tests/providers/test_watsonx_provider.py
tests/utils/test_tool_manager.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".env.sample">
# OpenAI API Key
OPENAI_API_KEY=

# Anthropic API Key
ANTHROPIC_API_KEY=

# AWS SDK credentials
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION=

# Azure
AZURE_API_KEY=

# Cerebras
CEREBRAS_API_KEY=

# Google Cloud
GOOGLE_APPLICATION_CREDENTIALS=./google-adc
GOOGLE_REGION=
GOOGLE_PROJECT_ID=

# Hugging Face token 
HF_TOKEN=

# Fireworks
FIREWORKS_API_KEY=

# Mistral
MISTRAL_API_KEY=

# Together AI
TOGETHER_API_KEY=

# WatsonX
WATSONX_SERVICE_URL=
WATSONX_API_KEY=
WATSONX_PROJECT_ID=

# xAI
XAI_API_KEY=

# Sambanova
SAMBANOVA_API_KEY=
</file>

<file path=".github/workflows/black.yml">
name: Lint

on: [push, pull_request]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: psf/black@stable
</file>

<file path=".github/workflows/run_pytest.yml">
name: Lint

on: [push, pull_request]

jobs:
  build_and_test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [ "3.10", "3.11", "3.12" ]
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install poetry
          poetry install --all-extras --with test
      - name: Test with pytest
        run: poetry run pytest -m "not integration"
</file>

<file path=".gitignore">
.idea/
.vscode/
__pycache__/
env/
.env
.google-adc
*.whl

# Testing
.coverage

# pyenv
.python-version

.DS_Store
**/.DS_Store
</file>

<file path=".pre-commit-config.yaml">
repos:
  # Using this mirror lets us use mypyc-compiled black, which is about 2x faster
  - repo: https://github.com/psf/black-pre-commit-mirror
    rev: 24.4.2
    hooks:
      - id: black
        # It is recommended to specify the latest version of Python
        # supported by your project here, or alternatively use
        # pre-commit's default_language_version, see
        # https://pre-commit.com/#top_level-default_language_version
        language_version: python3.12
</file>

<file path="aisuite/__init__.py">
from .client import Client
from .framework.message import Message
from .utils.tools import Tools
</file>

<file path="aisuite/client.py">
from .provider import ProviderFactory
import os
from .utils.tools import Tools


class Client:
    def __init__(self, provider_configs: dict = {}):
        """
        Initialize the client with provider configurations.
        Use the ProviderFactory to create provider instances.

        Args:
            provider_configs (dict): A dictionary containing provider configurations.
                Each key should be a provider string (e.g., "google" or "aws-bedrock"),
                and the value should be a dictionary of configuration options for that provider.
                For example:
                {
                    "openai": {"api_key": "your_openai_api_key"},
                    "aws-bedrock": {
                        "aws_access_key": "your_aws_access_key",
                        "aws_secret_key": "your_aws_secret_key",
                        "aws_region": "us-west-2"
                    }
                }
        """
        self.providers = {}
        self.provider_configs = provider_configs
        self._chat = None
        self._initialize_providers()

    def _initialize_providers(self):
        """Helper method to initialize or update providers."""
        for provider_key, config in self.provider_configs.items():
            provider_key = self._validate_provider_key(provider_key)
            self.providers[provider_key] = ProviderFactory.create_provider(
                provider_key, config
            )

    def _validate_provider_key(self, provider_key):
        """
        Validate if the provider key corresponds to a supported provider.
        """
        supported_providers = ProviderFactory.get_supported_providers()

        if provider_key not in supported_providers:
            raise ValueError(
                f"Invalid provider key '{provider_key}'. Supported providers: {supported_providers}. "
                "Make sure the model string is formatted correctly as 'provider:model'."
            )

        return provider_key

    def configure(self, provider_configs: dict = None):
        """
        Configure the client with provider configurations.
        """
        if provider_configs is None:
            return

        self.provider_configs.update(provider_configs)
        self._initialize_providers()  # NOTE: This will override existing provider instances.

    @property
    def chat(self):
        """Return the chat API interface."""
        if not self._chat:
            self._chat = Chat(self)
        return self._chat


class Chat:
    def __init__(self, client: "Client"):
        self.client = client
        self._completions = Completions(self.client)

    @property
    def completions(self):
        """Return the completions interface."""
        return self._completions


class Completions:
    def __init__(self, client: "Client"):
        self.client = client

    def _extract_thinking_content(self, response):
        """
        Extract content between <think> tags if present and store it in reasoning_content.

        Args:
            response: The response object from the provider

        Returns:
            Modified response object
        """
        if hasattr(response, "choices") and response.choices:
            message = response.choices[0].message
            if hasattr(message, "content") and message.content:
                content = message.content.strip()
                if content.startswith("<think>") and "</think>" in content:
                    # Extract content between think tags
                    start_idx = len("<think>")
                    end_idx = content.find("</think>")
                    thinking_content = content[start_idx:end_idx].strip()

                    # Store the thinking content
                    message.reasoning_content = thinking_content

                    # Remove the think tags from the original content
                    message.content = content[end_idx + len("</think>") :].strip()

        return response

    def _tool_runner(
        self,
        provider,
        model_name: str,
        messages: list,
        tools: any,
        max_turns: int,
        **kwargs,
    ):
        """
        Handle tool execution loop for max_turns iterations.

        Args:
            provider: The provider instance to use for completions
            model_name: Name of the model to use
            messages: List of conversation messages
            tools: Tools instance or list of callable tools
            max_turns: Maximum number of tool execution turns
            **kwargs: Additional arguments to pass to the provider

        Returns:
            The final response from the model with intermediate responses and messages
        """
        # Handle tools validation and conversion
        if isinstance(tools, Tools):
            tools_instance = tools
            kwargs["tools"] = tools_instance.tools()
        else:
            # Check if passed tools are callable
            if not all(callable(tool) for tool in tools):
                raise ValueError("One or more tools is not callable")
            tools_instance = Tools(tools)
            kwargs["tools"] = tools_instance.tools()

        turns = 0
        intermediate_responses = []  # Store intermediate responses
        intermediate_messages = []  # Store all messages including tool interactions

        while turns < max_turns:
            # Make the API call
            response = provider.chat_completions_create(model_name, messages, **kwargs)
            response = self._extract_thinking_content(response)

            # Store intermediate response
            intermediate_responses.append(response)

            # Check if there are tool calls in the response
            tool_calls = (
                getattr(response.choices[0].message, "tool_calls", None)
                if hasattr(response, "choices")
                else None
            )

            # Store the model's message
            intermediate_messages.append(response.choices[0].message)

            if not tool_calls:
                # Set the intermediate data in the final response
                response.intermediate_responses = intermediate_responses[
                    :-1
                ]  # Exclude final response
                response.choices[0].intermediate_messages = intermediate_messages
                return response

            # Execute tools and get results
            results, tool_messages = tools_instance.execute_tool(tool_calls)

            # Add tool messages to intermediate messages
            intermediate_messages.extend(tool_messages)

            # Add the assistant's response and tool results to messages
            messages.extend([response.choices[0].message, *tool_messages])

            turns += 1

        # Set the intermediate data in the final response
        response.intermediate_responses = intermediate_responses[
            :-1
        ]  # Exclude final response
        response.choices[0].intermediate_messages = intermediate_messages
        return response

    def create(self, model: str, messages: list, **kwargs):
        """
        Create chat completion based on the model, messages, and any extra arguments.
        Supports automatic tool execution when max_turns is specified.
        """
        # Check that correct format is used
        if ":" not in model:
            raise ValueError(
                f"Invalid model format. Expected 'provider:model', got '{model}'"
            )

        # Extract the provider key from the model identifier, e.g., "google:gemini-xx"
        provider_key, model_name = model.split(":", 1)

        # Validate if the provider is supported
        supported_providers = ProviderFactory.get_supported_providers()
        if provider_key not in supported_providers:
            raise ValueError(
                f"Invalid provider key '{provider_key}'. Supported providers: {supported_providers}. "
                "Make sure the model string is formatted correctly as 'provider:model'."
            )

        # Initialize provider if not already initialized
        if provider_key not in self.client.providers:
            config = self.client.provider_configs.get(provider_key, {})
            self.client.providers[provider_key] = ProviderFactory.create_provider(
                provider_key, config
            )

        provider = self.client.providers.get(provider_key)
        if not provider:
            raise ValueError(f"Could not load provider for '{provider_key}'.")

        # Extract tool-related parameters
        max_turns = kwargs.pop("max_turns", None)
        tools = kwargs.get("tools", None)

        # Check environment variable before allowing multi-turn tool execution
        if max_turns is not None and tools is not None:
            return self._tool_runner(
                provider,
                model_name,
                messages.copy(),
                tools,
                max_turns,
            )

        # Default behavior without tool execution
        # Delegate the chat completion to the correct provider's implementation
        response = provider.chat_completions_create(model_name, messages, **kwargs)
        return self._extract_thinking_content(response)
</file>

<file path="aisuite/framework/__init__.py">
from .provider_interface import ProviderInterface
from .chat_completion_response import ChatCompletionResponse
from .message import Message
</file>

<file path="aisuite/framework/chat_completion_response.py">
from aisuite.framework.choice import Choice


class ChatCompletionResponse:
    """Used to conform to the response model of OpenAI"""

    def __init__(self):
        self.choices = [Choice()]  # Adjust the range as needed for more choices
</file>

<file path="aisuite/framework/choice.py">
from aisuite.framework.message import Message
from typing import Literal, Optional, List


class Choice:
    def __init__(self):
        self.finish_reason: Optional[Literal["stop", "tool_calls"]] = None
        self.message = Message(
            content=None,
            tool_calls=None,
            role="assistant",
            refusal=None,
            reasoning_content=None,
        )
        self.intermediate_messages: List[Message] = []
</file>

<file path="aisuite/framework/message.py">
"""Interface to hold contents of api responses when they do not confirm to the OpenAI style response"""

from pydantic import BaseModel
from typing import Literal, Optional, List


class Function(BaseModel):
    arguments: str
    name: str


class ChatCompletionMessageToolCall(BaseModel):
    id: str
    function: Function
    type: Literal["function"]


class Message(BaseModel):
    content: Optional[str] = None
    reasoning_content: Optional[str] = None
    tool_calls: Optional[List[ChatCompletionMessageToolCall]] = None
    role: Optional[Literal["user", "assistant", "system", "tool"]] = None
    refusal: Optional[str] = None
</file>

<file path="aisuite/framework/provider_interface.py">
"""The shared interface for model providers."""


# TODO(rohit): Remove this. This interface is obsolete in favor of Provider.
class ProviderInterface:
    """Defines the expected behavior for provider-specific interfaces."""

    def chat_completion_create(self, messages=None, model=None, temperature=0) -> None:
        """Create a chat completion using the specified messages, model, and temperature.

        This method must be implemented by subclasses to perform completions.

        Args:
        ----
            messages (list): The chat history.
            model (str): The identifier of the model to be used in the completion.
            temperature (float): The temperature to use in the completion.

        Raises:
        ------
            NotImplementedError: If this method has not been implemented by a subclass.

        """
        raise NotImplementedError(
            "Provider Interface has not implemented chat_completion_create()"
        )
</file>

<file path="aisuite/provider.py">
from abc import ABC, abstractmethod
from pathlib import Path
import importlib
import os
import functools


class LLMError(Exception):
    """Custom exception for LLM errors."""

    def __init__(self, message):
        super().__init__(message)


class Provider(ABC):
    @abstractmethod
    def chat_completions_create(self, model, messages):
        """Abstract method for chat completion calls, to be implemented by each provider."""
        pass


class ProviderFactory:
    """Factory to dynamically load provider instances based on naming conventions."""

    PROVIDERS_DIR = Path(__file__).parent / "providers"

    @classmethod
    def create_provider(cls, provider_key, config):
        """Dynamically load and create an instance of a provider based on the naming convention."""
        # Convert provider_key to the expected module and class names
        provider_class_name = f"{provider_key.capitalize()}Provider"
        provider_module_name = f"{provider_key}_provider"

        module_path = f"aisuite.providers.{provider_module_name}"

        # Lazily load the module
        try:
            module = importlib.import_module(module_path)
        except ImportError as e:
            raise ImportError(
                f"Could not import module {module_path}: {str(e)}. Please ensure the provider is supported by doing ProviderFactory.get_supported_providers()"
            )

        # Instantiate the provider class
        provider_class = getattr(module, provider_class_name)
        return provider_class(**config)

    @classmethod
    @functools.cache
    def get_supported_providers(cls):
        """List all supported provider names based on files present in the providers directory."""
        provider_files = Path(cls.PROVIDERS_DIR).glob("*_provider.py")
        return {file.stem.replace("_provider", "") for file in provider_files}
</file>

<file path="aisuite/providers/anthropic_provider.py">
# Anthropic provider
# Links:
# Tool calling docs - https://docs.anthropic.com/en/docs/build-with-claude/tool-use

import anthropic
import json
from aisuite.provider import Provider
from aisuite.framework import ChatCompletionResponse
from aisuite.framework.message import Message, ChatCompletionMessageToolCall, Function

# Define a constant for the default max_tokens value
DEFAULT_MAX_TOKENS = 4096


class AnthropicMessageConverter:
    # Role constants
    ROLE_USER = "user"
    ROLE_ASSISTANT = "assistant"
    ROLE_TOOL = "tool"
    ROLE_SYSTEM = "system"

    # Finish reason mapping
    FINISH_REASON_MAPPING = {
        "end_turn": "stop",
        "max_tokens": "length",
        "tool_use": "tool_calls",
    }

    def convert_request(self, messages):
        """Convert framework messages to Anthropic format."""
        system_message = self._extract_system_message(messages)
        converted_messages = [self._convert_single_message(msg) for msg in messages]
        return system_message, converted_messages

    def convert_response(self, response):
        """Normalize the response from the Anthropic API to match OpenAI's response format."""
        normalized_response = ChatCompletionResponse()
        normalized_response.choices[0].finish_reason = self._get_finish_reason(response)
        normalized_response.usage = self._get_usage_stats(response)
        normalized_response.choices[0].message = self._get_message(response)
        return normalized_response

    def _convert_single_message(self, msg):
        """Convert a single message to Anthropic format."""
        if isinstance(msg, dict):
            return self._convert_dict_message(msg)
        return self._convert_message_object(msg)

    def _convert_dict_message(self, msg):
        """Convert a dictionary message to Anthropic format."""
        if msg["role"] == self.ROLE_TOOL:
            return self._create_tool_result_message(msg["tool_call_id"], msg["content"])
        elif msg["role"] == self.ROLE_ASSISTANT and "tool_calls" in msg:
            return self._create_assistant_tool_message(
                msg["content"], msg["tool_calls"]
            )
        return {"role": msg["role"], "content": msg["content"]}

    def _convert_message_object(self, msg):
        """Convert a Message object to Anthropic format."""
        if msg.role == self.ROLE_TOOL:
            return self._create_tool_result_message(msg.tool_call_id, msg.content)
        elif msg.role == self.ROLE_ASSISTANT and msg.tool_calls:
            return self._create_assistant_tool_message(msg.content, msg.tool_calls)
        return {"role": msg.role, "content": msg.content}

    def _create_tool_result_message(self, tool_call_id, content):
        """Create a tool result message in Anthropic format."""
        return {
            "role": self.ROLE_USER,
            "content": [
                {
                    "type": "tool_result",
                    "tool_use_id": tool_call_id,
                    "content": content,
                }
            ],
        }

    def _create_assistant_tool_message(self, content, tool_calls):
        """Create an assistant message with tool calls in Anthropic format."""
        message_content = []
        if content:
            message_content.append({"type": "text", "text": content})

        for tool_call in tool_calls:
            tool_input = (
                tool_call["function"]["arguments"]
                if isinstance(tool_call, dict)
                else tool_call.function.arguments
            )
            message_content.append(
                {
                    "type": "tool_use",
                    "id": (
                        tool_call["id"] if isinstance(tool_call, dict) else tool_call.id
                    ),
                    "name": (
                        tool_call["function"]["name"]
                        if isinstance(tool_call, dict)
                        else tool_call.function.name
                    ),
                    "input": json.loads(tool_input),
                }
            )

        return {"role": self.ROLE_ASSISTANT, "content": message_content}

    def _extract_system_message(self, messages):
        """Extract system message if present, otherwise return empty list."""
        # TODO: This is a temporary solution to extract the system message.
        # User can pass multiple system messages, which can mingled with other messages.
        # This needs to be fixed to handle this case.
        if messages and messages[0]["role"] == "system":
            system_message = messages[0]["content"]
            messages.pop(0)
            return system_message
        return []

    def _get_finish_reason(self, response):
        """Get the normalized finish reason."""
        return self.FINISH_REASON_MAPPING.get(response.stop_reason, "stop")

    def _get_usage_stats(self, response):
        """Get the usage statistics."""
        return {
            "prompt_tokens": response.usage.input_tokens,
            "completion_tokens": response.usage.output_tokens,
            "total_tokens": response.usage.input_tokens + response.usage.output_tokens,
        }

    def _get_message(self, response):
        """Get the appropriate message based on response type."""
        if response.stop_reason == "tool_use":
            tool_message = self.convert_response_with_tool_use(response)
            if tool_message:
                return tool_message

        return Message(
            content=response.content[0].text,
            role="assistant",
            tool_calls=None,
            refusal=None,
        )

    def convert_response_with_tool_use(self, response):
        """Convert Anthropic tool use response to the framework's format."""
        tool_call = next(
            (content for content in response.content if content.type == "tool_use"),
            None,
        )

        if tool_call:
            function = Function(
                name=tool_call.name, arguments=json.dumps(tool_call.input)
            )
            tool_call_obj = ChatCompletionMessageToolCall(
                id=tool_call.id, function=function, type="function"
            )
            text_content = next(
                (
                    content.text
                    for content in response.content
                    if content.type == "text"
                ),
                "",
            )

            return Message(
                content=text_content or None,
                tool_calls=[tool_call_obj] if tool_call else None,
                role="assistant",
                refusal=None,
            )
        return None

    def convert_tool_spec(self, openai_tools):
        """Convert OpenAI tool specification to Anthropic format."""
        anthropic_tools = []

        for tool in openai_tools:
            if tool.get("type") != "function":
                continue

            function = tool["function"]
            anthropic_tool = {
                "name": function["name"],
                "description": function["description"],
                "input_schema": {
                    "type": "object",
                    "properties": function["parameters"]["properties"],
                    "required": function["parameters"].get("required", []),
                },
            }
            anthropic_tools.append(anthropic_tool)

        return anthropic_tools


class AnthropicProvider(Provider):
    def __init__(self, **config):
        """Initialize the Anthropic provider with the given configuration."""
        self.client = anthropic.Anthropic(**config)
        self.converter = AnthropicMessageConverter()

    def chat_completions_create(self, model, messages, **kwargs):
        """Create a chat completion using the Anthropic API."""
        kwargs = self._prepare_kwargs(kwargs)
        system_message, converted_messages = self.converter.convert_request(messages)

        response = self.client.messages.create(
            model=model, system=system_message, messages=converted_messages, **kwargs
        )
        return self.converter.convert_response(response)

    def _prepare_kwargs(self, kwargs):
        """Prepare kwargs for the API call."""
        kwargs = kwargs.copy()
        kwargs.setdefault("max_tokens", DEFAULT_MAX_TOKENS)

        if "tools" in kwargs:
            kwargs["tools"] = self.converter.convert_tool_spec(kwargs["tools"])

        return kwargs
</file>

<file path="aisuite/providers/aws_provider.py">
import os
import json
from typing import List, Dict, Any, Tuple, Optional

import boto3
from aisuite.provider import Provider, LLMError
from aisuite.framework import ChatCompletionResponse
from aisuite.framework.message import Message
import botocore


class BedrockConfig:
    INFERENCE_PARAMETERS = ["maxTokens", "temperature", "topP", "stopSequences"]

    def __init__(self, **config):
        self.region_name = config.get(
            "region_name", os.getenv("AWS_REGION", "us-west-2")
        )

    def create_client(self):
        return boto3.client("bedrock-runtime", region_name=self.region_name)


# AWS Bedrock API Example -
# https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use-inference-call.html
# https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use-examples.html
class BedrockMessageConverter:
    @staticmethod
    def convert_request(
        messages: List[Dict[str, Any]],
    ) -> Tuple[List[Dict], List[Dict]]:
        """Convert messages to AWS Bedrock format."""
        # Convert all messages to dicts if they're Message objects
        messages = [
            message.model_dump() if hasattr(message, "model_dump") else message
            for message in messages
        ]

        # Handle system message
        system_message = []
        if messages and messages[0]["role"] == "system":
            system_message = [{"text": messages[0]["content"]}]
            messages = messages[1:]

        formatted_messages = []
        for message in messages:
            # Skip any additional system messages
            if message["role"] == "system":
                continue

            if message["role"] == "tool":
                bedrock_message = BedrockMessageConverter.convert_tool_result(message)
                if bedrock_message:
                    formatted_messages.append(bedrock_message)
            elif message["role"] == "assistant":
                bedrock_message = BedrockMessageConverter.convert_assistant(message)
                if bedrock_message:
                    formatted_messages.append(bedrock_message)
            else:  # user messages
                formatted_messages.append(
                    {
                        "role": message["role"],
                        "content": [{"text": message["content"]}],
                    }
                )

        return system_message, formatted_messages

    @staticmethod
    def convert_response_tool_call(
        response: Dict[str, Any],
    ) -> Optional[Dict[str, Any]]:
        """Convert AWS Bedrock tool call response to OpenAI format."""
        if response.get("stopReason") != "tool_use":
            return None

        tool_calls = []
        for content in response["output"]["message"]["content"]:
            if "toolUse" in content:
                tool = content["toolUse"]
                tool_calls.append(
                    {
                        "type": "function",
                        "id": tool["toolUseId"],
                        "function": {
                            "name": tool["name"],
                            "arguments": json.dumps(tool["input"]),
                        },
                    }
                )

        if not tool_calls:
            return None

        return {
            "role": "assistant",
            "content": None,
            "tool_calls": tool_calls,
            "refusal": None,
        }

    @staticmethod
    def convert_tool_result(message: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Convert OpenAI tool result format to AWS Bedrock format."""
        if message["role"] != "tool" or "content" not in message:
            return None

        tool_call_id = message.get("tool_call_id")
        if not tool_call_id:
            raise LLMError("Tool result message must include tool_call_id")

        try:
            content_json = json.loads(message["content"])
            content = [{"json": content_json}]
        except json.JSONDecodeError:
            content = [{"text": message["content"]}]

        return {
            "role": "user",
            "content": [
                {"toolResult": {"toolUseId": tool_call_id, "content": content}}
            ],
        }

    @staticmethod
    def convert_assistant(message: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Convert OpenAI assistant format to AWS Bedrock format."""
        if message["role"] != "assistant":
            return None

        content = []

        if message.get("content"):
            content.append({"text": message["content"]})

        if message.get("tool_calls"):
            for tool_call in message["tool_calls"]:
                if tool_call["type"] == "function":
                    try:
                        input_json = json.loads(tool_call["function"]["arguments"])
                    except json.JSONDecodeError:
                        input_json = tool_call["function"]["arguments"]

                    content.append(
                        {
                            "toolUse": {
                                "toolUseId": tool_call["id"],
                                "name": tool_call["function"]["name"],
                                "input": input_json,
                            }
                        }
                    )

        return {"role": "assistant", "content": content} if content else None

    @staticmethod
    def convert_response(response: Dict[str, Any]) -> ChatCompletionResponse:
        """Normalize the response from the Bedrock API to match OpenAI's response format."""
        norm_response = ChatCompletionResponse()

        # Check if the model is requesting tool use
        if response.get("stopReason") == "tool_use":
            tool_message = BedrockMessageConverter.convert_response_tool_call(response)
            if tool_message:
                norm_response.choices[0].message = Message(**tool_message)
                norm_response.choices[0].finish_reason = "tool_calls"
                return norm_response

        # Handle regular text response
        norm_response.choices[0].message.content = response["output"]["message"][
            "content"
        ][0]["text"]

        # Map Bedrock stopReason to OpenAI finish_reason
        stop_reason = response.get("stopReason")
        if stop_reason == "complete":
            norm_response.choices[0].finish_reason = "stop"
        elif stop_reason == "max_tokens":
            norm_response.choices[0].finish_reason = "length"
        else:
            norm_response.choices[0].finish_reason = stop_reason

        return norm_response


class AwsProvider(Provider):
    def __init__(self, **config):
        """Initialize the AWS Bedrock provider with the given configuration."""
        self.config = BedrockConfig(**config)
        self.client = self.config.create_client()
        self.transformer = BedrockMessageConverter()

    def convert_response(self, response: Dict[str, Any]) -> ChatCompletionResponse:
        """Normalize the response from the Bedrock API to match OpenAI's response format."""
        return self.transformer.convert_response(response)

    def _convert_tool_spec(self, kwargs: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Convert tool specifications to Bedrock format."""
        if "tools" not in kwargs:
            return None

        tool_config = {
            "tools": [
                {
                    "toolSpec": {
                        "name": tool["function"]["name"],
                        "description": tool["function"].get("description", " "),
                        "inputSchema": {"json": tool["function"]["parameters"]},
                    }
                }
                for tool in kwargs["tools"]
            ]
        }
        return tool_config

    def _prepare_request_config(self, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare the configuration for the Bedrock API request."""
        # Convert tools and remove from kwargs
        tool_config = self._convert_tool_spec(kwargs)
        kwargs.pop("tools", None)  # Remove tools from kwargs if present

        inference_config = {
            key: kwargs[key]
            for key in BedrockConfig.INFERENCE_PARAMETERS
            if key in kwargs
        }

        additional_fields = {
            key: value
            for key, value in kwargs.items()
            if key not in BedrockConfig.INFERENCE_PARAMETERS
        }

        request_config = {
            "inferenceConfig": inference_config,
            "additionalModelRequestFields": additional_fields,
        }

        if tool_config is not None:
            request_config["toolConfig"] = tool_config

        return request_config

    def chat_completions_create(
        self, model: str, messages: List[Dict[str, Any]], **kwargs
    ) -> ChatCompletionResponse:
        """Create a chat completion request to AWS Bedrock."""
        system_message, formatted_messages = self.transformer.convert_request(messages)
        request_config = self._prepare_request_config(kwargs)

        try:
            response = self.client.converse(
                modelId=model,
                messages=formatted_messages,
                system=system_message,
                **request_config
            )
        except botocore.exceptions.ClientError as e:
            if e.response["Error"]["Code"] == "ValidationException":
                error_message = e.response["Error"]["Message"]
                raise LLMError(error_message)
            else:
                raise

        return self.convert_response(response)
</file>

<file path="aisuite/providers/azure_provider.py">
import urllib.request
import json
import os

from aisuite.provider import Provider
from aisuite.framework import ChatCompletionResponse
from aisuite.framework.message import Message, ChatCompletionMessageToolCall, Function

# Azure provider is based on the documentation here -
# https://learn.microsoft.com/en-us/azure/machine-learning/reference-model-inference-api?view=azureml-api-2&source=recommendations&tabs=python
# Azure AI Model Inference API is used.
# From the documentation -
# """
# The Azure AI Model Inference is an API that exposes a common set of capabilities for foundational models
# and that can be used by developers to consume predictions from a diverse set of models in a uniform and consistent way.
# Developers can talk with different models deployed in Azure AI Foundry portal without changing the underlying code they are using.
#
# The Azure AI Model Inference API is available in the following models:
#
# Models deployed to serverless API endpoints:
#   Cohere Embed V3 family of models
#   Cohere Command R family of models
#   Meta Llama 2 chat family of models
#   Meta Llama 3 instruct family of models
#   Mistral-Small
#   Mistral-Large
#   Jais family of models
#   Jamba family of models
#   Phi-3 family of models
#
# Models deployed to managed inference:
#   Meta Llama 3 instruct family of models
#   Phi-3 family of models
#   Mixtral famility of models
#
# The API is compatible with Azure OpenAI model deployments.
# """


class AzureMessageConverter:
    @staticmethod
    def convert_request(messages):
        """Convert messages to Azure format."""
        transformed_messages = []
        for message in messages:
            if isinstance(message, Message):
                transformed_messages.append(message.model_dump(mode="json"))
            else:
                transformed_messages.append(message)
        return transformed_messages

    @staticmethod
    def convert_response(resp_json) -> ChatCompletionResponse:
        """Normalize the response from the Azure API to match OpenAI's response format."""
        completion_response = ChatCompletionResponse()
        choice = resp_json["choices"][0]
        message = choice["message"]

        # Set basic message content
        completion_response.choices[0].message.content = message.get("content")
        completion_response.choices[0].message.role = message.get("role", "assistant")

        # Handle tool calls if present
        if "tool_calls" in message and message["tool_calls"] is not None:
            tool_calls = []
            for tool_call in message["tool_calls"]:
                new_tool_call = ChatCompletionMessageToolCall(
                    id=tool_call["id"],
                    type=tool_call["type"],
                    function={
                        "name": tool_call["function"]["name"],
                        "arguments": tool_call["function"]["arguments"],
                    },
                )
                tool_calls.append(new_tool_call)
            completion_response.choices[0].message.tool_calls = tool_calls

        return completion_response


class AzureProvider(Provider):
    def __init__(self, **config):
        self.base_url = config.get("base_url") or os.getenv("AZURE_BASE_URL")
        self.api_key = config.get("api_key") or os.getenv("AZURE_API_KEY")
        self.api_version = config.get("api_version") or os.getenv("AZURE_API_VERSION")
        if not self.api_key:
            raise ValueError("For Azure, api_key is required.")
        if not self.base_url:
            raise ValueError(
                "For Azure, base_url is required. Check your deployment page for a URL like this - https://<model-deployment-name>.<region>.models.ai.azure.com"
            )
        self.transformer = AzureMessageConverter()

    def chat_completions_create(self, model, messages, **kwargs):
        url = f"{self.base_url}/chat/completions"

        if self.api_version:
            url = f"{url}?api-version={self.api_version}"

        # Remove 'stream' from kwargs if present
        kwargs.pop("stream", None)

        # Transform messages using converter
        transformed_messages = self.transformer.convert_request(messages)

        # Prepare the request payload
        data = {"messages": transformed_messages}

        # Add tools if provided
        if "tools" in kwargs:
            data["tools"] = kwargs["tools"]
            kwargs.pop("tools")

        # Add tool_choice if provided
        if "tool_choice" in kwargs:
            data["tool_choice"] = kwargs["tool_choice"]
            kwargs.pop("tool_choice")

        # Add remaining kwargs
        data.update(kwargs)

        body = json.dumps(data).encode("utf-8")
        headers = {"Content-Type": "application/json", "Authorization": self.api_key}

        try:
            req = urllib.request.Request(url, body, headers)
            with urllib.request.urlopen(req) as response:
                result = response.read()
                resp_json = json.loads(result)
                return self.transformer.convert_response(resp_json)

        except urllib.error.HTTPError as error:
            error_message = f"The request failed with status code: {error.code}\n"
            error_message += f"Headers: {error.info()}\n"
            error_message += error.read().decode("utf-8", "ignore")
            raise Exception(error_message)
</file>

<file path="aisuite/providers/cerebras_provider.py">
import os
import cerebras.cloud.sdk as cerebras
from aisuite.provider import Provider, LLMError
from aisuite.providers.message_converter import OpenAICompliantMessageConverter


class CerebrasMessageConverter(OpenAICompliantMessageConverter):
    """
    Cerebras-specific message converter if needed.
    """

    pass


class CerebrasProvider(Provider):
    def __init__(self, **config):
        self.client = cerebras.Cerebras(**config)
        self.transformer = CerebrasMessageConverter()

    def chat_completions_create(self, model, messages, **kwargs):
        """
        Makes a request to the Cerebras chat completions endpoint using the official client.
        """
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                **kwargs,  # Pass any additional arguments to the Cerebras API.
            )
            return self.transformer.convert_response(response.model_dump())

        # Re-raise Cerebras API-specific exceptions.
        except cerebras.cloud.sdk.PermissionDeniedError as e:
            raise
        except cerebras.cloud.sdk.AuthenticationError as e:
            raise
        except cerebras.cloud.sdk.RateLimitError as e:
            raise

        # Wrap all other exceptions in LLMError.
        except Exception as e:
            raise LLMError(f"An error occurred: {e}")
</file>

<file path="aisuite/providers/cohere_provider.py">
import os
import cohere
import json
from aisuite.framework import ChatCompletionResponse
from aisuite.framework.message import Message, ChatCompletionMessageToolCall, Function
from aisuite.provider import Provider, LLMError


class CohereMessageConverter:
    """
    Cohere-specific message converter
    """

    def convert_request(self, messages):
        """Convert framework messages to Cohere format."""
        converted_messages = []

        for message in messages:
            if isinstance(message, dict):
                role = message.get("role")
                content = message.get("content")
                tool_calls = message.get("tool_calls")
                tool_plan = message.get("tool_plan")
            else:
                role = message.role
                content = message.content
                tool_calls = message.tool_calls
                tool_plan = getattr(message, "tool_plan", None)

            # Convert to Cohere's format
            if role == "tool":
                # Handle tool response messages
                converted_message = {
                    "role": role,
                    "tool_call_id": (
                        message.get("tool_call_id")
                        if isinstance(message, dict)
                        else message.tool_call_id
                    ),
                    "content": self._convert_tool_content(content),
                }
            elif role == "assistant" and tool_calls:
                # Handle assistant messages with tool calls
                converted_message = {
                    "role": role,
                    "tool_calls": [
                        {
                            "id": tc.id if not isinstance(tc, dict) else tc["id"],
                            "function": {
                                "name": (
                                    tc.function.name
                                    if not isinstance(tc, dict)
                                    else tc["function"]["name"]
                                ),
                                "arguments": (
                                    tc.function.arguments
                                    if not isinstance(tc, dict)
                                    else tc["function"]["arguments"]
                                ),
                            },
                            "type": "function",
                        }
                        for tc in tool_calls
                    ],
                    "tool_plan": tool_plan,
                }
                if content:
                    converted_message["content"] = content
            else:
                # Handle regular messages
                converted_message = {"role": role, "content": content}

            converted_messages.append(converted_message)

        return converted_messages

    def _convert_tool_content(self, content):
        """Convert tool response content to Cohere's expected format."""
        if isinstance(content, str):
            try:
                # Try to parse as JSON first
                data = json.loads(content)
                return [{"type": "document", "document": {"data": json.dumps(data)}}]
            except json.JSONDecodeError:
                # If not JSON, return as plain text
                return content
        elif isinstance(content, list):
            # If content is already in Cohere's format, return as is
            return content
        else:
            # For other types, convert to string
            return str(content)

    @staticmethod
    def convert_response(response_data) -> ChatCompletionResponse:
        """Convert Cohere's response to our standard format."""
        normalized_response = ChatCompletionResponse()

        # Set usage information
        normalized_response.usage = {
            "prompt_tokens": response_data.usage.tokens.input_tokens,
            "completion_tokens": response_data.usage.tokens.output_tokens,
            "total_tokens": response_data.usage.tokens.input_tokens
            + response_data.usage.tokens.output_tokens,
        }

        # Handle tool calls
        if response_data.finish_reason == "TOOL_CALL":
            tool_call = response_data.message.tool_calls[0]
            function = Function(
                name=tool_call.function.name, arguments=tool_call.function.arguments
            )
            tool_call_obj = ChatCompletionMessageToolCall(
                id=tool_call.id, function=function, type="function"
            )
            normalized_response.choices[0].message = Message(
                content=response_data.message.tool_plan,  # Use tool_plan as content
                tool_calls=[tool_call_obj],
                role="assistant",
                refusal=None,
            )
            normalized_response.choices[0].finish_reason = "tool_calls"
        else:
            # Handle regular text response
            normalized_response.choices[0].message.content = (
                response_data.message.content[0].text
            )
            normalized_response.choices[0].finish_reason = "stop"

        return normalized_response


class CohereProvider(Provider):
    def __init__(self, **config):
        """
        Initialize the Cohere provider with the given configuration.
        Pass the entire configuration dictionary to the Cohere client constructor.
        """
        # Ensure API key is provided either in config or via environment variable
        config.setdefault("api_key", os.getenv("CO_API_KEY"))
        if not config["api_key"]:
            raise ValueError(
                "Cohere API key is missing. Please provide it in the config or set the CO_API_KEY environment variable."
            )
        self.client = cohere.ClientV2(**config)
        self.transformer = CohereMessageConverter()

    def chat_completions_create(self, model, messages, **kwargs):
        """
        Makes a request to Cohere using the official client.
        """
        try:
            # Transform messages using converter
            transformed_messages = self.transformer.convert_request(messages)

            # Make the request to Cohere
            response = self.client.chat(
                model=model, messages=transformed_messages, **kwargs
            )

            return self.transformer.convert_response(response)
        except Exception as e:
            raise LLMError(f"An error occurred: {e}")
</file>

<file path="aisuite/providers/deepseek_provider.py">
import openai
import os
from aisuite.provider import Provider, LLMError


class DeepseekProvider(Provider):
    def __init__(self, **config):
        """
        Initialize the DeepSeek provider with the given configuration.
        Pass the entire configuration dictionary to the OpenAI client constructor.
        """
        # Ensure API key is provided either in config or via environment variable
        config.setdefault("api_key", os.getenv("DEEPSEEK_API_KEY"))
        if not config["api_key"]:
            raise ValueError(
                "DeepSeek API key is missing. Please provide it in the config or set the OPENAI_API_KEY environment variable."
            )
        config["base_url"] = "https://api.deepseek.com"

        # NOTE: We could choose to remove above lines for api_key since OpenAI will automatically
        # infer certain values from the environment variables.
        # Eg: OPENAI_API_KEY, OPENAI_ORG_ID, OPENAI_PROJECT_ID. Except for OPEN_AI_BASE_URL which has to be the deepseek url

        # Pass the entire config to the OpenAI client constructor
        self.client = openai.OpenAI(**config)

    def chat_completions_create(self, model, messages, **kwargs):
        # Any exception raised by OpenAI will be returned to the caller.
        # Maybe we should catch them and raise a custom LLMError.
        response = self.client.chat.completions.create(
            model=model,
            messages=messages,
            **kwargs  # Pass any additional arguments to the OpenAI API
        )
        return response
</file>

<file path="aisuite/providers/fireworks_provider.py">
import os
import httpx
import json
from aisuite.provider import Provider, LLMError
from aisuite.framework import ChatCompletionResponse
from aisuite.framework.message import Message, ChatCompletionMessageToolCall


class FireworksMessageConverter:
    @staticmethod
    def convert_request(messages):
        """Convert messages to Fireworks format."""
        transformed_messages = []
        for message in messages:
            if isinstance(message, Message):
                message_dict = message.model_dump(mode="json")
                message_dict.pop("refusal", None)  # Remove refusal field if present
                transformed_messages.append(message_dict)
            else:
                transformed_messages.append(message)
        return transformed_messages

    @staticmethod
    def convert_response(resp_json) -> ChatCompletionResponse:
        """Normalize the response from the Fireworks API to match OpenAI's response format."""
        completion_response = ChatCompletionResponse()
        choice = resp_json["choices"][0]
        message = choice["message"]

        # Set basic message content
        completion_response.choices[0].message.content = message.get("content")
        completion_response.choices[0].message.role = message.get("role", "assistant")

        # Handle tool calls if present
        if "tool_calls" in message and message["tool_calls"] is not None:
            tool_calls = []
            for tool_call in message["tool_calls"]:
                new_tool_call = ChatCompletionMessageToolCall(
                    id=tool_call["id"],
                    type=tool_call["type"],
                    function={
                        "name": tool_call["function"]["name"],
                        "arguments": tool_call["function"]["arguments"],
                    },
                )
                tool_calls.append(new_tool_call)
            completion_response.choices[0].message.tool_calls = tool_calls

        return completion_response


# Models that support tool calls:
# [As of 01/20/2025 from https://docs.fireworks.ai/guides/function-calling]
# Llama 3.1 405B Instruct
# Llama 3.1 70B Instruct
# Qwen 2.5 72B Instruct
# Mixtral MoE 8x22B Instruct
# Firefunction-v2: Latest and most performant model, optimized for complex function calling scenarios (on-demand only)
# Firefunction-v1: Previous generation, Mixtral-based function calling model optimized for fast routing and structured output (on-demand only)
class FireworksProvider(Provider):
    """
    Fireworks AI Provider using httpx for direct API calls.
    """

    BASE_URL = "https://api.fireworks.ai/inference/v1/chat/completions"

    def __init__(self, **config):
        """
        Initialize the Fireworks provider with the given configuration.
        The API key is fetched from the config or environment variables.
        """
        self.api_key = config.get("api_key", os.getenv("FIREWORKS_API_KEY"))
        if not self.api_key:
            raise ValueError(
                "Fireworks API key is missing. Please provide it in the config or set the FIREWORKS_API_KEY environment variable."
            )

        # Optionally set a custom timeout (default to 30s)
        self.timeout = config.get("timeout", 30)
        self.transformer = FireworksMessageConverter()

    def chat_completions_create(self, model, messages, **kwargs):
        """
        Makes a request to the Fireworks AI chat completions endpoint using httpx.
        """
        # Remove 'stream' from kwargs if present
        kwargs.pop("stream", None)

        # Transform messages using converter
        transformed_messages = self.transformer.convert_request(messages)

        # Prepare the request payload
        data = {
            "model": model,
            "messages": transformed_messages,
        }

        # Add tools if provided
        if "tools" in kwargs:
            data["tools"] = kwargs["tools"]
            kwargs.pop("tools")

        # Add tool_choice if provided
        if "tool_choice" in kwargs:
            data["tool_choice"] = kwargs["tool_choice"]
            kwargs.pop("tool_choice")

        # Add remaining kwargs
        data.update(kwargs)

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        try:
            # Make the request to Fireworks AI endpoint.
            response = httpx.post(
                self.BASE_URL, json=data, headers=headers, timeout=self.timeout
            )
            response.raise_for_status()
            return self.transformer.convert_response(response.json())
        except httpx.HTTPStatusError as error:
            error_message = (
                f"The request failed with status code: {error.status_code}\n"
            )
            error_message += f"Headers: {error.headers}\n"
            error_message += error.response.text
            raise LLMError(error_message)
        except Exception as e:
            raise LLMError(f"An error occurred: {e}")

    def _normalize_response(self, response_data):
        """
        Normalize the response to a common format (ChatCompletionResponse).
        """
        normalized_response = ChatCompletionResponse()
        normalized_response.choices[0].message.content = response_data["choices"][0][
            "message"
        ]["content"]
        return normalized_response
</file>

<file path="aisuite/providers/google_provider.py">
"""The interface to Google's Vertex AI."""

import os
import json
from typing import List, Dict, Any, Optional

import vertexai
from vertexai.generative_models import (
    GenerativeModel,
    GenerationConfig,
    Content,
    Part,
    Tool,
    FunctionDeclaration,
)
import pprint

from aisuite.framework import ProviderInterface, ChatCompletionResponse, Message


DEFAULT_TEMPERATURE = 0.7
ENABLE_DEBUG_MESSAGES = False

# Links.
# https://codelabs.developers.google.com/codelabs/gemini-function-calling#6
# https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling#chat-samples


class GoogleMessageConverter:
    @staticmethod
    def convert_user_role_message(message: Dict[str, Any]) -> Content:
        """Convert user or system messages to Google Vertex AI format."""
        parts = [Part.from_text(message["content"])]
        return Content(role="user", parts=parts)

    @staticmethod
    def convert_assistant_role_message(message: Dict[str, Any]) -> Content:
        """Convert assistant messages to Google Vertex AI format."""
        if "tool_calls" in message and message["tool_calls"]:
            # Handle function calls
            tool_call = message["tool_calls"][
                0
            ]  # Assuming single function call for now
            function_call = tool_call["function"]

            # Create a Part from the function call
            parts = [
                Part.from_dict(
                    {
                        "function_call": {
                            "name": function_call["name"],
                            # "arguments": json.loads(function_call["arguments"])
                        }
                    }
                )
            ]
            # return Content(role="function", parts=parts)
        else:
            # Handle regular text messages
            parts = [Part.from_text(message["content"])]
            # return Content(role="model", parts=parts)

        return Content(role="model", parts=parts)

    @staticmethod
    def convert_tool_role_message(message: Dict[str, Any]) -> Part:
        """Convert tool messages to Google Vertex AI format."""
        if "content" not in message:
            raise ValueError("Tool result message must have a content field")

        try:
            content_json = json.loads(message["content"])
            part = Part.from_function_response(
                name=message["name"], response=content_json
            )
            # TODO: Return Content instead of Part. But returning Content is not working.
            return part
        except json.JSONDecodeError:
            raise ValueError("Tool result message must be valid JSON")

    @staticmethod
    def convert_request(messages: List[Dict[str, Any]]) -> List[Content]:
        """Convert messages to Google Vertex AI format."""
        # Convert all messages to dicts if they're Message objects
        messages = [
            message.model_dump() if hasattr(message, "model_dump") else message
            for message in messages
        ]

        formatted_messages = []
        for message in messages:
            if message["role"] == "tool":
                vertex_message = GoogleMessageConverter.convert_tool_role_message(
                    message
                )
                if vertex_message:
                    formatted_messages.append(vertex_message)
            elif message["role"] == "assistant":
                formatted_messages.append(
                    GoogleMessageConverter.convert_assistant_role_message(message)
                )
            else:  # user or system role
                formatted_messages.append(
                    GoogleMessageConverter.convert_user_role_message(message)
                )

        return formatted_messages

    @staticmethod
    def convert_response(response) -> ChatCompletionResponse:
        """Normalize the response from Vertex AI to match OpenAI's response format."""
        openai_response = ChatCompletionResponse()

        if ENABLE_DEBUG_MESSAGES:
            print("Dumping the response")
            pprint.pprint(response)

        # TODO: We need to go through each part, because function call may not be the first part.
        #       Currently, we are only handling the first part, but this is not enough.
        #
        # This is a valid response:
        # candidates {
        #   content {
        #     role: "model"
        #     parts {
        #       text: "The current temperature in San Francisco is 72 degrees Celsius. \n\n"
        #     }
        #     parts {
        #       function_call {
        #         name: "is_it_raining"
        #         args {
        #           fields {
        #             key: "location"
        #             value {
        #               string_value: "San Francisco"
        #             }
        #           }
        #         }
        #       }
        #     }
        #   }
        #   finish_reason: STOP

        # Check if the response contains function calls
        # Note: Just checking if the function_call attribute exists is not enough,
        #       it is important to check if the function_call is not None.
        if (
            hasattr(response.candidates[0].content.parts[0], "function_call")
            and response.candidates[0].content.parts[0].function_call
        ):
            function_call = response.candidates[0].content.parts[0].function_call

            # args is a MapComposite.
            # Convert the MapComposite to a dictionary
            args_dict = {}
            # Another way to try is: args_dict = dict(function_call.args)
            for key, value in function_call.args.items():
                args_dict[key] = value
            if ENABLE_DEBUG_MESSAGES:
                print("Dumping the args_dict")
                pprint.pprint(args_dict)

            openai_response.choices[0].message = {
                "role": "assistant",
                "content": None,
                "tool_calls": [
                    {
                        "type": "function",
                        "id": f"call_{hash(function_call.name)}",  # Generate a unique ID
                        "function": {
                            "name": function_call.name,
                            "arguments": json.dumps(args_dict),
                        },
                    }
                ],
                "refusal": None,
            }
            openai_response.choices[0].message = Message(
                **openai_response.choices[0].message
            )
            openai_response.choices[0].finish_reason = "tool_calls"
        else:
            # Handle regular text response
            openai_response.choices[0].message.content = (
                response.candidates[0].content.parts[0].text
            )
            openai_response.choices[0].finish_reason = "stop"

        return openai_response


class GoogleProvider(ProviderInterface):
    """Implements the ProviderInterface for interacting with Google's Vertex AI."""

    def __init__(self, **config):
        """Set up the Google AI client with a project ID."""
        self.project_id = config.get("project_id") or os.getenv("GOOGLE_PROJECT_ID")
        self.location = config.get("region") or os.getenv("GOOGLE_REGION")
        self.app_creds_path = config.get("application_credentials") or os.getenv(
            "GOOGLE_APPLICATION_CREDENTIALS"
        )

        if not self.project_id or not self.location or not self.app_creds_path:
            raise EnvironmentError(
                "Missing one or more required Google environment variables: "
                "GOOGLE_PROJECT_ID, GOOGLE_REGION, GOOGLE_APPLICATION_CREDENTIALS. "
                "Please refer to the setup guide: /guides/google.md."
            )

        vertexai.init(project=self.project_id, location=self.location)

        self.transformer = GoogleMessageConverter()

    def chat_completions_create(self, model, messages, **kwargs):
        """Request chat completions from the Google AI API.

        Args:
        ----
            model (str): Identifies the specific provider/model to use.
            messages (list of dict): A list of message objects in chat history.
            kwargs (dict): Optional arguments for the Google AI API.

        Returns:
        -------
            The ChatCompletionResponse with the completion result.

        """

        # Set the temperature if provided, otherwise use the default
        temperature = kwargs.get("temperature", DEFAULT_TEMPERATURE)

        # Convert messages to Vertex AI format
        message_history = self.transformer.convert_request(messages)

        # Handle tools if provided
        tools = None
        if "tools" in kwargs:
            tools = [
                Tool(
                    function_declarations=[
                        FunctionDeclaration(
                            name=tool["function"]["name"],
                            description=tool["function"].get("description", ""),
                            parameters={
                                "type": "object",
                                "properties": {
                                    param_name: {
                                        "type": param_info.get("type", "string"),
                                        "description": param_info.get(
                                            "description", ""
                                        ),
                                        **(
                                            {"enum": param_info["enum"]}
                                            if "enum" in param_info
                                            else {}
                                        ),
                                    }
                                    for param_name, param_info in tool["function"][
                                        "parameters"
                                    ]["properties"].items()
                                },
                                "required": tool["function"]["parameters"].get(
                                    "required", []
                                ),
                            },
                        )
                        for tool in kwargs["tools"]
                    ]
                )
            ]

        # Create the GenerativeModel
        model = GenerativeModel(
            model,
            generation_config=GenerationConfig(temperature=temperature),
            tools=tools,
        )

        if ENABLE_DEBUG_MESSAGES:
            print("Dumping the message_history")
            pprint.pprint(message_history)

        # Start chat and get response
        chat = model.start_chat(history=message_history[:-1])
        last_message = message_history[-1]

        # If the last message is a function response, send the Part object directly
        # Otherwise, send just the text content
        message_to_send = (
            Content(role="function", parts=[last_message])
            if isinstance(last_message, Part)
            else last_message.parts[0].text
        )
        # response = chat.send_message(message_to_send)
        response = chat.send_message(message_to_send)

        # Convert and return the response
        return self.transformer.convert_response(response)
</file>

<file path="aisuite/providers/groq_provider.py">
import os
import groq
from aisuite.provider import Provider, LLMError
from aisuite.providers.message_converter import OpenAICompliantMessageConverter

# Implementation of Groq provider.
# Groq's message format is same as OpenAI's.
# Tool calling specification is also exactly the same as OpenAI's.
# Links:
# https://console.groq.com/docs/tool-use
# Groq supports tool calling for the following models, as of 16th Nov 2024:
#   llama3-groq-70b-8192-tool-use-preview
#   llama3-groq-8b-8192-tool-use-preview
#   llama-3.1-70b-versatile
#   llama-3.1-8b-instant
#   llama3-70b-8192
#   llama3-8b-8192
#   mixtral-8x7b-32768 (parallel tool use not supported)
#   gemma-7b-it (parallel tool use not supported)
#   gemma2-9b-it (parallel tool use not supported)


class GroqMessageConverter(OpenAICompliantMessageConverter):
    """
    Groq-specific message converter if needed
    """

    pass


class GroqProvider(Provider):
    def __init__(self, **config):
        """
        Initialize the Groq provider with the given configuration.
        Pass the entire configuration dictionary to the Groq client constructor.
        """
        # Ensure API key is provided either in config or via environment variable
        self.api_key = config.get("api_key", os.getenv("GROQ_API_KEY"))
        if not self.api_key:
            raise ValueError(
                "Groq API key is missing. Please provide it in the config or set the GROQ_API_KEY environment variable."
            )
        config["api_key"] = self.api_key
        self.client = groq.Groq(**config)
        self.transformer = GroqMessageConverter()

    def chat_completions_create(self, model, messages, **kwargs):
        """
        Makes a request to the Groq chat completions endpoint using the official client.
        """
        try:
            # Transform messages using converter
            transformed_messages = self.transformer.convert_request(messages)

            response = self.client.chat.completions.create(
                model=model,
                messages=transformed_messages,
                **kwargs,  # Pass any additional arguments to the Groq API
            )
            return self.transformer.convert_response(response.model_dump())
        except Exception as e:
            raise LLMError(f"An error occurred: {e}")
</file>

<file path="aisuite/providers/huggingface_provider.py">
import os
import json
from huggingface_hub import InferenceClient
from aisuite.provider import Provider, LLMError
from aisuite.framework import ChatCompletionResponse
from aisuite.framework.message import Message


class HuggingfaceProvider(Provider):
    """
    HuggingFace Provider using the official InferenceClient.
    This provider supports calls to HF serverless Inference Endpoints
    which use Text Generation Inference (TGI) as the backend.
    TGI is OpenAI protocol compliant.
    https://huggingface.co/inference-endpoints/
    """

    def __init__(self, **config):
        """
        Initialize the provider with the given configuration.
        The token is fetched from the config or environment variables.
        """
        # Ensure API key is provided either in config or via environment variable
        self.token = config.get("token") or os.getenv("HF_TOKEN")
        if not self.token:
            raise ValueError(
                "Hugging Face token is missing. Please provide it in the config or set the HF_TOKEN environment variable."
            )

        # Initialize the InferenceClient with the specified model and timeout if provided
        self.model = config.get("model")
        self.timeout = config.get("timeout", 30)
        self.client = InferenceClient(
            token=self.token, model=self.model, timeout=self.timeout
        )

    def chat_completions_create(self, model, messages, **kwargs):
        """
        Makes a request to the Inference API endpoint using InferenceClient.
        """
        # Validate and transform messages
        transformed_messages = []
        for message in messages:
            if isinstance(message, Message):
                transformed_message = self.transform_from_message(message)
            elif isinstance(message, dict):
                transformed_message = message
            else:
                raise ValueError(f"Invalid message format: {message}")

            # Ensure 'content' is a non-empty string
            if (
                "content" not in transformed_message
                or transformed_message["content"] is None
            ):
                transformed_message["content"] = ""

            transformed_messages.append(transformed_message)

        try:
            # Prepare the payload
            payload = {
                "messages": transformed_messages,
                **kwargs,  # Include other parameters like temperature, max_tokens, etc.
            }

            # Make the API call using the client
            response = self.client.chat_completion(model=model, **payload)

            return self._normalize_response(response)

        except Exception as e:
            raise LLMError(f"An error occurred: {e}")

    def transform_from_message(self, message: Message):
        """Transform framework Message to a format that HuggingFace understands."""
        # Ensure content is a string
        content = message.content if message.content is not None else ""

        # Transform the message
        transformed_message = {
            "role": message.role,
            "content": content,
        }

        # Include tool_calls if present
        if message.tool_calls:
            transformed_message["tool_calls"] = [
                {
                    "id": tool_call.id,
                    "function": {
                        "name": tool_call.function.name,
                        "arguments": tool_call.function.arguments,
                    },
                    "type": tool_call.type,
                }
                for tool_call in message.tool_calls
            ]

        return transformed_message

    def transform_to_message(self, message_dict: dict):
        """Transform HuggingFace message (dict) to a format that the framework Message understands."""
        # Ensure required fields are present
        message_dict.setdefault("content", "")  # Set empty string if content is missing
        message_dict.setdefault("refusal", None)  # Set None if refusal is missing
        message_dict.setdefault("tool_calls", None)  # Set None if tool_calls is missing

        # Handle tool calls if present and not None
        if message_dict.get("tool_calls"):
            for tool_call in message_dict["tool_calls"]:
                if "function" in tool_call:
                    # Ensure function arguments are stringified
                    if isinstance(tool_call["function"].get("arguments"), dict):
                        tool_call["function"]["arguments"] = json.dumps(
                            tool_call["function"]["arguments"]
                        )

        return Message(**message_dict)

    def _normalize_response(self, response_data):
        """
        Normalize the response to a common format (ChatCompletionResponse).
        """
        normalized_response = ChatCompletionResponse()
        message_data = response_data["choices"][0]["message"]
        normalized_response.choices[0].message = self.transform_to_message(message_data)
        return normalized_response
</file>

<file path="aisuite/providers/message_converter.py">
from aisuite.framework import ChatCompletionResponse
from aisuite.framework.message import Message, ChatCompletionMessageToolCall


class OpenAICompliantMessageConverter:
    """
    Base class for message converters that are compatible with OpenAI's API.
    """

    # Class variable that derived classes can override
    tool_results_as_strings = False

    @staticmethod
    def convert_request(messages):
        """Convert messages to OpenAI-compatible format."""
        transformed_messages = []
        for message in messages:
            tmsg = None
            if isinstance(message, Message):
                message_dict = message.model_dump(mode="json")
                message_dict.pop("refusal", None)  # Remove refusal field if present
                tmsg = message_dict
            else:
                tmsg = message
            # Check if tmsg is a dict, otherwise get role attribute
            role = tmsg["role"] if isinstance(tmsg, dict) else tmsg.role
            if role == "tool":
                if OpenAICompliantMessageConverter.tool_results_as_strings:
                    # Handle both dict and object cases for content
                    if isinstance(tmsg, dict):
                        tmsg["content"] = str(tmsg["content"])
                    else:
                        tmsg.content = str(tmsg.content)

            transformed_messages.append(tmsg)
        return transformed_messages

    @staticmethod
    def convert_response(response_data) -> ChatCompletionResponse:
        """Normalize the response to match OpenAI's response format."""
        completion_response = ChatCompletionResponse()
        choice = response_data["choices"][0]
        message = choice["message"]

        # Set basic message content
        completion_response.choices[0].message.content = message["content"]
        completion_response.choices[0].message.role = message.get("role", "assistant")

        # Handle tool calls if present
        if "tool_calls" in message and message["tool_calls"] is not None:
            tool_calls = []
            for tool_call in message["tool_calls"]:
                tool_calls.append(
                    ChatCompletionMessageToolCall(
                        id=tool_call.get("id"),
                        type="function",  # Always set to "function" as it's the only valid value
                        function=tool_call.get("function"),
                    )
                )
            completion_response.choices[0].message.tool_calls = tool_calls

        return completion_response
</file>

<file path="aisuite/providers/mistral_provider.py">
import os
from mistralai import Mistral
from aisuite.framework.message import Message
from aisuite.framework import ChatCompletionResponse
from aisuite.provider import Provider, LLMError
from aisuite.providers.message_converter import OpenAICompliantMessageConverter


# Implementation of Mistral provider.
# Mistral's message format is same as OpenAI's. Just different class names, but fully cross-compatible.
# Links:
# https://docs.mistral.ai/capabilities/function_calling/


class MistralMessageConverter(OpenAICompliantMessageConverter):
    """
    Mistral-specific message converter
    """

    @staticmethod
    def convert_response(response_data) -> ChatCompletionResponse:
        """Convert Mistral's response to our standard format."""
        # Convert Mistral's response object to dict format
        response_dict = response_data.model_dump()
        return super(MistralMessageConverter, MistralMessageConverter).convert_response(
            response_dict
        )


# Function calling is available for the following models:
# [As of 01/19/2025 from https://docs.mistral.ai/capabilities/function_calling/]
# Mistral Large
# Mistral Small
# Codestral 22B
# Ministral 8B
# Ministral 3B
# Pixtral 12B
# Mixtral 8x22B
# Mistral Nemo
class MistralProvider(Provider):
    """
    Mistral AI Provider using the official Mistral client.
    """

    def __init__(self, **config):
        """
        Initialize the Mistral provider with the given configuration.
        Pass the entire configuration dictionary to the Mistral client constructor.
        """
        # Ensure API key is provided either in config or via environment variable
        config.setdefault("api_key", os.getenv("MISTRAL_API_KEY"))
        if not config["api_key"]:
            raise ValueError(
                "Mistral API key is missing. Please provide it in the config or set the MISTRAL_API_KEY environment variable."
            )
        self.client = Mistral(**config)
        self.transformer = MistralMessageConverter()

    def chat_completions_create(self, model, messages, **kwargs):
        """
        Makes a request to Mistral using the official client.
        """
        try:
            # Transform messages using converter
            transformed_messages = self.transformer.convert_request(messages)

            # Make the request to Mistral
            response = self.client.chat.complete(
                model=model, messages=transformed_messages, **kwargs
            )

            return self.transformer.convert_response(response)
        except Exception as e:
            raise LLMError(f"An error occurred: {e}")
</file>

<file path="aisuite/providers/nebius_provider.py">
import os
from aisuite.provider import Provider
from openai import Client


BASE_URL = "https://api.studio.nebius.ai/v1"


# TODO(rohitcp): This needs to be added to our internal testbed. Tool calling not tested.
class NebiusProvider(Provider):
    def __init__(self, **config):
        """
        Initialize the Nebius AI Studio provider with the given configuration.
        Pass the entire configuration dictionary to the OpenAI client constructor.
        """
        # Ensure API key is provided either in config or via environment variable
        config.setdefault("api_key", os.getenv("NEBIUS_API_KEY"))
        if not config["api_key"]:
            raise ValueError(
                "Nebius AI Studio API key is missing. Please provide it in the config or set the NEBIUS_API_KEY environment variable. You can get your API key at https://studio.nebius.ai/settings/api-keys"
            )

        config["base_url"] = BASE_URL
        # Pass the entire config to the OpenAI client constructor
        self.client = Client(**config)

    def chat_completions_create(self, model, messages, **kwargs):
        return self.client.chat.completions.create(
            model=model,
            messages=messages,
            **kwargs  # Pass any additional arguments to the Nebius API
        )
</file>

<file path="aisuite/providers/ollama_provider.py">
import os
import httpx
from aisuite.provider import Provider, LLMError
from aisuite.framework import ChatCompletionResponse


class OllamaProvider(Provider):
    """
    Ollama Provider that makes HTTP calls instead of using SDK.
    It uses the /api/chat endpoint.
    Read more here - https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion
    If OLLAMA_API_URL is not set and not passed in config, then it will default to "http://localhost:11434"
    """

    _CHAT_COMPLETION_ENDPOINT = "/api/chat"
    _CONNECT_ERROR_MESSAGE = "Ollama is likely not running. Start Ollama by running `ollama serve` on your host."

    def __init__(self, **config):
        """
        Initialize the Ollama provider with the given configuration.
        """
        self.url = config.get("api_url") or os.getenv(
            "OLLAMA_API_URL", "http://localhost:11434"
        )

        # Optionally set a custom timeout (default to 30s)
        self.timeout = config.get("timeout", 30)

    def chat_completions_create(self, model, messages, **kwargs):
        """
        Makes a request to the chat completions endpoint using httpx.
        """
        kwargs["stream"] = False
        data = {
            "model": model,
            "messages": messages,
            **kwargs,  # Pass any additional arguments to the API
        }

        try:
            response = httpx.post(
                self.url.rstrip("/") + self._CHAT_COMPLETION_ENDPOINT,
                json=data,
                timeout=self.timeout,
            )
            response.raise_for_status()
        except httpx.ConnectError:  # Handle connection errors
            raise LLMError(f"Connection failed: {self._CONNECT_ERROR_MESSAGE}")
        except httpx.HTTPStatusError as http_err:
            raise LLMError(f"Ollama request failed: {http_err}")
        except Exception as e:
            raise LLMError(f"An error occurred: {e}")

        # Return the normalized response
        return self._normalize_response(response.json())

    def _normalize_response(self, response_data):
        """
        Normalize the API response to a common format (ChatCompletionResponse).
        """
        normalized_response = ChatCompletionResponse()
        normalized_response.choices[0].message.content = response_data["message"][
            "content"
        ]
        return normalized_response
</file>

<file path="aisuite/providers/openai_provider.py">
import openai
import os
from aisuite.provider import Provider, LLMError
from aisuite.providers.message_converter import OpenAICompliantMessageConverter


class OpenaiProvider(Provider):
    def __init__(self, **config):
        """
        Initialize the OpenAI provider with the given configuration.
        Pass the entire configuration dictionary to the OpenAI client constructor.
        """
        # Ensure API key is provided either in config or via environment variable
        config.setdefault("api_key", os.getenv("OPENAI_API_KEY"))
        if not config["api_key"]:
            raise ValueError(
                "OpenAI API key is missing. Please provide it in the config or set the OPENAI_API_KEY environment variable."
            )

        # NOTE: We could choose to remove above lines for api_key since OpenAI will automatically
        # infer certain values from the environment variables.
        # Eg: OPENAI_API_KEY, OPENAI_ORG_ID, OPENAI_PROJECT_ID, OPENAI_BASE_URL, etc.

        # Pass the entire config to the OpenAI client constructor
        self.client = openai.OpenAI(**config)
        self.transformer = OpenAICompliantMessageConverter()

    def chat_completions_create(self, model, messages, **kwargs):
        # Any exception raised by OpenAI will be returned to the caller.
        # Maybe we should catch them and raise a custom LLMError.
        try:
            transformed_messages = self.transformer.convert_request(messages)
            response = self.client.chat.completions.create(
                model=model,
                messages=transformed_messages,
                **kwargs,  # Pass any additional arguments to the OpenAI API
            )
            return response
        except Exception as e:
            raise LLMError(f"An error occurred: {e}")
</file>

<file path="aisuite/providers/sambanova_provider.py">
import os
from aisuite.provider import Provider, LLMError
from openai import OpenAI
from aisuite.providers.message_converter import OpenAICompliantMessageConverter


class SambanovaMessageConverter(OpenAICompliantMessageConverter):
    """
    SambaNova-specific message converter.
    """

    pass


class SambanovaProvider(Provider):
    """
    SambaNova Provider using OpenAI client for API calls.
    """

    def __init__(self, **config):
        """
        Initialize the SambaNova provider with the given configuration.
        Pass the entire configuration dictionary to the OpenAI client constructor.
        """
        # Ensure API key is provided either in config or via environment variable
        self.api_key = config.get("api_key", os.getenv("SAMBANOVA_API_KEY"))
        if not self.api_key:
            raise ValueError(
                "Sambanova API key is missing. Please provide it in the config or set the SAMBANOVA_API_KEY environment variable."
            )

        config["api_key"] = self.api_key
        config["base_url"] = "https://api.sambanova.ai/v1/"
        # Pass the entire config to the OpenAI client constructor
        self.client = OpenAI(**config)
        self.transformer = SambanovaMessageConverter()

    def chat_completions_create(self, model, messages, **kwargs):
        """
        Makes a request to the SambaNova chat completions endpoint using the OpenAI client.
        """
        try:
            # Transform messages using converter
            transformed_messages = self.transformer.convert_request(messages)

            response = self.client.chat.completions.create(
                model=model,
                messages=transformed_messages,
                **kwargs,  # Pass any additional arguments to the Sambanova API
            )
            return self.transformer.convert_response(response.model_dump())
        except Exception as e:
            raise LLMError(f"An error occurred: {e}")
</file>

<file path="aisuite/providers/together_provider.py">
import os
import httpx
from aisuite.provider import Provider, LLMError
from aisuite.providers.message_converter import OpenAICompliantMessageConverter


class TogetherMessageConverter(OpenAICompliantMessageConverter):
    """
    Together-specific message converter if needed
    """

    pass


class TogetherProvider(Provider):
    """
    Together AI Provider using httpx for direct API calls.
    """

    BASE_URL = "https://api.together.xyz/v1/chat/completions"

    def __init__(self, **config):
        """
        Initialize the Together provider with the given configuration.
        The API key is fetched from the config or environment variables.
        """
        self.api_key = config.get("api_key", os.getenv("TOGETHER_API_KEY"))
        if not self.api_key:
            raise ValueError(
                "Together API key is missing. Please provide it in the config or set the TOGETHER_API_KEY environment variable."
            )

        # Optionally set a custom timeout (default to 30s)
        self.timeout = config.get("timeout", 30)
        self.transformer = TogetherMessageConverter()

    def chat_completions_create(self, model, messages, **kwargs):
        """
        Makes a request to the Together AI chat completions endpoint using httpx.
        """
        # Transform messages using converter
        transformed_messages = self.transformer.convert_request(messages)

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        data = {
            "model": model,
            "messages": transformed_messages,
            **kwargs,  # Pass any additional arguments to the API
        }

        try:
            # Make the request to Together AI endpoint.
            response = httpx.post(
                self.BASE_URL, json=data, headers=headers, timeout=self.timeout
            )
            response.raise_for_status()
            return self.transformer.convert_response(response.json())
        except httpx.HTTPStatusError as http_err:
            raise LLMError(f"Together AI request failed: {http_err}")
        except Exception as e:
            raise LLMError(f"An error occurred: {e}")
</file>

<file path="aisuite/providers/watsonx_provider.py">
from aisuite.provider import Provider
import os
from ibm_watsonx_ai import Credentials
from ibm_watsonx_ai.foundation_models import ModelInference
from aisuite.framework import ChatCompletionResponse


class WatsonxProvider(Provider):
    def __init__(self, **config):
        self.service_url = config.get("service_url") or os.getenv("WATSONX_SERVICE_URL")
        self.api_key = config.get("api_key") or os.getenv("WATSONX_API_KEY")
        self.project_id = config.get("project_id") or os.getenv("WATSONX_PROJECT_ID")

        if not self.service_url or not self.api_key or not self.project_id:
            raise EnvironmentError(
                "Missing one or more required WatsonX environment variables: "
                "WATSONX_SERVICE_URL, WATSONX_API_KEY, WATSONX_PROJECT_ID. "
                "Please refer to the setup guide: /guides/watsonx.md."
            )

    def chat_completions_create(self, model, messages, **kwargs):
        model = ModelInference(
            model_id=model,
            credentials=Credentials(
                api_key=self.api_key,
                url=self.service_url,
            ),
            project_id=self.project_id,
        )

        res = model.chat(messages=messages, params=kwargs)
        return self.normalize_response(res)

    def normalize_response(self, response):
        openai_response = ChatCompletionResponse()
        openai_response.choices[0].message.content = response["choices"][0]["message"][
            "content"
        ]
        return openai_response
</file>

<file path="aisuite/providers/xai_provider.py">
import os
import httpx
from aisuite.provider import Provider, LLMError
from aisuite.framework import ChatCompletionResponse
from aisuite.providers.message_converter import OpenAICompliantMessageConverter


class XaiMessageConverter(OpenAICompliantMessageConverter):
    """
    xAI-specific message converter if needed
    """

    pass


class XaiProvider(Provider):
    """
    xAI Provider using httpx for direct API calls.
    """

    BASE_URL = "https://api.x.ai/v1/chat/completions"

    def __init__(self, **config):
        """
        Initialize the xAI provider with the given configuration.
        The API key is fetched from the config or environment variables.
        """
        self.api_key = config.get("api_key", os.getenv("XAI_API_KEY"))
        if not self.api_key:
            raise ValueError(
                "xAI API key is missing. Please provide it in the config or set the XAI_API_KEY environment variable."
            )

        # Optionally set a custom timeout (default to 30s)
        self.timeout = config.get("timeout", 30)
        self.transformer = XaiMessageConverter()

    def chat_completions_create(self, model, messages, **kwargs):
        """
        Makes a request to the xAI chat completions endpoint using httpx.
        """
        # Transform messages using converter
        transformed_messages = self.transformer.convert_request(messages)

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        data = {
            "model": model,
            "messages": transformed_messages,
            **kwargs,  # Pass any additional arguments to the API
        }

        try:
            # Make the request to xAI endpoint.
            response = httpx.post(
                self.BASE_URL, json=data, headers=headers, timeout=self.timeout
            )
            response.raise_for_status()
            return self.transformer.convert_response(response.json())
        except httpx.HTTPStatusError as http_err:
            raise LLMError(f"xAI request failed: {http_err}")
        except Exception as e:
            raise LLMError(f"An error occurred: {e}")
</file>

<file path="aisuite/utils/tools.py">
from typing import Callable, Dict, Any, Type, Optional
from pydantic import BaseModel, create_model, Field, ValidationError
import inspect
import json
from docstring_parser import parse


class Tools:
    def __init__(self, tools: list[Callable] = None):
        self._tools = {}
        if tools:
            for tool in tools:
                self._add_tool(tool)

    # Add a tool function with or without a Pydantic model.
    def _add_tool(self, func: Callable, param_model: Optional[Type[BaseModel]] = None):
        """Register a tool function with metadata. If no param_model is provided, infer from function signature."""
        if param_model:
            tool_spec = self._convert_to_tool_spec(func, param_model)
        else:
            tool_spec, param_model = self.__infer_from_signature(func)

        self._tools[func.__name__] = {
            "function": func,
            "param_model": param_model,
            "spec": tool_spec,
        }

    # Return tools in the specified format (default OpenAI).
    def tools(self, format="openai") -> list:
        """Return tools in the specified format (default OpenAI)."""
        if format == "openai":
            return self.__convert_to_openai_format()
        return [tool["spec"] for tool in self._tools.values()]

    # Convert the function and its Pydantic model to a unified tool specification.
    def _convert_to_tool_spec(
        self, func: Callable, param_model: Type[BaseModel]
    ) -> Dict[str, Any]:
        """Convert the function and its Pydantic model to a unified tool specification."""
        type_mapping = {str: "string", int: "integer", float: "number", bool: "boolean"}

        properties = {}
        for field_name, field in param_model.model_fields.items():
            field_type = field.annotation

            # Handle enum types
            if hasattr(field_type, "__members__"):  # Check if it's an enum
                enum_values = [
                    member.value if hasattr(member, "value") else member.name
                    for member in field_type
                ]
                properties[field_name] = {
                    "type": "string",
                    "enum": enum_values,
                    "description": field.description or "",
                }
                # Convert enum default value to string if it exists
                if str(field.default) != "PydanticUndefined":
                    properties[field_name]["default"] = (
                        field.default.value
                        if hasattr(field.default, "value")
                        else field.default
                    )
            else:
                properties[field_name] = {
                    "type": type_mapping.get(field_type, str(field_type)),
                    "description": field.description or "",
                }
                # Add default if it exists and isn't PydanticUndefined
                if str(field.default) != "PydanticUndefined":
                    properties[field_name]["default"] = field.default

        return {
            "name": func.__name__,
            "description": func.__doc__ or "",
            "parameters": {
                "type": "object",
                "properties": properties,
                "required": [
                    name
                    for name, field in param_model.model_fields.items()
                    if field.is_required and str(field.default) == "PydanticUndefined"
                ],
            },
        }

    def __extract_param_descriptions(self, func: Callable) -> dict[str, str]:
        """Extract parameter descriptions from function docstring.

        Args:
            func: The function to extract parameter descriptions from

        Returns:
            Dictionary mapping parameter names to their descriptions
        """
        docstring = inspect.getdoc(func) or ""
        parsed_docstring = parse(docstring)

        param_descriptions = {}
        for param in parsed_docstring.params:
            param_descriptions[param.arg_name] = param.description or ""

        return param_descriptions

    def __infer_from_signature(
        self, func: Callable
    ) -> tuple[Dict[str, Any], Type[BaseModel]]:
        """Infer parameters(required and optional) and requirements directly from the function signature."""
        signature = inspect.signature(func)
        fields = {}
        required_fields = []

        # Get function's docstring and parse parameter descriptions
        param_descriptions = self.__extract_param_descriptions(func)
        docstring = inspect.getdoc(func) or ""

        # Parse the docstring to get the main function description
        parsed_docstring = parse(docstring)
        function_description = parsed_docstring.short_description or ""
        if parsed_docstring.long_description:
            function_description += "\n\n" + parsed_docstring.long_description

        for param_name, param in signature.parameters.items():
            # Check if a type annotation is missing
            if param.annotation == inspect._empty:
                raise TypeError(
                    f"Parameter '{param_name}' in function '{func.__name__}' must have a type annotation."
                )

            # Determine field type and optionality
            param_type = param.annotation
            description = param_descriptions.get(param_name, "")

            if param.default == inspect._empty:
                fields[param_name] = (param_type, Field(..., description=description))
                required_fields.append(param_name)
            else:
                fields[param_name] = (
                    param_type,
                    Field(default=param.default, description=description),
                )

        # Dynamically create a Pydantic model based on inferred fields
        param_model = create_model(f"{func.__name__.capitalize()}Params", **fields)

        # Convert inferred model to a tool spec format
        tool_spec = self._convert_to_tool_spec(func, param_model)

        # Update the tool spec with the parsed function description instead of raw docstring
        tool_spec["description"] = function_description

        return tool_spec, param_model

    def __convert_to_openai_format(self) -> list:
        """Convert tools to OpenAI's format."""
        return [
            {"type": "function", "function": tool["spec"]}
            for tool in self._tools.values()
        ]

    def results_to_messages(self, results: list, message: any) -> list:
        """Converts results to messages."""
        # if message is empty return empty list
        if not message or len(results) == 0:
            return []

        messages = []
        # Iterate over results and match with tool calls from the message
        for result in results:
            # Find matching tool call from message.tool_calls
            for tool_call in message.tool_calls:
                if tool_call.id == result["tool_call_id"]:
                    messages.append(
                        {
                            "role": "tool",
                            "name": result["name"],
                            "content": json.dumps(result["content"]),
                            "tool_call_id": tool_call.id,
                        }
                    )
                    break

        return messages

    def execute(self, tool_calls) -> list:
        """Executes registered tools based on the tool calls from the model.

        Args:
            tool_calls: List of tool calls from the model

        Returns:
            List of results from executing each tool call
        """
        results = []

        # Handle single tool call or list of tool calls
        if not isinstance(tool_calls, list):
            tool_calls = [tool_calls]

        for tool_call in tool_calls:
            # Handle both dictionary and object-style tool calls
            if isinstance(tool_call, dict):
                tool_name = tool_call["function"]["name"]
                arguments = tool_call["function"]["arguments"]
            else:
                tool_name = tool_call.function.name
                arguments = tool_call.function.arguments

            # Ensure arguments is a dict
            if isinstance(arguments, str):
                arguments = json.loads(arguments)

            if tool_name not in self._tools:
                raise ValueError(f"Tool '{tool_name}' not registered.")

            tool = self._tools[tool_name]
            tool_func = tool["function"]
            param_model = tool["param_model"]

            # Validate and parse the arguments with Pydantic if a model exists
            try:
                validated_args = param_model(**arguments)
                result = tool_func(**validated_args.model_dump())
                results.append(result)
            except ValidationError as e:
                raise ValueError(f"Error in tool '{tool_name}' parameters: {e}")

        return results

    def execute_tool(self, tool_calls) -> tuple[list, list]:
        """Executes registered tools based on the tool calls from the model.

        Args:
            tool_calls: List of tool calls from the model

        Returns:
            List of tuples containing (result, result_message) for each tool call
        """
        results = []
        messages = []

        # Handle single tool call or list of tool calls
        if not isinstance(tool_calls, list):
            tool_calls = [tool_calls]

        for tool_call in tool_calls:
            # Handle both dictionary and object-style tool calls
            if isinstance(tool_call, dict):
                tool_name = tool_call["function"]["name"]
                arguments = tool_call["function"]["arguments"]
                tool_call_id = tool_call["id"]
            else:
                tool_name = tool_call.function.name
                arguments = tool_call.function.arguments
                tool_call_id = tool_call.id

            # Ensure arguments is a dict
            if isinstance(arguments, str):
                arguments = json.loads(arguments)

            if tool_name not in self._tools:
                raise ValueError(f"Tool '{tool_name}' not registered.")

            tool = self._tools[tool_name]
            tool_func = tool["function"]
            param_model = tool["param_model"]

            # Validate and parse the arguments with Pydantic if a model exists
            try:
                validated_args = param_model(**arguments)
                result = tool_func(**validated_args.model_dump())
                results.append(result)
                messages.append(
                    {
                        "role": "tool",
                        "name": tool_name,
                        "content": json.dumps(result),
                        "tool_call_id": tool_call_id,
                    }
                )
            except ValidationError as e:
                raise ValueError(f"Error in tool '{tool_name}' parameters: {e}")

        return results, messages
</file>

<file path="CONTRIBUTING.md">
<!-- omit in toc -->
# Contributing to aisuite

First off, thanks for taking the time to contribute!

All types of contributions are encouraged and valued. See the [Table of Contents](#table-of-contents)
for different ways to help and details about how this project handles them. Please make sure to read
the relevant section before making your contribution. It will make it a lot easier for us maintainers
and smooth out the experience for all involved. The community looks forward to your contributions.

> And if you like the project, but just don't have time to contribute, that's fine. There are other easy
> ways to support the project and show your appreciation, which we would also be very happy about:
> - Star the project
> - Tweet about it
> - Refer this project in your project's readme
> - Mention the project at local meetups and tell your friends/colleagues

<!-- omit in toc -->
## Table of Contents

- [I Have a Question](#i-have-a-question)
- [I Want To Contribute](#i-want-to-contribute)
  - [Reporting Bugs](#reporting-bugs)
  - [Suggesting Enhancements](#suggesting-enhancements)
  - [Your First Code Contribution](#your-first-code-contribution)
  - [Improving The Documentation](#improving-the-documentation)
- [Styleguides](#styleguides)
  - [Commit Messages](#commit-messages)




## I Have a Question

> If you want to ask a question, we assume that you have read the available
> [Documentation](https://github.com/andrewyng/aisuite/blob/main/README.md).

Before you ask a question, it is best to search for existing [Issues](https://github.com/andrewyng/aisuite/issues)
that might help you. If you find a relevant issue that already exists and still need clarification, please add your question to that existing issue. We also recommend reaching out to the community in the aisuite [Discord](https://discord.gg/T6Nvn8ExSb) server.

If you then still feel the need to ask a question and need clarification, we recommend the following:

- Open an [Issue](https://github.com/andrewyng/aisuite/issues/new).
- Provide as much context as you can about what you're running into.
- Provide project and platform versions (python, OS, etc.), depending on what seems relevant.

We (or someone in the community) will then take care of the issue as soon as possible.


## I Want To Contribute

> ### Legal Notice <!-- omit in toc -->
> When contributing to this project, you must agree that you have authored 100% of the content, that
> you have the necessary rights to the content and that the content you contribute may be provided
> under the project license.

### Reporting Bugs

<!-- omit in toc -->
#### Before Submitting a Bug Report

A good bug report shouldn't leave others needing to chase you up for more information. Therefore, we ask
you to investigate carefully, collect information and describe the issue in detail in your report. Please
complete the following steps in advance to help us fix any potential bug as fast as possible.

- Make sure that you are using the latest version.
- Determine if your bug is really a bug and not an error on your side e.g. using incompatible environment 
  components/versions (Make sure that you have read the [documentation](https://github.com/andrewyng/aisuite/blob/main/README.md).
  If you are looking for support, you might want to check [this section](#i-have-a-question)).
- To see if other users have experienced (and potentially already solved) the same issue you are having,
  check if there is not already a bug report existing for your bug or error in the [bug tracker](https://github.com/andrewyng/aisuite?q=label%3Abug).
- Also make sure to search the internet (including Stack Overflow) to see if users outside of the GitHub
  community have discussed the issue.
- Collect information about the bug:
  - Stack trace (Traceback)
  - OS, Platform and Version (Windows, Linux, macOS, x86, ARM)
  - Version of the interpreter, compiler, SDK, runtime environment, package manager, depending on
    what seems relevant.
  - Possibly your input and the output
  - Can you reliably reproduce the issue? And can you also reproduce it with older versions?

<!-- omit in toc -->
#### How Do I Submit a Good Bug Report?

> You must never report security related issues, vulnerabilities or bugs including sensitive information to
> the issue tracker, or elsewhere in public. Instead sensitive bugs must be sent by email to <joaquin.dominguez@proton.me>.
<!-- You may add a PGP key to allow the messages to be sent encrypted as well. -->

We use GitHub issues to track bugs and errors. If you run into an issue with the project:

- Open an [Issue](https://github.com/andrewyng/aisuite/issues/new). (Since we can't be sure at
  this point whether it is a bug or not, we ask you not to talk about a bug yet and not to label the issue.)
- Explain the behavior you would expect and the actual behavior.
- Please provide as much context as possible and describe the *reproduction steps* that someone else can
  follow to recreate the issue on their own. This usually includes your code. For good bug reports you
  should isolate the problem and create a reduced test case.
- Provide the information you collected in the previous section.

Once it's filed:

- The project team will label the issue accordingly.
- A team member will try to reproduce the issue with your provided steps. If there are no reproduction 
  steps or no obvious way to reproduce the issue, the team will ask you for those steps and mark the
  issue as `needs-repro`. Bugs with the `needs-repro` tag will not be addressed until they are reproduced.
- If the team is able to reproduce the issue, it will be marked `needs-fix`, as well as possibly other
  tags (such as `critical`), and the issue will be left to be
  [implemented by someone](#your-first-code-contribution).

Please use the issue templates provided.


### Suggesting Enhancements

This section guides you through submitting an enhancement suggestion for aisuite,
**including completely new features and minor improvements to existing functionality**. Following these
guidelines will help maintainers and the community to understand your suggestion and find related suggestions.

<!-- omit in toc -->
#### Before Submitting an Enhancement

- Make sure that you are using the latest version.
- Read the [documentation](https://github.com/andrewyng/aisuite/blob/main/README.md) carefully
  and find out if the functionality is already covered, maybe by an individual configuration.
- Perform a [search](https://github.com/andrewyng/aisuite/issues) to see if the enhancement has
  already been suggested. If it has, add a comment to the existing issue instead of opening a new one.
- Find out whether your idea fits with the scope and aims of the project. It's up to you to make a strong
  case to convince the project's developers of the merits of this feature. Keep in mind that we want features that will be useful to the majority of our users and not just a small subset. If you're just targeting a minority of users, consider writing an add-on/plugin library.

<!-- omit in toc -->
#### How Do I Submit a Good Enhancement Suggestion?

Enhancement suggestions are tracked as [GitHub issues](https://github.com/andrewyng/aisuite/issues).

- Use a **clear and descriptive title** for the issue to identify the suggestion.
- Provide a **step-by-step description of the suggested enhancement** in as many details as possible.
- **Describe the current behavior** and **explain which behavior you expected to see instead** and why.
  At this point you can also tell which alternatives do not work for you.
- **Explain why this enhancement would be useful** to most aisuite users. You may also want to
  point out the other projects that solved it better and which could serve as inspiration.


### Your First Code Contribution

#### Pre-requisites

You should first [fork](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo)
the `aisuite` repository and then clone your forked repository:

```bash
git clone https://github.com/<YOUR_GITHUB_USER>/aisuite.git
```



Once in the cloned repository directory, make a branch on the forked repository with your username and
description of PR:
```bash
git checkout -B <username>/<description>
```

Please install the development and test dependencies:
```bash
poetry install --with dev,test
```

`aisuite` uses pre-commit to ensure the formatting is consistent:
```bash
pre-commit install
```

**Make suggested changes**

Afterwards, our suite of formatting tests will run automatically before each `git commit`. You can also
run these manually:
```bash
pre-commit run --all-files
```

If a formatting test fails, it will fix the modified code in place and abort the `git commit`. After looking
over the changes, you can `git add <modified files>` and then repeat the previous git commit command.

**Note**: a github workflow will check the files with the same formatter and reject the PR if it doesn't
pass, so please make sure it passes locally.


#### Testing
`aisuite` tracks unit tests. Pytest is used to execute said unit tests in `tests/`:

```bash
poetry run pytest tests
```

If your code changes implement a new function, please make a corresponding unit test to the `test/*` files.

#### Contributing Workflow
We actively welcome your pull requests.

1. Create your new branch from main in your forked repo, with your username and a name describing the work
   you're completing e.g. user-123/add-feature-x.
2. If you've added code that should be tested, add tests. Ensure all tests pass. See the testing section
   for more information.
3. If you've changed APIs, update the documentation.
4. Make sure your code lints.



### Improving The Documentation
We welcome valuable contributions in the form of new documentation or revised documentation that provide
further clarity or accuracy. Each function should be clearly documented. Well-documented code is easier
to review and understand/extend.

## Styleguides
For code documentation, please follow the [Google styleguide](https://github.com/google/styleguide/blob/gh-pages/pyguide.md#38-comments-and-docstrings).
</file>

<file path="examples/aisuite_tool_abstraction.ipynb">
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "sys.path.append('../../aisuite')\n",
    "# Load from .env file if available\n",
    "load_dotenv(find_dotenv())\n",
    "os.environ['ALLOW_MULTI_TURN'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock tool functions.\n",
    "def get_current_temperature(location: str, unit: str):\n",
    "    \"\"\"This is a short description of what the function does.\n",
    "\n",
    "    This is a longer description that can span\n",
    "    multiple lines and provide more details.\n",
    "\n",
    "    Args:\n",
    "        param1: Description of param1\n",
    "        param2: Description of param2\n",
    "    \"\"\"\n",
    "    return \"70\"\n",
    "\n",
    "def is_it_raining(location: str):\n",
    "    # Simulate fetching rain probability\n",
    "    return \"yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the model with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisuite import Client\n",
    "\n",
    "client = Client()\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Can you plan a picnic for today afternoon in San Francisco? Check the temperature and if its raining.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- response from LLM ---------\n",
      "ChatCompletion(id='chatcmpl-AvuR3w6M83nWHL9sIO23pgvD0E5PF', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_PVhrHTUQ2qY7FDyPkCT54phU', function=Function(arguments='{\\n\"location\": \"San Francisco\",\\n\"unit\": \"Fahrenheit\"\\n}', name='get_current_temperature'), type='function')]))], created=1738364997, model='gpt-4-0613', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=24, prompt_tokens=110, total_tokens=134, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "Executing tool:  get_current_temperature\n",
      "--------- tool_message to send to LLM ---------\n",
      "[{'role': 'tool', 'name': 'get_current_temperature', 'content': '\"70\"', 'tool_call_id': 'call_PVhrHTUQ2qY7FDyPkCT54phU'}]\n",
      "--------- response from LLM ---------\n",
      "ChatCompletion(id='chatcmpl-AvuR4hpiCgXFxRMNLg1Scv1UD2JlR', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Fvy3JUhIbVqzV0Nb0QfSnWQC', function=Function(arguments='{\\n\"location\": \"San Francisco\"\\n}', name='is_it_raining'), type='function')]))], created=1738364998, model='gpt-4-0613', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=145, total_tokens=163, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "Executing tool:  is_it_raining\n",
      "--------- tool_message to send to LLM ---------\n",
      "[{'role': 'tool', 'name': 'is_it_raining', 'content': '\"yes\"', 'tool_call_id': 'call_Fvy3JUhIbVqzV0Nb0QfSnWQC'}]\n",
      "--------- response from LLM ---------\n",
      "ChatCompletion(id='chatcmpl-AvuR7puWSJLEpCNIZLLkF40gYJp3c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I'm sorry, it seems like it will be raining this afternoon in San Francisco. You might want to plan your picnic for another day. Also the temperature is forecasted to be around 70 degrees Fahrenheit.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738365001, model='gpt-4-0613', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=44, prompt_tokens=175, total_tokens=219, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "I'm sorry, it seems like it will be raining this afternoon in San Francisco. You might want to plan your picnic for another day. Also the temperature is forecasted to be around 70 degrees Fahrenheit.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"openai:gpt-4\", messages=messages, tools=[get_current_temperature, is_it_raining], max_turns=4)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"anthropic:claude-3-5-sonnet-20241022\", messages=messages, tools=[get_current_temperature, is_it_raining], max_turns=4)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_PVhrHTUQ2qY7FDyPkCT54phU', function=Function(arguments='{\\n\"location\": \"San Francisco\",\\n\"unit\": \"Fahrenheit\"\\n}', name='get_current_temperature'), type='function')]),\n",
      " {'content': '\"70\"',\n",
      "  'name': 'get_current_temperature',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_PVhrHTUQ2qY7FDyPkCT54phU'},\n",
      " ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Fvy3JUhIbVqzV0Nb0QfSnWQC', function=Function(arguments='{\\n\"location\": \"San Francisco\"\\n}', name='is_it_raining'), type='function')]),\n",
      " {'content': '\"yes\"',\n",
      "  'name': 'is_it_raining',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'call_Fvy3JUhIbVqzV0Nb0QfSnWQC'},\n",
      " ChatCompletionMessage(content=\"I'm sorry, it seems like it will be raining this afternoon in San Francisco. You might want to plan your picnic for another day. Also the temperature is forecasted to be around 70 degrees Fahrenheit.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "pprint(response.choices[0].intermediate_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisuite import Tools\n",
    "tools = Tools(tools=[get_current_temperature, is_it_raining])\n",
    "tools.tools()\n",
    "# tools.add_description(\"is_it_raining\", \"Use this function to understand if it is going to rain or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = append(messages, response.choices[0].intermediate_messages)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="examples/AISuiteDemo.ipynb">
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "AI Suite is a light wrapper to provide a unified interface between LLM providers."
      ],
      "metadata": {
        "id": "hZq_yZRcbxdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install AI Suite\n",
        "!pip install aisuite[all]"
      ],
      "metadata": {
        "id": "1mt8kgFHXMvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b56619e8-0dd8-4850-d3b2-1f1169672aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aisuite[all]\n",
            "  Downloading aisuite-0.1.5-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting anthropic<0.31.0,>=0.30.1 (from aisuite[all])\n",
            "  Downloading anthropic-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting groq<0.10.0,>=0.9.0 (from aisuite[all])\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.35.8 in /usr/local/lib/python3.10/dist-packages (from aisuite[all]) (1.52.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (4.12.2)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.35.8->aisuite[all]) (4.66.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.23.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.2.3)\n",
            "Downloading anthropic-0.30.1-py3-none-any.whl (863 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m863.9/863.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aisuite-0.1.5-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: aisuite, groq, anthropic\n",
            "Successfully installed aisuite-0.1.5 anthropic-0.30.1 groq-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Pretty Printing Function\n",
        "In this section, we define a custom pretty-printing function that enhances the readability of data structures when printed. This function utilizes Python's built-in pprint module, allowing users to specify a custom width for output formatting."
      ],
      "metadata": {
        "id": "KwFlLByRbWKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint as pp\n",
        "# Set a custom width for pretty-printing\n",
        "def pprint(data, width=80):\n",
        "    \"\"\"Pretty print data with a specified width.\"\"\"\n",
        "    pp(data, width=width)# List of model identifiers to query\n"
      ],
      "metadata": {
        "id": "-Wf7j6abbQmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up API Keys\n",
        "\n",
        "Here we will securely set our API keys as environment variables. This is helpful because we dont want to hardcode sensitive information (like API keys) directly into our code. By using environment variables, we can keep our credentials secure while still allowing our program to access them. Normally we would use a .env file to store our passwords to our enviroments, but since we are going to be working in colab we will do things a little different."
      ],
      "metadata": {
        "id": "Cce1aLBvctaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsK7GrHyV-c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35fef9dc-e226-4e9d-e6c7-a597882b74f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your GROQ API key: \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "os.environ['GROQ_API_KEY'] = getpass('Enter your GROQ API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Simple Chat Interaction with an AI Language Model\n",
        "This code initiates a chat interaction with a language model (specifically Groqs LLaMA 3.2), where the model responds to the user's input. We use the aisuite library to communicate with the model and retrieve the response."
      ],
      "metadata": {
        "id": "m2mhu-VbSWfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import aisuite as ai\n",
        "\n",
        "# Initialize the AI client for accessing the language model\n",
        "client = ai.Client()\n",
        "\n",
        "# Define a conversation with a system message and a user message\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful agent, who answers with brevity.\"},\n",
        "    {\"role\": \"user\", \"content\": 'Hi'},\n",
        "]\n",
        "\n",
        "# Request a response from the model\n",
        "response = client.chat.completions.create(model=\"groq:llama-3.2-3b-preview\", messages=messages)\n",
        "\n",
        "# Print the model's response\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "mBEOEq99eGjR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "446fdba3-9072-4470-b3b8-627717013604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How can I assist you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining a Function to Interact with the Language Model\n",
        "\n",
        "This function, ask, streamlines the process of sending a user message to a language model and retrieving a response. It encapsulates the logic required to set up the conversation and can be reused throughout the notebook for different queries. It will not perserve any history or any continuing conversation.  \n",
        "\n"
      ],
      "metadata": {
        "id": "YJSahowjiJBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(message, sys_message=\"You are a helpful agent.\",\n",
        "         model=\"groq:llama-3.2-3b-preview\"):\n",
        "    # Initialize the AI client for accessing the language model\n",
        "    client = ai.Client()\n",
        "\n",
        "    # Construct the messages list for the chat\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": sys_message},\n",
        "        {\"role\": \"user\", \"content\": message}\n",
        "    ]\n",
        "\n",
        "    # Send the messages to the model and get the response\n",
        "    response = client.chat.completions.create(model=model, messages=messages)\n",
        "\n",
        "    # Return the content of the model's response\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "n8DK8_RqqXFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"Hi. what is capital of Japan?\")"
      ],
      "metadata": {
        "id": "FGcqY4lBjtFj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0520933a-8f2f-4185-a8a2-c591283482a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello. The capital of Japan is Tokyo.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The real value of AI Suite is the ablity to run a variety of different models.  Let's first set up a collection of different API keys which we can try out."
      ],
      "metadata": {
        "id": "wpeW6Pj6j_6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = getpass('Enter your OPENAI API key: ')\n",
        "os.environ['ANTHROPIC_API_KEY'] = getpass('Enter your ANTHROPIC API key: ')"
      ],
      "metadata": {
        "id": "9_kJlkGfj_NG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45074c6-bbc6-4214-df0c-6d162a176f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OPENAI API key: \n",
            "Enter your ANTHROPIC API key: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Confirm each model is using a different provider\n"
      ],
      "metadata": {
        "id": "mfPtlJlbTY6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(ask(\"Who is your creator?\"))\n",
        "print(ask('Who is your creator?', model='anthropic:claude-3-5-sonnet-20240620'))\n",
        "print(ask('Who is your creator?', model='openai:gpt-4o'))\n"
      ],
      "metadata": {
        "id": "iHVESCGJuWWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3102b43a-e754-4288-ec1d-9777791f25b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I was created by Meta AI, a leading artificial intelligence research organization. My knowledge was developed from a large corpus of text, which I use to generate human-like responses to user queries.\n",
            "I was created by Anthropic.\n",
            "I was developed by OpenAI, an organization that focuses on artificial intelligence research and deployment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying Multiple AI Models for a Common Question\n",
        "In this section, we will query several different versions of the LLaMA language model to get varied responses to the same question. This approach allows us to compare how different models handle the same prompt, providing insights into their performance and style."
      ],
      "metadata": {
        "id": "BWBL4D2H2B_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "models = [\n",
        "    'llama-3.1-8b-instant',\n",
        "    'llama-3.2-1b-preview',\n",
        "    'llama-3.2-3b-preview',\n",
        "    'llama3-70b-8192',\n",
        "    'llama3-8b-8192'\n",
        "]\n",
        "\n",
        "# Initialize a list to hold the responses from each model\n",
        "ret = []\n",
        "\n",
        "# Loop through each model and get a response for the specified question\n",
        "for x in models:\n",
        "    ret.append(ask('Write a short one sentence explanation of the origins of AI?', model=f'groq:{x}'))\n",
        "\n",
        "# Print the model's name and its corresponding response\n",
        "for idx, x in enumerate(ret):\n",
        "    pprint(models[idx] + ': \\n ' + x + ' ')"
      ],
      "metadata": {
        "id": "E_gg-sgYuoOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c582ba-3471-4b0e-b9ca-317df8a1c1c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('llama-3.1-8b-instant: \\n'\n",
            " ' The origins of Artificial Intelligence (AI) date back to the 1956 Dartmouth '\n",
            " 'Summer Research Project on Artificial Intelligence, where a group of '\n",
            " 'computer scientists, led by John McCarthy, Marvin Minsky, Nathaniel '\n",
            " 'Rochester, and Claude Shannon, coined the term and laid the foundation for '\n",
            " 'the development of AI as a distinct field of study. ')\n",
            "('llama-3.2-1b-preview: \\n'\n",
            " ' The origins of Artificial Intelligence (AI) date back to the mid-20th '\n",
            " 'century, when the first computer programs, which mimicked human-like '\n",
            " 'intelligence through algorithms and rule-based systems, were developed by '\n",
            " 'renowned mathematicians and computer scientists, including Alan Turing, '\n",
            " 'Marvin Minsky, and John McCarthy in the 1950s. ')\n",
            "('llama-3.2-3b-preview: \\n'\n",
            " ' The origins of Artificial Intelligence (AI) date back to the 1950s, with '\n",
            " 'the Dartmouth Summer Research Project on Artificial Intelligence, led by '\n",
            " 'computer scientists John McCarthy, Marvin Minsky, and Nathaniel Rochester, '\n",
            " 'marking the birth of AI as a formal field of research. ')\n",
            "('llama3-70b-8192: \\n'\n",
            " ' The origins of Artificial Intelligence (AI) can be traced back to the 1950s '\n",
            " 'when computer scientist Alan Turing proposed the Turing Test, a method for '\n",
            " 'determining whether a machine could exhibit intelligent behavior equivalent '\n",
            " 'to, or indistinguishable from, that of a human. ')\n",
            "('llama3-8b-8192: \\n'\n",
            " ' The origins of Artificial Intelligence (AI) can be traced back to the '\n",
            " '1950s, when computer scientists DARPA funded the development of the first AI '\n",
            " 'programs, such as the Logical Theorist, which aimed to simulate human '\n",
            " 'problem-solving abilities and learn from experience. ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying Different AI Providers for a Common Question\n",
        "In this section, we will query multiple AI models from different providers to get varied responses to the same question regarding the origins of AI. This comparison allows us to observe how different models from different architectures respond to the same prompt."
      ],
      "metadata": {
        "id": "Z8pnJPdD2NL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of AI model providers to query\n",
        "providers = [\n",
        "    'groq:llama3-70b-8192',\n",
        "    'openai:gpt-4o',\n",
        "    'anthropic:claude-3-5-sonnet-20240620'\n",
        "]\n",
        "\n",
        "# Initialize a list to hold the responses from each provider\n",
        "ret = []\n",
        "\n",
        "# Loop through each provider and get a response for the specified question\n",
        "for x in providers:\n",
        "    ret.append(ask('Write a short one sentence explanation of the origins of AI?', model=x))\n",
        "\n",
        "# Print the provider's name and its corresponding response\n",
        "for idx, x in enumerate(ret):\n",
        "    pprint(providers[idx] + ': \\n' + x + ' \\n\\n')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "j4TqhC5J1YIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a50e300-0a7a-4562-8a34-f31c4b9072d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('groq:llama3-70b-8192: \\n'\n",
            " 'The origins of Artificial Intelligence (AI) can be traced back to the 1950s '\n",
            " 'when computer scientists like Alan Turing, Marvin Minsky, and John McCarthy '\n",
            " 'began exploring ways to create machines that could think and learn like '\n",
            " 'humans, leading to the development of the first AI programs and '\n",
            " 'algorithms. \\n'\n",
            " '\\n')\n",
            "('openai:gpt-4o: \\n'\n",
            " 'The origins of AI trace back to the mid-20th century, when pioneers like '\n",
            " 'Alan Turing and John McCarthy began exploring the possibility of creating '\n",
            " 'machines that could simulate human intelligence through computational '\n",
            " 'processes. \\n'\n",
            " '\\n')\n",
            "('anthropic:claude-3-5-sonnet-20240620: \\n'\n",
            " 'The origins of AI can be traced back to the 1950s when computer scientists '\n",
            " 'began exploring the concept of creating machines that could simulate human '\n",
            " 'intelligence and problem-solving abilities. \\n'\n",
            " '\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating and Evaluating Questions with AI Models\n",
        "In this section, we will randomly generate questions using a language model and then have two other models provide answers to those questions. The user will then evaluate which answer is better, allowing for a comparative analysis of responses from different models."
      ],
      "metadata": {
        "id": "OgPCC0y_U4WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Initialize a list to store the best responses\n",
        "best = []\n",
        "\n",
        "# Loop to generate and evaluate questions\n",
        "for _ in range(20):\n",
        "    # Shuffle the providers list to randomly select models for each iteration\n",
        "    random.shuffle(providers)\n",
        "\n",
        "    # Generate a question using the first provider\n",
        "    question = ask('Please generate a short question that is suitable for asking an LLM.', model=providers[0])\n",
        "\n",
        "    # Get answers from the second and third providers\n",
        "    answer_1 = ask('Please give a short answer to this question: ' + question, model=providers[1])\n",
        "    answer_2 = ask('Please give a short answer to this question: ' + question, model=providers[2])\n",
        "\n",
        "    # Print the generated question and the two answers\n",
        "    pprint(f\"Original text:\\n  {question}\\n\\n\")\n",
        "    pprint(f\"Option 1 text:\\n  {answer_1}\\n\\n\")\n",
        "    pprint(f\"Option 2 text:\\n  {answer_2}\\n\\n\")\n",
        "\n",
        "    # Store the provider names and the user's choice of the best answer\n",
        "    best.append(str(providers) + ', ' + input(\"Which is best 1 or 2. 3 if indistinguishable: \"))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fMx-TfLk09ft",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56153c03-a1e6-4b72-fd16-b36197ccb5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Original text:\\n'\n",
            " \"  Here's a short question suitable for asking an LLM:\\n\"\n",
            " '\\n'\n",
            " 'What are the potential benefits and risks of artificial intelligence in '\n",
            " 'healthcare?\\n'\n",
            " '\\n')\n",
            "('Option 1 text:\\n'\n",
            " '  **Benefits:**\\n'\n",
            " '1. Improved diagnostics and personalized treatment plans.\\n'\n",
            " '2. Increased efficiency in administrative tasks.\\n'\n",
            " '3. Faster drug discovery and development.\\n'\n",
            " '4. Enhanced patient monitoring and support.\\n'\n",
            " '\\n'\n",
            " '**Risks:**\\n'\n",
            " '1. Privacy and data security concerns.\\n'\n",
            " '2. Potential biases in AI algorithms.\\n'\n",
            " '3. Over-reliance on AI systems by healthcare professionals.\\n'\n",
            " '4. Ethical and accountability issues in decision-making.\\n'\n",
            " '\\n')\n",
            "('Option 2 text:\\n'\n",
            " '  The potential benefits of artificial intelligence (AI) in healthcare '\n",
            " 'include:\\n'\n",
            " '\\n'\n",
            " '* Improved diagnosis accuracy and speed\\n'\n",
            " '* Enhanced patient outcomes through personalized medicine\\n'\n",
            " '* Increased efficiency and reduced costs through automation\\n'\n",
            " '* Better disease prevention and detection\\n'\n",
            " '* Enhanced research capabilities and new treatment discoveries\\n'\n",
            " '\\n'\n",
            " 'However, there are also potential risks, such as:\\n'\n",
            " '\\n'\n",
            " '* Bias in AI decision-making due to flawed data or algorithms\\n'\n",
            " '* Job displacement of healthcare professionals\\n'\n",
            " '* Cybersecurity risks to patient data\\n'\n",
            " '* Dependence on technology leading to deskilling of healthcare workers\\n'\n",
            " '* Unintended consequences of AI-driven decision-making that may not align '\n",
            " 'with human values.\\n'\n",
            " '\\n'\n",
            " 'These benefits and risks highlight the need for responsible development, '\n",
            " 'deployment, and oversight of AI in healthcare.\\n'\n",
            " '\\n')\n",
            "Which is best 1 or 2. 3 if indistinguishable: 3\n",
            "('Original text:\\n'\n",
            " '  What are the potential applications of large language models in '\n",
            " 'healthcare?\\n'\n",
            " '\\n')\n",
            "('Option 1 text:\\n'\n",
            " '  Large language models have numerous potential applications in healthcare, '\n",
            " 'including:\\n'\n",
            " '\\n'\n",
            " '1. **Clinical Decision Support**: Providing doctors with accurate diagnoses, '\n",
            " 'treatment options, and medication recommendations.\\n'\n",
            " '2. **Medical Text Analysis**: Analyzing large amounts of medical literature, '\n",
            " 'patient records, and clinical notes to identify patterns and insights.\\n'\n",
            " '3. **Patient Engagement**: Generating personalized health summaries, '\n",
            " 'communicating medical information in simple language, and facilitating '\n",
            " 'patient-provider communication.\\n'\n",
            " '4. **Disease Surveillance**: Monitoring social media and online platforms '\n",
            " 'for disease outbreaks and tracking epidemiological trends.\\n'\n",
            " '5. **Medical Writing Assistance**: Assisting healthcare professionals in '\n",
            " 'generating medical reports, discharge summaries, and other documents.\\n'\n",
            " '6. **Chatbots and Virtual Assistants**: Offering patients timely support and '\n",
            " 'answers to medical queries.\\n'\n",
            " '7. **Research and Development**: Accelerating biomedical research by '\n",
            " 'analyzing large datasets, identifying research gaps, and suggesting '\n",
            " 'potential areas of investigation.\\n'\n",
            " '\\n'\n",
            " 'These applications have the potential to improve healthcare outcomes, reduce '\n",
            " 'costs, and enhance patient experiences.\\n'\n",
            " '\\n')\n",
            "('Option 2 text:\\n'\n",
            " '  Large language models in healthcare could potentially be used for:\\n'\n",
            " '\\n'\n",
            " '1. Clinical decision support\\n'\n",
            " '2. Medical literature analysis and summarization\\n'\n",
            " '3. Patient triage and symptom checking\\n'\n",
            " '4. Medical education and training\\n'\n",
            " '5. Automated medical coding and documentation\\n'\n",
            " '6. Drug discovery and development\\n'\n",
            " '7. Personalized treatment recommendations\\n'\n",
            " '8. Health-related chatbots for patient engagement\\n'\n",
            " '9. Medical research and hypothesis generation\\n'\n",
            " '10. Natural language processing of electronic health records\\n'\n",
            " '\\n'\n",
            " 'These applications could help improve efficiency, accuracy, and '\n",
            " 'accessibility in various aspects of healthcare.\\n'\n",
            " '\\n')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d17783dc1f7c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Store the provider names and the user's choice of the best answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mbest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Which is best 1 or 2. 3 if indistinguishable: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}
</file>

<file path="examples/chat-ui/.streamlit/config.toml">
[theme]
primaryColor = "#1E90FF"  # Blue color for primary components
backgroundColor = "#0e1117"  # Background color
secondaryBackgroundColor = "#262730"  # Secondary background color
textColor = "#ffffff"  # Text color
font = "sans serif"
</file>

<file path="examples/chat-ui/chat.py">
import os
import requests
import streamlit as st
import sys
import yaml
from dotenv import load_dotenv, find_dotenv

sys.path.append("../../../aisuite")
from aisuite.client import Client

# Configure Streamlit to use wide mode and hide the top streamlit menu
st.set_page_config(layout="wide", menu_items={})
# Add heading with padding
st.markdown(
    "<div style='padding-top: 1rem;'><h2 style='text-align: center; color: #ffffff;'>Chat & Compare LLM responses</h2></div>",
    unsafe_allow_html=True,
)
st.markdown(
    """
    <style>
        /* Apply default font size globally */
        html, body, [class*="css"] {
            font-size: 14px !important;
        }
        
        /* Style for Reset button focus */
        button[data-testid="stButton"][aria-label="Reset Chat"]:focus {
            border-color: red !important;
            box-shadow: 0 0 0 2px red !important;
        }
    </style>
    """,
    unsafe_allow_html=True,
)
st.markdown(
    """
    <style>
        /* Hide Streamlit's default top bar */
        #MainMenu {visibility: hidden;}
        header {visibility: hidden;}
        footer {visibility: hidden;}
        
        /* Remove top padding/margin */
        .block-container {
            padding-top: 0rem;
            padding-bottom: 0rem;
            margin-top: 0rem;
        }

        /* Remove padding from the app container */
        .appview-container {
            padding-top: 0rem;
        }
        
        /* Custom CSS for scrollable chat container */
        .chat-container {
            height: 650px;
            overflow-y: auto !important;
            background-color: #1E1E1E;
            border: 1px solid #333;
            border-radius: 10px;
            padding: 20px;
            margin: 10px 0;
        }
        
        /* Ensure the container takes full width */
        .stMarkdown {
            width: 100%;
        }
        
        /* Style for chat messages to ensure they're visible */
        .chat-message {
            margin: 10px 0;
            padding: 10px;
        }
        
        #text_area_1 {
            min-height: 20px !important;
        } 
    </style>
    """,
    unsafe_allow_html=True,
)

# Load configuration and initialize aisuite client
with open("config.yaml", "r") as file:
    config = yaml.safe_load(file)
configured_llms = config["llms"]
load_dotenv(find_dotenv())
client = Client()


# Function to display chat history
def display_chat_history(chat_history, model_name):
    for message in chat_history:
        role_display = "User" if message["role"] == "user" else model_name
        role = "user" if message["role"] == "user" else "assistant"
        if role == "user":
            with st.chat_message(role, avatar=""):
                st.write(message["content"])
        else:
            with st.chat_message(role, avatar=""):
                st.write(message["content"])


# Helper function to query each LLM
def query_llm(model_config, chat_history):
    print(f"Querying {model_config['name']} with {chat_history}")
    try:
        model = model_config["provider"] + ":" + model_config["model"]
        response = client.chat.completions.create(model=model, messages=chat_history)
        print(
            f"Response from {model_config['name']}: {response.choices[0].message.content}"
        )
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"Error querying {model_config['name']}: {e}")
        return "Error with LLM response."


# Initialize session states
if "chat_history_1" not in st.session_state:
    st.session_state.chat_history_1 = []
if "chat_history_2" not in st.session_state:
    st.session_state.chat_history_2 = []
if "is_processing" not in st.session_state:
    st.session_state.is_processing = False
if "use_comparison_mode" not in st.session_state:
    st.session_state.use_comparison_mode = False

# Top Section - Controls
col1, col2 = st.columns([1, 2])
with col1:
    st.session_state.use_comparison_mode = st.checkbox("Comparison Mode", value=True)

# Move LLM selection below comparison mode checkbox - now in columns
llm_col1, llm_col2 = st.columns(2)
with llm_col1:
    selected_model_1 = st.selectbox(
        "Choose LLM Model 1",
        [llm["name"] for llm in configured_llms],
        key="model_1",
        index=0 if configured_llms else 0,
    )
with llm_col2:
    if st.session_state.use_comparison_mode:
        selected_model_2 = st.selectbox(
            "Choose LLM Model 2",
            [llm["name"] for llm in configured_llms],
            key="model_2",
            index=1 if len(configured_llms) > 1 else 0,
        )

# Display Chat Histories first, always
# Middle Section - Display Chat Histories
if st.session_state.use_comparison_mode:
    col1, col2 = st.columns(2)
    with col1:
        chat_container = st.container(height=500)
        with chat_container:
            display_chat_history(st.session_state.chat_history_1, selected_model_1)
    with col2:
        chat_container = st.container(height=500)
        with chat_container:
            display_chat_history(st.session_state.chat_history_2, selected_model_2)
else:
    chat_container = st.container(height=500)
    with chat_container:
        display_chat_history(st.session_state.chat_history_1, selected_model_1)

# Bottom Section - User Input
st.markdown("<div style='height: 20px;'></div>", unsafe_allow_html=True)

col1, col2, col3 = st.columns([6, 1, 1])
with col1:
    user_query = st.text_area(
        label="Enter your query",
        label_visibility="collapsed",
        placeholder="Enter your query...",
        key="query_input",
        height=70,
    )


# CSS for aligning buttons with the bottom of the text area
st.markdown(
    """
    <style>
        /* Adjust the container of the buttons to align at the bottom */
        .stButton > button {
            margin-top: 35px !important; /* Adjust the margin to align */
        }

        /* Align buttons and "Processing..." text to the bottom of the text area */
        .button-container {
            margin-top: 42px !important;
            text-align: center; /* Center-aligns "Processing..." */
        }
    </style>
    """,
    unsafe_allow_html=True,
)

with col2:
    send_button = False  # Initialize send_button
    if st.session_state.is_processing:
        st.markdown(
            "<div class='button-container'>Processing... </div>",
            unsafe_allow_html=True,
        )
    else:
        send_button = st.button("Send Query", use_container_width=True)

with col3:
    if st.button("Reset Chat", use_container_width=True):
        st.session_state.chat_history_1 = []
        st.session_state.chat_history_2 = []
        st.rerun()

# Handle send button click and processing
if send_button and user_query and not st.session_state.is_processing:
    # Set processing state
    st.session_state.is_processing = True

    # Append user's message to chat histories first
    st.session_state.chat_history_1.append({"role": "user", "content": user_query})
    if st.session_state.use_comparison_mode:
        st.session_state.chat_history_2.append({"role": "user", "content": user_query})

    st.rerun()

# Handle the actual processing
if st.session_state.is_processing and user_query:
    # Query the selected LLM(s)
    model_config_1 = next(
        llm for llm in configured_llms if llm["name"] == selected_model_1
    )
    response_1 = query_llm(model_config_1, st.session_state.chat_history_1)
    st.session_state.chat_history_1.append({"role": "assistant", "content": response_1})

    if st.session_state.use_comparison_mode:
        model_config_2 = next(
            llm for llm in configured_llms if llm["name"] == selected_model_2
        )
        response_2 = query_llm(model_config_2, st.session_state.chat_history_2)
        st.session_state.chat_history_2.append(
            {"role": "assistant", "content": response_2}
        )

    # Reset processing state
    st.session_state.is_processing = False
    st.rerun()
</file>

<file path="examples/chat-ui/config.yaml">
# config.yaml
llms:
  - name: "OpenAI GPT-4o"
    provider: "openai"
    model: "gpt-4o"
  - name: "Anthropic Claude 3.5 Sonnet"
    provider: "anthropic"
    model: "claude-3-5-sonnet-20240620"
  - name: "Azure/OpenAI GPT-4o"
    provider: "azure"
    model: "gpt-4o"
  - name: "Huggingface/Mistral 7B"
    provider: "huggingface"
    model: "mistralai/Mistral-7B-Instruct"
</file>

<file path="examples/chat-ui/README.md">
# Chat UI

This is a simple chat UI built using Streamlit. It uses the `aisuite` library to power the chat.

You will need to install streamlit to run this example.

```bash
pip install streamlit
```

You will also need to create a `config.yaml` file in the same directory as the `chat.py` file. An example config file has been provided. You need to set environment variables for the API keys and other configuration for the LLMs you want to use. Place a .env file in this directory since `chat.py` will look for it.

In config.yaml, you can specify the LLMs you want to use in the chat. The chat UI will then display all these LLMs and you can select the one you want to use.

To run the app, simply run the following command in your terminal:

```bash
streamlit run chat.py
```

You can choose different LLMs by ticking the "Comparison Mode" checkbox. Then select the two LLMs you want to compare.
Here are some sample queries you can try:

```
User: "What is the weather in Tokyo?"
```

```
User: "Write a poem about the weather in Tokyo."
```

```
User: "Write a python program to print the fibonacci sequence."
Assistant: "-- Content from LLM 1 --"
User: "Write test cases for this program."
```
</file>

<file path="examples/client.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34f8c48-90fc-4981-8d2b-b47724c2a6dd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Client Examples\n",
    "\n",
    "Client provides a uniform interface for interacting with LLMs from various providers. It adapts the official python libraries from providers such as Mistral, OpenAI, Groq, Anthropic, AWS, etc to conform to the OpenAI chat completion interface. It directly calls the REST endpoints in some cases.\n",
    "\n",
    "Below are some examples of how to use Client to interact with different LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T15:30:02.064319Z",
     "start_time": "2024-07-04T15:30:02.051986Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "sys.path.append('../../aisuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75736ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def configure_environment(additional_env_vars=None):\n",
    "    \"\"\"\n",
    "    Load environment variables from .env file and apply any additional variables.\n",
    "    :param additional_env_vars: A dictionary of additional environment variables to apply.\n",
    "    \"\"\"\n",
    "    # Load from .env file if available\n",
    "    load_dotenv(find_dotenv())\n",
    "\n",
    "    # Apply additional environment variables\n",
    "    if additional_env_vars:\n",
    "        for key, value in additional_env_vars.items():\n",
    "            os.environ[key] = value\n",
    "\n",
    "# Define additional API keys and credentials\n",
    "additional_keys = {\n",
    "    'GROQ_API_KEY': 'xxx',\n",
    "    'AWS_ACCESS_KEY_ID': 'xxx',\n",
    "    'AWS_SECRET_ACCESS_KEY': 'xxx',\n",
    "    'ANTHROPIC_API_KEY': 'xxx',\n",
    "    'NEBIUS_API_KEY': 'xxx',\n",
    "}\n",
    "\n",
    "# Configure environment\n",
    "configure_environment(additional_env_vars=additional_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3a24f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T15:31:12.914321Z",
     "start_time": "2024-07-04T15:31:12.796445Z"
    }
   },
   "outputs": [],
   "source": [
    "import aisuite as ai\n",
    "\n",
    "client = ai.Client()\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Respond in Pirate English. Always try to include the phrase - No rum No fun.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke about Captain Jack Sparrow\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a6879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "anthropic_claude_3_opus = \"anthropic:claude-3-5-sonnet-20240620\"\n",
    "response = client.chat.completions.create(model=anthropic_claude_3_opus, messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9893c7e4-799a-42c9-84de-f9e643044462",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_bedrock_llama3_8b = \"aws:meta.llama3-1-8b-instruct-v1:0\"\n",
    "response = client.chat.completions.create(model=aws_bedrock_llama3_8b, messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMP NOTE: Azure expects model endpoint to be passed in the format of \"azure:<model_name>\".\n",
    "# The model name is the deployment name in Project/Deployments.\n",
    "# In the example below, the model is \"mistral-large-2407\", but the name given to the\n",
    "# deployment is \"aisuite-mistral-large-2407\" under the deployments section in Azure.\n",
    "client.configure({\"azure\" : {\n",
    "  \"api_key\": os.environ[\"AZURE_API_KEY\"],\n",
    "  \"base_url\": \"https://aisuite-mistral-large-2407.westus3.models.ai.azure.com/v1/\",\n",
    "}});\n",
    "azure_model = \"azure:aisuite-mistral-large-2407\"\n",
    "response = client.chat.completions.create(model=azure_model, messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace expects the model to be passed in the format of \"huggingface:<model_name>\".\n",
    "# The model name is the full name of the model in HuggingFace.\n",
    "# In the example below, the model is \"mistralai/Mistral-7B-Instruct-v0.3\".\n",
    "# The model is deployed as serverless inference endpoint in HuggingFace.\n",
    "hf_model = \"huggingface:mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "response = client.chat.completions.create(model=hf_model, messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2aad6-8603-4227-9566-778f714eb0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Groq expects the model to be passed in the format of \"groq:<model_name>\".\n",
    "# The model name is the full name of the model in Groq.\n",
    "# In the example below, the model is \"llama3-8b-8192\".\n",
    "groq_llama3_8b = \"groq:llama3-8b-8192\"\n",
    "# groq_llama3_70b = \"groq:llama3-70b-8192\"\n",
    "response = client.chat.completions.create(model=groq_llama3_8b, messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6819ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_tinyllama = \"ollama:tinyllama\"\n",
    "ollama_phi3mini = \"ollama:phi3:mini\"\n",
    "response = client.chat.completions.create(model=ollama_phi3mini, messages=messages, temperature=0.75)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94961b2bddedbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T15:31:39.472675Z",
     "start_time": "2024-07-04T15:31:38.283368Z"
    }
   },
   "outputs": [],
   "source": [
    "mistral_7b = \"mistral:open-mistral-7b\"\n",
    "response = client.chat.completions.create(model=mistral_7b, messages=messages, temperature=0.2)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611210a4dc92845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_gpt35 = \"openai:gpt-3.5-turbo\"\n",
    "response = client.chat.completions.create(model=openai_gpt35, messages=messages, temperature=0.75)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d033a-a580-4239-9176-27f3d53e7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nebius_model = \"nebius:Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "response = client.chat.completions.create(model=nebius_model, messages=messages, top_p=0.01)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321783ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fireworks_model = \"fireworks:accounts/fireworks/models/llama-v3p2-3b-instruct\"\n",
    "response = client.chat.completions.create(model=fireworks_model, messages=messages, temperature=0.75, presence_penalty=0.5, frequency_penalty=0.5)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e5ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "togetherai_model = \"together:meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    "response = client.chat.completions.create(model=togetherai_model, messages=messages, temperature=0.75, top_p=0.7, top_k=50)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf63a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_15_flash = \"google:gemini-1.5-flash\"\n",
    "response = client.chat.completions.create(model=gemini_15_flash, messages=messages, temperature=0.75)\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="examples/DeepseekPost.ipynb">
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe04b4f-7f26-44e9-a6cf-adc60d8b1a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "sys.path.append('../../aisuite')\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7200c4f4-0fb1-4630-a6fa-be0a54c424fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aisuite as ai\n",
    "\n",
    "client = ai.Client()\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Talk using Pirate English.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke in 1 line.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9aee9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      "Arrr, why be pirates awful at learnin' the alphabet? They always get lost at \"C\"!\n",
      "\n",
      "Reasoning content:\n",
      "Alright, the user wants me to talk in Pirate English and tell a joke in one line. Let's break this down. First, I need to switch my usual language style to pirate lingo. That means using words like \"Arrr,\" \"matey,\" \"ye,\" \"gold,\" \"parrot,\" etc. Next, the joke has to be concise, just one line. Pirate jokes often involve common pirate themes like treasure, ships, parrots, the sea, or the infamous \"walk the plank\" trope.\n",
      "\n",
      "I should brainstorm some pirate-related puns or wordplay. Maybe something with a parrot? Like, why don't pirates shower before they walk the plank? Because they'll just wash up on shore later. But that's two lines. Need to condense. Alternatively, a play on \"pieces of eight\" or \"gold.\" How about: \"Why don't pirates take up gardening? 'Cause the sea be weedin' 'em out!\" Wait, that might not be clear. Or \"What's a pirate's favorite letter? Arrr (R)!\" Classic, but maybe overused. Let's think of another. Maybe something with treasure. \"Why did the pirate's treasure go to school? To improve its 'arrr-ticulation'!\" Hmm, but that's a bit forced. Or \"What's a pirate's worst nightmare? A sunken chest with no booty!\" That's a bit better. Wait, the user wants a joke in one line. Let me check the example response. It was: \"Why don't pirates shower before walkin' the plank? 'Cause they'll just wash up on shore later!\" That's two lines, but maybe acceptable as one if structured properly. Alternatively, maybe a shorter pun. \"Why did the pirate buy an eyepatch? Because he couldn't afford an arrr-moire!\" Hmm, not sure. Alternatively, \"What's a pirate's favorite restaurant? Arr-rrrby's!\" Maybe too obscure. Alternatively, \"Why don't pirates fight on empty stomachs? 'Cause they prefer to battle ships!\" Battleships... That's a play on \"battle ships\" vs. \"battleships.\" Maybe that's a good one. Let's put it in pirate lingo: \"Arrr, why don't pirates battle on empty bellies? 'Cause they'd rather sink a ship than their supper!\" Hmm, not quite. Let's simplify. \"Why don't pirates starve? 'Cause they sail on a sea of 'soups'!\" No, that's not right. Wait, the classic one: \"Why couldn't the pirate learn the alphabet? He kept getting lost at 'C' (sea)!\" That's a good one. Let me pirate-ify it. \"Arrr, why can't the scurvy pirate learn his letters? 'Cause he be always lost at 'C' (sea)!\" That's one line. Alternatively, shorter: \"Why's a pirate bad at the alphabet? He sails past 'C'!\" Hmm. Maybe that's the one. Let me check if that's clear. \"C\" sounds like \"sea,\" so pirates are always at sea, hence can't get past C. Yeah, that works. Let me make sure it's in pirate talk. \"Arrr, why be pirates awful at learnin' the alphabet? They always get lost at 'C' (sea)!\" That's concise and fits the pirate theme. Alright, that should work.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(model=\"deepseek:deepseek-reasoner\", messages=messages, temperature=0.75)\n",
    "print(f\"Content:\\n{response.choices[0].message.content}\\n\\nReasoning content:\\n{response.choices[0].message.reasoning_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a97a740-a430-4aca-9950-64a2d7e7aa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      "Arrr, matey! Why did the pirate take a parrot? To have an extra 'R' for all his 'Arrr's!\n",
      "\n",
      "Reasoning content:\n",
      "Alright, so the user wants me to tell a joke in one line using Pirate English. Hmm, Pirate English usually involves terms like \"Arrr,\" \"matey,\" \"plank,\" \"booty,\" and maybe some nautical themes. I need to make it short and funny. Let me think of a pirate-related pun or wordplay. \n",
      "\n",
      "Maybe something about a pirate's favorite letter? That's a classic setup. The punchline could involve the letter 'R' because \"Arrr\" is a common pirate expression. So, \"Arrr, matey! Why did the pirate take a parrot on his ship? To have a bird's eye view and a bit o' chatter, savvy?\" Wait, that's a bit long. I need to keep it to one line.\n",
      "\n",
      "Let me simplify it. How about focusing on the letter 'R' since pirates often say \"Arrr.\" So, \"Arrr, matey! Why did the pirate take a parrot on his ship? To have an 'R' you in reserve, matey!\" Hmm, not sure if that's funny enough. Maybe the parrot is there to help with the 'R's. \n",
      "\n",
      "Wait, another angle: pirates love their treasure, so maybe the parrot is there to help find it. But I think the letter 'R' is a better pun. Let me tweak it. \"Arrr, matey! Why did the pirate take a parrot? To have an extra 'R' for all his 'Arrr's!\" Yeah, that works. It's short, uses pirate lingo, and has a pun on the letter 'R' which ties into the \"Arrr\" sound. I think that's a good one.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(model=\"groq:DeepSeek-R1-Distill-Llama-70b\", messages=messages, temperature=0.75)\n",
    "print(f\"Content:\\n{response.choices[0].message.content}\\n\\nReasoning content:\\n{response.choices[0].message.reasoning_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79228e3c-549d-4da2-9daf-16a3648cfe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      "Why did the pirate go to the eye doctor? He had a patchy vision, matey!\n",
      "\n",
      "Reasoning content:\n",
      "Okay, so I need to tell a joke in one line using pirate English. Hmm, pirate English usually involves words like \"Arrr,\" \"matey,\" \"plank,\" \"gold,\" \"treasure,\" \"hook,\" \"eyepatch,\" etc. Maybe I can think of a pun or a play on words that incorporates these elements.\n",
      "\n",
      "Let me brainstorm a bit. Pirates often talk about walking the plank, so that's a common phrase. Maybe something related to that. Or maybe something about their accessories, like hooks or eyepatches. Or perhaps something about treasure or gold.\n",
      "\n",
      "Wait, the user provided an example joke: \"Why did the pirate quit his job? Because he was sick o' all the arrr-guments!\" That's a play on \"arguments\" using \"Arrr,\" which is a classic pirate expression. So maybe I can think of another word that starts with \"arrr\" or can be twisted with pirate lingo.\n",
      "\n",
      "How about something like, \"Why did the pirate take a parrot on his ship?\" Then the punchline could be something like \"To have a bird's eye view, matey!\" But that's two lines. I need it to be one line.\n",
      "\n",
      "Wait, maybe \"Why did the pirate bury his treasure? Because he wanted arrr-guably the best spot!\" Hmm, not sure if that's funny. Or maybe \"Why did the pirate get kicked off the ship? He kept walking the plank!\" But that doesn't really have a pun.\n",
      "\n",
      "Let me think of another approach. Maybe using \"hook\" as a pun. \"Why did the pirate get a hook? Because he wanted to catch more opportunities, matey!\" That's a bit forced.\n",
      "\n",
      "Alternatively, \"Why did the pirate go to the dentist? He had a treasure-ble toothache!\" That might work. Or \"Why did the pirate refuse to play poker? Because he knew the cards were marked, savvy!\"\n",
      "\n",
      "Hmm, I think I can come up with something better. Maybe using \"Arrr\" in the punchline. How about \"Why did the pirate become a teacher? Because he was great at arrr-ticulation!\" Wait, articulation? That's a stretch.\n",
      "\n",
      "Wait, maybe \"Why did the pirate go to the eye doctor? He had a patchy vision, matey!\" That's a play on \"eyepatch\" and \"patchy vision.\" Yeah, that could work.\n",
      "\n",
      "So, putting it all together, the joke would be: \"Why did the pirate go to the eye doctor? He had a patchy vision, matey!\" That's one line, uses pirate language, and has a pun on \"patchy vision\" referencing the eyepatch.\n",
      "\n",
      "Alternatively, maybe \"Why did the pirate go to the optometrist? To get his eyepatch checked, arrr!\" But that's a bit direct without a pun.\n",
      "\n",
      "I think the patchy vision one is better because it ties the eyepatch to vision problems, making it a play on words. So that should be a good one-liner pirate joke.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(model=\"sambanova:DeepSeek-R1-Distill-Llama-70B\", messages=messages, temperature=0.75)\n",
    "print(f\"Content:\\n{response.choices[0].message.content}\\n\\nReasoning content:\\n{response.choices[0].message.reasoning_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26348c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      "Arrr, why did the pirate quit his job? Because he realized he didn't have a plank... but he had a plan!\n",
      "\n",
      "Reasoning content:\n",
      "Okay, so the user wants me to tell a joke in one line using Pirate English.Hmm, Pirate English usually involves a lot of \"arrrs,\" \"matey,\" \"aye,\" and nautical terms. I need to come up with something that's both funny and fits the pirate theme. Maybe something about pirate life or common phrases pirates use.\n",
      "\n",
      "Let me think about pirate-related puns. Oh, how about something with \"plank\"? Pirates make people walk the plank, right? So maybe a play on words with \"plank\" and something else. Hmm, \"Plank\" and \"plan\" sound similar. Maybe a pirate saying they don't have a plank, but they have a plan instead.\n",
      "\n",
      "Wait, that could work. \"Why did the pirate quit his job? Because he realized he didn't have a plank... but he had a plan!\" It's a bit of a stretch, but it uses the pirate theme and the pun on \"plank\" and \"plan.\"\n",
      "\n",
      "I think that's a solid one-liner. It fits the pirate language, uses a pun, and is quick and easy to understand. Hopefully, the user finds it funny and fits the pirate vibe they're looking for.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(model=\"together:deepseek-ai/DeepSeek-R1-Distill-Llama-70B\", messages=messages, temperature=0.75)\n",
    "print(f\"Content:\\n{response.choices[0].message.content}\\n\\nReasoning content:\\n{response.choices[0].message.reasoning_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36c6d86-5af5-4b4b-a166-869e4bfe5777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="examples/llm_reasoning.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39a806c-02a3-4a2d-8c51-f1ab1ea79d2e",
   "metadata": {},
   "source": [
    "# LLM Reasoning\n",
    "\n",
    "This notebook compares how LLMs from different Generative AI providers perform on three examples that can show issues with LLM reasoning:\n",
    "\n",
    "* [The Reversal Curse](https://github.com/lukasberglund/reversal_curse) shows that LLMs trained on \"A is B\" fail to learn \"B is A\".\n",
    "* [How many r's in the word strawberry?](https://x.com/karpathy/status/1816637781659254908) shows \"the weirdness of LLM Tokenization\".  \n",
    "* [Which number is bigger, 9.11 or 9.9?](https://x.com/DrJimFan/status/1816521330298356181) shows that \"LLMs are alien beasts.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e413bd-983c-42a0-9580-96fedc7b1275",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../.env.sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d843e36-7de6-4726-8a39-c5dcd3c7cc11",
   "metadata": {},
   "source": [
    "Make sure your ~/.env file (copied from the .env.sample file above) has the API keys of the LLM providers to compare set before running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c966895-1a63-4922-80b7-5a20e47f29de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../aisuite')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d5c5be-1085-4252-9d5e-80b50961484b",
   "metadata": {},
   "source": [
    "## Specify LLMs to Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3d5ef-b1c9-48dd-9b89-30799fd4b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aisuite as ai\n",
    "\n",
    "client = ai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a904f-fef0-4f25-b3ed-41085bf0f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "llms = [\n",
    "        \"anthropic:claude-3-5-sonnet-20240620\",\n",
    "        \"aws:meta.llama3-1-8b-instruct-v1:0\",\n",
    "        \"groq:llama3-8b-8192\",\n",
    "        \"groq:llama3-70b-8192\",\n",
    "        \"huggingface:mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        \"openai:gpt-3.5-turbo\",\n",
    "       ]\n",
    "\n",
    "def compare_llm(messages):\n",
    "    execution_times = []\n",
    "    responses = []\n",
    "    for llm in llms:\n",
    "        start_time = time.time()\n",
    "        response = client.chat.completions.create(model=llm, messages=messages)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        responses.append(response.choices[0].message.content.strip())\n",
    "        execution_times.append(execution_time)\n",
    "        print(f\"{llm} - {execution_time:.2f} seconds: {response.choices[0].message.content.strip()}\")\n",
    "    return responses, execution_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3e8aa2-4ff4-485b-93d9-4a6f22d62e67",
   "metadata": {},
   "source": [
    "## The Reversal Curse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c4a8ef-e23b-4d4a-8561-3e5a2a866bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who is Tom Cruise's mother?\"},\n",
    "]\n",
    "\n",
    "responses, execution_times = compare_llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f7f42-2adb-4903-ab17-3143a5d950ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def display(llms, execution_times, responses):\n",
    "    data = {\n",
    "        'Provider:Model Name': llms,\n",
    "        'Execution Time': execution_times,\n",
    "        'Model Response ': responses\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.index = df.index + 1\n",
    "    styled_df = df.style.set_table_styles(\n",
    "        [{'selector': 'th', 'props': [('text-align', 'center')]}, \n",
    "         {'selector': 'td', 'props': [('text-align', 'center')]}]\n",
    "    ).set_properties(**{'text-align': 'center'})\n",
    "    \n",
    "    return styled_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2359ad5-9f0b-4bd6-9838-54df91de0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(llms, execution_times, responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f6cca-7f34-4a91-aab0-070560640033",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who is Mary Lee Pfeiffer's son?\"},\n",
    "]\n",
    "\n",
    "responses, execution_times = compare_llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7704d-a187-41bc-b119-c94461d0ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(llms, execution_times, responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8e0fb-17f0-4781-bf6a-c23ac86922ad",
   "metadata": {},
   "source": [
    "## How many r's in the word strawberry?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537871e-68b6-44c3-886a-d3ebe7a692c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How many r's in the word strawberry?\"},\n",
    "]\n",
    "\n",
    "responses, execution_times = compare_llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678e393-4967-49f1-9e0f-251471dc92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(llms, execution_times, responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3fb5f-a173-4a33-b843-65df6d1086f9",
   "metadata": {},
   "source": [
    "## Which number is bigger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf2fd6-f63a-4f9b-af15-1df25590e4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Which number is bigger, 9.11 or 9.9?\"},\n",
    "]\n",
    "\n",
    "responses, execution_times = compare_llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa14ed1-c83b-4c8f-bb14-d318bf0c9a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(llms, execution_times, responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b213a-b7bf-4cce-8c30-a8408454370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Which number is bigger, 9.11 or 9.9? Think step by step.\"},\n",
    "]\n",
    "\n",
    "responses, execution_times = compare_llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3fb8fc-a7a2-47d3-9db2-792f03cc47c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(llms, execution_times, responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66987d26-4245-4de1-816f-fa57475101f3",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "1. Not all LLMs are created equal - not even all Llama 3 (or 3.1) are created equal (by different providers).\n",
    "2. Ask LLM to think step by step may help improve its reasoning.\n",
    "3. The way tokenization works in LLM could lead to a lot of weirdness in LLM (see AK's awesome [video](https://www.youtube.com/watch?v=zduSFxRajkE) for a deep dive).\n",
    "4. A more comprehensive benchmark would be desired, but a quick LLM comparison like shown here can be the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e13c90-3680-4f1d-8f65-768a78b7adb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="examples/QnA_with_pdf.ipynb">
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyMuPDF requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "sys.path.append('../aisuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aisuite as ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def configure_environment(additional_env_vars=None):\n",
    "    \"\"\"\n",
    "    Load environment variables from .env file and apply any additional variables.\n",
    "    :param additional_env_vars: A dictionary of additional environment variables to apply.\n",
    "    \"\"\"\n",
    "    # Load from .env file if available\n",
    "    load_dotenv(find_dotenv())\n",
    "\n",
    "    # Apply additional environment variables\n",
    "    if additional_env_vars:\n",
    "        for key, value in additional_env_vars.items():\n",
    "            os.environ[key] = value\n",
    "\n",
    "# Define additional API keys and credentials\n",
    "additional_keys = {}\n",
    "\n",
    "# Configure environment\n",
    "configure_environment(additional_env_vars=additional_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and extracted text from pdf.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import fitz\n",
    "from io import BytesIO\n",
    "\n",
    "# Link to paper in pdf format on the cost of avocados.\n",
    "pdf_path = \"https://arxiv.org/pdf/2104.04649\"\n",
    "pdf_text = \"\"\n",
    "# Download PDF and load it into memory\n",
    "response = requests.get(pdf_path)\n",
    "if response.status_code == 200:\n",
    "    pdf_data = BytesIO(response.content)  # Load PDF data into BytesIO\n",
    "    # Open PDF from memory using fitz\n",
    "    with fitz.open(stream=pdf_data, filetype=\"pdf\") as pdf:\n",
    "        text = \"\"\n",
    "        for page_num in range(pdf.page_count):\n",
    "            page = pdf[page_num]\n",
    "            pdf_text += page.get_text(\"text\")  # Extract text\n",
    "            pdf_text += \"\\n\" + \"=\"*50 + \"\\n\"  # Separator for each page\n",
    "    print(\"Downloaded and extracted text from pdf.\")\n",
    "else:\n",
    "    print(f\"Failed to download PDF: {response.status_code}\")\n",
    "\n",
    "question = \"Is the price of organic avocados higher than non-organic avocados? What has been the trend?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ai.Client()\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question only based on the below text.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Answer the question based on the following text:\\n\\n{pdf_text}\\n\\nQuestion: {question}\\n\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the text, yes, the price of organic avocados is consistently higher than conventional (non-organic) avocados. Specifically:\n",
      "\n",
      "1. Figure 2 shows a bar chart comparing average prices of conventional and organic avocados from 2015-2020. The text states that \"the average price of organic avocados is generally always higher than conventional avocados.\"\n",
      "\n",
      "2. Figure 3, a pie chart, illustrates that \"Nearly 58% of organic avocado sales averaged $1.80 per avocado and roughly 42% of conventional avocados averaged $1.30 per avocado.\"\n",
      "\n",
      "3. In the conclusion section, the text explicitly states: \"The price of organic avocados is on average 35-40% higher than conventional avocados.\"\n",
      "\n",
      "Regarding the trend, while the text doesn't provide detailed information on price trends over time, Figure 2 shows the average prices for both organic and conventional avocados from 2015-2020, indicating that this price difference has been consistent over that period.\n"
     ]
    }
   ],
   "source": [
    "anthropic_claude_3_opus = \"anthropic:claude-3-5-sonnet-20240620\"\n",
    "response = client.chat.completions.create(model=anthropic_claude_3_opus, messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, according to the analysis presented in the text, the price of organic avocados is higher\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hf_model = \"huggingface:mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "response = client.chat.completions.create(model=hf_model, messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, yes, the price of organic avocados is on average 35-40% higher than conventional avocados.\n",
      "\n",
      "As for the trend, it can be observed that there is a steady growth in sales volume year after year for both conventional and organic avocados.\n",
      "\n",
      "However, in terms of price, the average price of organic avocados has been consistently higher than conventional avocados over the years. This can also be seen in Figure 2, which shows that the average price of organic avocados is generally always higher than conventional avocados.\n"
     ]
    }
   ],
   "source": [
    "fireworks_model = \"fireworks:accounts/fireworks/models/llama-v3p2-3b-instruct\"\n",
    "response = client.chat.completions.create(model=fireworks_model, messages=messages, temperature=0.75, presence_penalty=0.5, frequency_penalty=0.5)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the price of organic avocados is higher than non-organic avocados. According to the text, the average price of organic avocados is generally 35-40% higher than conventional avocados.\n"
     ]
    }
   ],
   "source": [
    "nebius_model = \"nebius:meta-llama/Meta-Llama-3.1-8B-Instruct-fast\"\n",
    "response = client.chat.completions.create(model=nebius_model, messages=messages, top_p=0.01)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="examples/simple_tool_calling.ipynb">
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "sys.path.append('../../aisuite')\n",
    "\n",
    "# Load from .env file if available\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a request to model without tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisuite import Client\n",
    "\n",
    "client = Client()\n",
    "# Configuring Azure. Rest all providers use environment variables for their parameters.\n",
    "client.configure({\"azure\" : {\n",
    "  \"api_key\": os.environ[\"AZURE_API_KEY\"],\n",
    "  \"base_url\": \"https://aisuite-mistral-large-2407.westus3.models.ai.azure.com/v1/\",\n",
    "}})\n",
    "# model = \"anthropic:claude-3-5-sonnet-20241022\"\n",
    "# model = \"aws:mistral.mistral-7b-instruct-v0:2\"\n",
    "# model = \"azure:aisuite-mistral-large\"\n",
    "# model = \"cohere:command-r-plus\"\n",
    "# model = \"deepseek:deepseek-chat\"\n",
    "# model = \"fireworks:accounts/fireworks/models/llama-v3p1-405b-instruct\"\n",
    "# model = \"google:gemini-1.5-pro-002\"\n",
    "# model = \"groq:llama-3.3-70b-versatile\"\n",
    "# model = \"huggingface:meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model = \"mistral:mistral-large-latest\"\n",
    "# model = \"nebius:\"\n",
    "# model = \"ollama:\"\n",
    "# model = \"sambanova:Meta-Llama-3.3-70B-Instruct\"\n",
    "# model = \"together:meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
    "# model = \"watsonx:\"\n",
    "model = \"xai:grok-2-latest\"\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What is the current temperature in San Francisco in Celsius?\"}]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model, messages=messages)\n",
    "\n",
    "print(\"For model: \" + model)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equip model with tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock tool functions.\n",
    "def get_current_temperature(location: str, unit: str):\n",
    "    # Simulate fetching temperature from an API\n",
    "    return {\"temperature\": 72}\n",
    "\n",
    "def get_rain_probability(location: str):\n",
    "    # Simulate fetching rain probability\n",
    "    return {\"location\": location, \"probability\": 40}\n",
    "\n",
    "# Function to get the available tools (functions) to provide to the model\n",
    "# Note: we could use decorators or utils from OpenAI to generate this.\n",
    "def get_available_tools():\n",
    "    return [\n",
    "        {   \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_temperature\",\n",
    "                \"description\": \"Get the current temperature for a specific location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g., San Francisco, CA\"\n",
    "                        },\n",
    "                        \"unit\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"Celsius\", \"Fahrenheit\"],\n",
    "                            \"description\": \"The temperature unit to use.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\", \"unit\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_rain_probability\",\n",
    "                \"description\": \"Get the probability of rain for a specific location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g., San Francisco, CA\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process tool calls and get the result\n",
    "def handle_tool_call(tool_call):\n",
    "    function_name = tool_call.function.name\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    # Map function names to actual tool function implementations\n",
    "    tools_map = {\n",
    "        \"get_current_temperature\": get_current_temperature,\n",
    "        \"get_rain_probability\": get_rain_probability,\n",
    "    }\n",
    "    return tools_map[function_name](**arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format tool response as a message\n",
    "def create_tool_response_message(tool_call, tool_result):\n",
    "    return {\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": tool_call.id,\n",
    "        \"name\": tool_call.function.name,\n",
    "        \"content\": json.dumps(tool_result)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the model with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "sys.path.append('../../aisuite')\n",
    "\n",
    "# Load from .env file if available\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "from aisuite import Client\n",
    "\n",
    "client = Client()\n",
    "client.configure({\"azure\" : {\n",
    "  \"api_key\": os.environ[\"AZURE_API_KEY\"],\n",
    "  \"base_url\": \"https://aisuite-mistral-large-2407.westus3.models.ai.azure.com/v1/\",\n",
    "}})\n",
    "\n",
    "# model = \"anthropic:claude-3-5-sonnet-20241022\"\n",
    "# model = \"aws:mistral.mistral-7b-instruct-v0:2\"\n",
    "# model = \"azure:aisuite-mistral-large\"\n",
    "# model = \"cohere:command-r-plus\"\n",
    "# model = \"deepseek:deepseek-chat\"\n",
    "# model = \"fireworks:accounts/fireworks/models/llama-v3p1-405b-instruct\"\n",
    "# model = \"google:gemini-1.5-pro-002\"\n",
    "# model = \"groq:llama-3.3-70b-versatile\"\n",
    "# model = \"huggingface:meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model = \"mistral:mistral-large-latest\"\n",
    "# model = \"nebius:\"\n",
    "# model = \"ollama:\"\n",
    "# model = \"sambanova:Meta-Llama-3.3-70B-Instruct\"\n",
    "# model = \"together:meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
    "# model = \"watsonx:\"\n",
    "model = \"xai:grok-2-latest\"\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What is the current temperature in San Francisco in Celsius?\"}]\n",
    "\n",
    "tools = get_available_tools()\n",
    "\n",
    "# Make the initial request to OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    model=model, messages=messages, tools=tools)\n",
    "\n",
    "print(response)\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process tool calls - Parse tool name, args, and call the function. Pass the result to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if response.choices[0].message.tool_calls:\n",
    "    for tool_call in response.choices[0].message.tool_calls:\n",
    "        tool_result = handle_tool_call(tool_call)\n",
    "        print(tool_result)\n",
    "\n",
    "        messages.append(response.choices[0].message) # Model's function call message\n",
    "        messages.append(create_tool_response_message(tool_call, tool_result))\n",
    "        # Send the tool response back to the model\n",
    "        final_response = client.chat.completions.create(\n",
    "            model=model, messages=messages, tools=tools)\n",
    "        print(final_response.choices[0].message)\n",
    "        \n",
    "        # Output the final response from the model\n",
    "        print(final_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</file>

<file path="examples/tool_calling_abstraction.ipynb">
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9efdda2f-e3ab-4ec3-9b04-3ebea6fdf4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "sys.path.append('../../aisuite')\n",
    "\n",
    "# Load from .env file if available\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7604862",
   "metadata": {},
   "source": [
    "### Define the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaba7cb2-29de-4552-8fd5-8b966fbc0cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def will_it_rain(location: str, time_of_day: str):\n",
    "    \"\"\"Check if it will rain in a location at a given time today.\n",
    "\n",
    "    Args:\n",
    "        location (str): Name of the city\n",
    "        time_of_day (str): Time of the day in HH:MM format.\n",
    "    \"\"\"\n",
    "    return \"YES\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf943639",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090dc2d1",
   "metadata": {},
   "source": [
    "### Using OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29537ae1",
   "metadata": {},
   "source": [
    "### JSON spec for the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1a7196a-e5f2-4016-8dca-eca804ef18ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"will_it_rain\",\n",
    "        \"description\": \"Check if it will rain in a location at a given time today\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Name of the city\"\n",
    "                },\n",
    "                \"time_of_day\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Time of the day in HH:MM format.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\", \"time_of_day\"]\n",
    "        }\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359d145",
   "metadata": {},
   "source": [
    "### Send user request to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b522391-5a82-4d27-bbad-9db91e531815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"I live in San Francisco. Can you check for weather \"\n",
    "               \"and plan an outdoor picnic for me at 2pm?\"\n",
    "}]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\", messages=messages, tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc47b536",
   "metadata": {},
   "source": [
    "### Process tool call response, Execute the tool & call the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d281fe8e-9d87-4150-be4d-f01a2525edf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like it will rain in San Francisco at 2 PM today. It's probably not the best day for an outdoor picnic. Would you like to consider indoor activities or reschedule the picnic for another day?\n"
     ]
    }
   ],
   "source": [
    "response2 = None\n",
    "if response.choices[0].message.tool_calls:\n",
    "    tool_call = response.choices[0].message.tool_calls[0]\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    result = will_it_rain(args[\"location\"], args[\"time_of_day\"])\n",
    "    messages.append(response.choices[0].message)\n",
    "    messages.append({\n",
    "        \"role\": \"tool\", \"tool_call_id\": tool_call.id, \"content\": str(result)\n",
    "    })\n",
    "\n",
    "    response2 = client.chat.completions.create(\n",
    "        model=\"gpt-4o\", messages=messages, tools=tools)\n",
    "    print(response2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81fb905",
   "metadata": {},
   "source": [
    "### Optionally, continue the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6de5c21-2983-4539-a4a2-cdd76071fdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you're set on having the picnic despite the rain, here are a few suggestions to make it enjoyable:\n",
      "\n",
      "1. **Location:** Consider a location with some shelter, like a gazebo in a park, or find a picnic spot with large trees for some natural cover. Alternatively, you might want to think about an indoor space with large windows where you can enjoy the view without getting wet.\n",
      "\n",
      "2. **Packing Essentials:**\n",
      "   - Bring waterproof blankets or tarps to sit on.\n",
      "   - Pack an umbrella or a rain poncho to stay dry.\n",
      "   - Use waterproof containers for your food to keep everything dry.\n",
      "   - Bring extra towels and a change of clothes.\n",
      "\n",
      "3. **Food Ideas:** Opt for warm, hearty meals that are comforting on a rainy day, such as soup in a thermos or grilled sandwiches.\n",
      "\n",
      "4. **Activities:** Plan indoor-friendly games or activities you can enjoy sheltered from the rain, like card games or board games.\n",
      "\n",
      "5. **Safety First:** Check the weather forecast regularly in case of any severe weather warnings.\n",
      "\n",
      "Enjoy your rainy-day picnic!\n"
     ]
    }
   ],
   "source": [
    "messages.append(response2.choices[0].message)\n",
    "messages.append({\n",
    "    \"role\": \"user\", \"content\": \"Schedule it despite the rain\"\n",
    "})\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d5fd91",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897b9aa2",
   "metadata": {},
   "source": [
    "### Using aisuite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b587445",
   "metadata": {},
   "source": [
    "### Call the model with tools. Tool call is handled internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b74257b-8d53-4326-8ea1-d4a7f89c0e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is expected to rain in San Francisco at 2 PM today, so it might not be the best time for an outdoor picnic. You might want to consider indoor alternatives or plan for another day when the weather is more favorable for outdoor activities. If you have any other plans in mind or need further assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from aisuite import Client\n",
    "\n",
    "client = Client()\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"I live in San Francisco. Can you check for weather \"\n",
    "               \"and plan an outdoor picnic for me at 2pm?\"\n",
    "}]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"openai:gpt-4o\", messages=messages, tools=[will_it_rain], max_turns=2\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f0327",
   "metadata": {},
   "source": [
    "### Optionally, continue the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1736dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! If you'd like to go ahead and plan an outdoor picnic despite the rain, here are a few tips to help you enjoy your experience:\n",
      "\n",
      "1. **Choose a Covered Location**: Try to find a spot with a shelter, like a gazebo or a pavilion, in one of San Francisco's parks. These can provide good protection from the rain.\n",
      "\n",
      "2. **Prepare for Wet Weather**:\n",
      "   - Bring waterproof blankets or tarps to sit on.\n",
      "   - Pack umbrellas and raincoats to stay dry.\n",
      "   - Use waterproof containers and bags to keep your picnic items protected.\n",
      "\n",
      "3. **Select Comforting Foods**: Warm beverages in thermoses and foods that are enjoyable when slightly cooler, such as sandwiches, can be comforting on a rainy day.\n",
      "\n",
      "4. **Stay Safe**: Be cautious of slippery surfaces and keep an eye on kids or pets if they are joining you.\n",
      "\n",
      "5. **Enjoy the Atmosphere**: Rain can create a cozy and calming environment. Embrace the natural sounds and sights of the rain.\n",
      "\n",
      "6. **Plan Indoor Activities**: Bring board games or books in case the rain becomes too heavy.\n",
      "\n",
      "Have fun, and make the most of your unique outdoor picnic experience! If you need more help with planning or have any other questions, let me know.\n"
     ]
    }
   ],
   "source": [
    "messages.extend(response.choices[0].intermediate_messages)\n",
    "messages.append({\n",
    "    \"role\": \"user\", \"content\": \"Schedule it despite the rain\"\n",
    "})\n",
    "response = client.chat.completions.create(\n",
    "    model=\"openai:gpt-4o\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf09c0a",
   "metadata": {},
   "source": [
    "### Using, other providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c266c68-095b-4625-8de5-70960a117f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but it looks like it will be raining in San Francisco at 2:00 PM today. This might not be the best conditions for an outdoor picnic. However, I can suggest a few alternatives:\n",
      "\n",
      "1. Reschedule: We could check the weather for a different time today or another day this week. Would you like me to check a different time or date?\n",
      "\n",
      "2. Indoor picnic: You could have an indoor picnic at home or find a covered area in a park with picnic shelters.\n",
      "\n",
      "3. Rain-ready picnic: If you're up for an adventure, you could prepare for a rainy day picnic with appropriate gear like umbrellas, waterproof blankets, and covered food containers.\n",
      "\n",
      "4. Alternative activity: We could look into indoor activities or attractions in San Francisco that you might enjoy instead.\n",
      "\n",
      "What would you prefer to do? If you'd like to try for a different time or date, please let me know, and I'll be happy to check the weather again.\n"
     ]
    }
   ],
   "source": [
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"I live in San Francisco. Can you check for weather \"\n",
    "               \"and plan an outdoor picnic for me at 2pm?\"\n",
    "}]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"anthropic:claude-3-5-sonnet-20240620\", messages=messages, tools=[will_it_rain], max_turns=2\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a8c5ab-3a9f-4bf1-8174-aba313a00085",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"groq:llama3-8b-8192\", messages=messages, tools=[will_it_rain], max_turns=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c5192-ddc6-4933-a658-32c8303206ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file>

<file path="guides/anthropic.md">
# Anthropic

To use Anthropic with `aisuite` you will need to [create an account](https://console.anthropic.com/login). Once logged in, go to the [API Keys](https://console.anthropic.com/settings/keys)
and click the "Create Key" button and export that key into your environment.


```shell
export ANTHROPIC_API_KEY="your-anthropic-api-key"
```

## Create a Chat Completion

Install the `anthropic` python client

Example with pip
```shell
pip install anthropic
```

Example with poetry
```shell
poetry add anthropic
```

In your code:
```python
import aisuite as ai
client = ai.Client()


provider = "anthropic"
model_id = "claude-3-5-sonnet-20241022"

messages = [
    {"role": "system", "content": "Respond in Pirate English."},
    {"role": "user", "content": "Tell me a joke."},
]

response = client.chat.completions.create(
    model=f"{provider}:{model_id}",
    messages=messages,
)

print(response.choices[0].message.content)
```

Happy coding! If you would like to contribute, please read our [Contributing Guide](../CONTRIBUTING.md).
</file>

<file path="guides/aws.md">
# AWS

To use AWS Bedrock with `aisuite` you will need to create an AWS account and
navigate to https://console.aws.amazon.com/bedrock/home. This route
will be redirected to your default region. In this example the region has been set to
`us-west-2`. Anywhere the region is specified can be replaced with your desired region.

Navigate to the [overview](https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/overview) page
directly or by clicking on the `Get started` link.

## Foundation Model Access

You will first need to give your AWS account access to the foundation models by
visiting the [modelaccess](https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/modelaccess)
page to enable the models you would like to use. 

After enabling the foundation models, navigate to [providers page](https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/providers) 
and select the provider of the model you would like to use. From this page select the specific model you would like to use and 
make note of the `Model ID` (currently located near the bottom) this will be used when using the chat completion example below.

Once that has been enabled set your Access Key and Secret in the env variables:

```shell
export AWS_ACCESS_KEY="your-access-key"
export AWS_SECRET_KEY="your-secret-key"
export AWS_REGION="region-name" 
```
*Note: AWS_REGION is optional, a default of `us-west-2` has been set for easy of use*

## Create a Chat Completion

Install the boto3 client using your package installer

Example with pip
```shell
pip install boto3
```

Example with poetry
```shell
poetry add boto3
```

In your code:
```python
import aisuite as ai
client = ai.Client()


provider = "aws"
model_id = "meta.llama3-1-405b-instruct-v1:0" # Model ID from above

messages = [
    {"role": "system", "content": "Respond in Pirate English."},
    {"role": "user", "content": "Tell me a joke."},
]

response = client.chat.completions.create(
    model=f"{provider}:{model_id}",
    messages=messages,
)

print(response.choices[0].message.content)
```

Happy coding! If you would like to contribute, please read our [Contributing Guide](../CONTRIBUTING.md).
</file>

<file path="guides/azure.md">
# Azure AI

To use Azure AI with the `aisuite` library, you'll need to set up an Azure account and configure your environment for Azure AI services.

## Create an Azure Account and deploy a model from AI Studio

1. Visit [Azure Portal](https://portal.azure.com/) and sign up for an account if you don't have one.
2. Create a project and resource group.
3. Choose a model from https://ai.azure.com/explore/models and deploy it. You can choose serverless deployment option.
4. Give a deployment name. Lets say you choose to deploy Mistral-large-2407. You could leave the deployment names as "mistral-large-2407" or give a custom name.
5. You can see the deployment from project/deployment option. Note the Target URI from the Endpoint panel. It should look something like this - "https://aisuite-Mistral-large-2407.westus3.models.ai.azure.com".
6. Also note, that is provides a Chat completion URL. It should look like this - https://aisuite-Mistral-large-2407.westus3.models.ai.azure.com/v1/chat/completions


## Obtain Necessary Details & set environment variables.

After creating your deployment, you'll need to gather the following information:

1. API Key: Found in the "Keys and Endpoint" section of your Azure OpenAI resource.
2. Base URL: This can be obtained from your deployment details. It will look something like this - `https://aisuite-Mistral-large-2407.westus3.models.ai.azure.com/v1/`
3. API Version: Optional configuration and mainly introduced for Azure OpenAI services. Once specified, the `api-version` query parameters will be added in the end of the API request.


Set the following environment variables:

```shell
export AZURE_API_KEY="your-api-key"
export AZURE_BASE_URL="https://deployment-name.region-name.models.ai.azure.com/v1"
export AZURE_API_VERSION="=2024-08-01-preview"
```

## Create a Chat Completion

With your account set up and environment configured, you can send a chat completion request:

```python
import aisuite as ai

# Either set the environment variables or set the below two parameters.
# Setting the params in ai.Client() will override the values from environment vars.
client = ai.Client(
    base_url=os.environ["AZURE_OPENAI_BASE_URL"],
    api_key=os.environ["AZURE_OPENAI_API_KEY"],
    api_version=os.environ["AZURE_API_VERSION"]
)

model = "azure:aisuite-Mistral-large-2407"  # Replace with your deployment name.
# The model name must match the deployment name in the base-url.

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What's the weather like today?"},
]

response = client.chat.completions.create(
    model=model,
    messages=messages,
)

print(response.choices[0].message.content)
```

Happy coding! If you would like to contribute, please read our [Contributing Guide](../CONTRIBUTING.md).
</file>

<file path="guides/cerebras.md">
# Cerebras AI Suite Provider Guide

## About Cerebras

At Cerebras, we've developed the world's largest and fastest AI processor, the Wafer-Scale Engine-3 (WSE-3). The Cerebras CS-3 system, powered by the WSE-3, represents a new class of AI supercomputer that sets the standard for generative AI training and inference with unparalleled performance and scalability.

With Cerebras as your inference provider, you can:
- Achieve unprecedented speed for AI inference workloads
- Build commercially with high throughput
- Effortlessly scale your AI workloads with our seamless clustering technology

Our CS-3 systems can be quickly and easily clustered to create the largest AI supercomputers in the world, making it simple to place and run the largest models. Leading corporations, research institutions, and governments are already using Cerebras solutions to develop proprietary models and train popular open-source models.

Want to experience the power of Cerebras? Check out our [website](https://cerebras.net) for more resources and explore options for accessing our technology through the Cerebras Cloud or on-premise deployments!

> [!NOTE]  
> This SDK has a mechanism that sends a few requests to `/v1/tcp_warming` upon construction to reduce the TTFT. If this behaviour is not desired, set `warm_tcp_connection=False` in the constructor.
>
> If you are repeatedly reconstructing the SDK instance it will lead to poor performance. It is recommended that you construct the SDK once and reuse the instance if possible.

## Documentation

For the most comprehensive and up-to-date Cerebras Inference docs, please visit [inference-docs.cerebras.ai](https://inference-docs.cerebras.ai).

## Usage
Get an API Key from [cloud.cerebras.ai](https://cloud.cerebras.ai/) and add it to your environment variables:

```shell
export CEREBRAS_API_KEY="your-cerebras-api-key"
```

Use the python client.

```python
import aisuite as ai
client = ai.Client()

messages = [
    {"role": "system", "content": "Respond in Pirate English."},
    {"role": "user", "content": "Tell me a joke."},
]

response = client.chat.completions.create(
    model="cerebras:llama3.1-8b",
    messages=messages,
    temperature=0.75
)
print(response.choices[0].message.content)

```

## Requirements

Python 3.8 or higher.
</file>

<file path="guides/cohere.md">
# Cohere

To use Cohere with `aisuite`, youll need an [Cohere account](https://cohere.com/). After logging in, go to the [API Keys](https://dashboard.cohere.com/api-keys) section in your account settings, agree to the terms of service, connect your card, and generate a new key. Once you have your key, add it to your environment as follows:

```shell
export CO_API_KEY="your-cohere-api-key"
```

## Create a Chat Completion

Install the `cohere` Python client:

Example with pip:
```shell
pip install cohere
```

Example with poetry:
```shell
poetry add cohere
```

In your code:
```python
import aisuite as ai

client = ai.Client()

provider = "cohere"
model_id = "command-r-plus-08-2024"

messages = [
    {"role": "user", "content": "Hi, how are you?"}
]

response = client.chat.completions.create(
    model=f"{provider}:{model_id}",
    messages=messages,
)

print(response.choices[0].message.content)
```

Happy coding! If youd like to contribute, please read our [Contributing Guide](CONTRIBUTING.md).
</file>

<file path="guides/deepseek.md">
# DeepSeek

To use DeepSeek with `aisuite`, youll need an [DeepSeek account](https://platform.deepseek.com). After logging in, go to the [API Keys](https://platform.deepseek.com/api_keys) section in your account settings and generate a new key. Once you have your key, add it to your environment as follows:

```shell
export DEEPSEEK_API_KEY="your-deepseek-api-key"
```

## Create a Chat Completion

(Note: The DeepSeek uses an API format consistent with OpenAI, hence why we need to install OpenAI, there is no DeepSeek Library at least not for now)

Install the `openai` Python client:

Example with pip:
```shell
pip install openai
```

Example with poetry:
```shell
poetry add openai
```

In your code:
```python
import aisuite as ai
client = ai.Client()

provider = "deepseek"
model_id = "deepseek-chat"

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Whats the weather like in San Francisco?"},
]

response = client.chat.completions.create(
    model=f"{provider}:{model_id}",
    messages=messages,
)

print(response.choices[0].message.content)
```

Happy coding! If youd like to contribute, please read our [Contributing Guide](../CONTRIBUTING.md).
</file>

<file path="guides/google.md">
# Google (Vertex) AI

To use Google (Vertex) AI with the `aisuite` library, you'll first need to create a Google Cloud account and set up your environment to work with Google Cloud.

## Create a Google Cloud Account and Project

Google Cloud provides in-depth [documentation](https://cloud.google.com/vertex-ai/docs/start/cloud-environment) on getting started with their platform, but here are the basic steps:

### Create your account.

Visit [Google Cloud](https://cloud.google.com/free) and follow the instructions for registering a new account. If you already have an account with Google Cloud, sign in and skip to the next step.

### Create a new project and enable billing.

Once you have an account, you can create a new project. Visit the [project selector page](https://console.cloud.google.com/projectselector2/home/dashboard) and click the "New Project" button. Give your project a name and click "Create Project." Your project will be created and you will be redirected to the project dashboard.

Now that you have a project, you'll need to enable billing. Visit the [how-to page](https://cloud.google.com/billing/docs/how-to/verify-billing-enabled#confirm_billing_is_enabled_on_a_project) for billing enablement instructions.

### Set your project ID in an environment variable.

Set the `GOOGLE_PROJECT_ID` environment variable to the ID of your project. You can find the Project ID by visiting the project dashboard in the "Project Info" section toward the top of the page.

### Set your preferred region in an environment variable.

Set the `GOOGLE_REGION` environment variable. You can find the region by going to Project Dashboard under VertexAI side navigation menu, and then scrolling to the bottom of the page.

## Create a Service Account For API Access

Because `aisuite` needs to authenticate with Google Cloud to access the Vertex AI API, you'll need to create a service account and set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to the path of a JSON file containing the service account's credentials, which you can download from the Google Cloud Console.

This is documented [here](https://cloud.google.com/docs/authentication/provide-credentials-adc#how-to), and the basic steps are as follows:

1. Visit the [service accounts page](https://console.cloud.google.com/iam-admin/serviceaccounts) in the Google Cloud Console.
2. Click the "+ Create Service Account" button toward the top of the page.
3. Follow the steps for naming your service account and granting access to the project.
4. Click "Done" to create the service account.
5. Now, click the "Keys" tab towards the top of the page.
6. Click the "Add Key" menu, then select "Create New Key."
6. Choose "JSON" as the key type, and click "Create."
7. Move this file to a location on your file system like your home directory.
8. Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to the path of the JSON file.

## Double check your environment is configured correctly.

At this point, you should have three environment variables set to ensure your environment is set up correctly:

- `GOOGLE_PROJECT_ID`
- `GOOGLE_REGION`
- `GOOGLE_APPLICATION_CREDENTIALS`

Once these are set, you are ready to write some code and send a chat completion request.

## Create a chat completion.

With your account and service account set up, you can send a chat completion request.

Export the environment variables:

```shell
export GOOGLE_PROJECT_ID="your-project-id"
export GOOGLE_REGION="your-region"
export GOOGLE_APPLICATION_CREDENTIALS="path/to/your/service-account-file.json"
```

Install the Vertex AI SDK:

```shell
pip install vertexai
```

In your code:

```python
import aisuite as ai
client = ai.Client()

model="google:gemini-1.5-pro-001"

messages = [
    {"role": "system", "content": "Respond in Pirate English."},
    {"role": "user", "content": "Tell me a joke."},
]

response = client.chat.completions.create(
    model=model,
    messages=messages,
)

print(response.choices[0].message.content)
```

Happy coding! If you would like to contribute, please read our [Contributing Guide](../CONTRIBUTING.md).
</file>

<file path="guides/groq.md">
# Groq

To use Groq with `aisuite`, youll need a free [Groq account](https://console.groq.com/). After logging in, go to the [API Keys](https://console.groq.com/keys) section in your account settings and generate a new Groq API key. Once you have your key, add it to your environment as follows:

```shell
export GROQ_API_KEY="your-groq-api-key"
```

## Create a Python Chat Completion

1. First, install the `groq` Python client library:

```shell
pip install groq
```

2. Now you can simply create your first chat completion with the following example code or customize by swapoping out the `model_id` with any of the other available [models powered by Groq](https://console.groq.com/docs/models) and `messages` array with whatever you'd like:
```python
import aisuite as ai
client = ai.Client()

provider = "groq"
model_id = "llama-3.2-3b-preview"

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Whats the weather like in San Francisco?"},
]

response = client.chat.completions.create(
    model=f"{provider}:{model_id}",
    messages=messages,
)

print(response.choices[0].message.content)
```


Happy coding! If youd like to contribute, please read our [Contributing Guide](CONTRIBUTING.md).
</file>

<file path="guides/huggingface.md">
# Hugging Face AI

To use Hugging Face with the `aisuite` library, you'll need to set up a Hugging Face account, obtain the necessary API credentials, and configure your environment for Hugging Face's API.

## Create a Hugging Face Account and Deploy a Model

1. Visit [Hugging Face](https://huggingface.co/) and sign up for an account if you don't already have one.
2. Explore conversational models on the [Hugging Face Model Hub](https://huggingface.co/models?inference=warm&other=conversational&sort=trending) and select a model you want to use. Popular models include conversational AI models like `gpt2`, `gpt3`, and `mistral`.
3. Deploy or host your chosen model if needed; Hugging Face provides various hosting options, including free, individual, and organizational hosting. Using Serverless Inference API is a fast way to get started.
5. Once the model is deployed (or if using a public model directly), note the model's unique identifier (e.g., `mistralai/Mistral-7B-Instruct-v0.3`), which you'll use for making requests.

## Obtain Necessary Details & Set Environment Variables

After setting up your model, you'll need to gather the following information:

- **API Token**: You can generate an API token in your [Hugging Face account settings](https://huggingface.co/settings/tokens).

Set the following environment variables to make authentication and requests easy:

```shell
export HF_TOKEN="your-api-token"
```

## Create a Chat Completion

With your account set up and environment variables configured, you can send a chat completion request as follows:

```python
import os
import aisuite as ai

# Either set the environment variables or define the parameters below.
# Setting the parameters in ai.Client() will override the environment variable values.
client = ai.Client()

model = "huggingface:your-model-name"  # Replace with your model's identifier.

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What's the weather like today?"},
]

response = client.chat.completions.create(
    model=model,
    messages=messages,
)

print(response.choices[0].message.content)
```

### Notes

- Ensure that the `model` variable matches the identifier of your model as seen in the Hugging Face Model Hub.
- If you encounter any rate limits or API access restrictions, you may have to upgrade your Hugging Face plan to enable higher usage limits.
"""

Happy coding! If you would like to contribute, please read our [Contributing Guide](../CONTRIBUTING.md).
</file>

<file path="guides/mistral.md">
# Mistral

To use Mistral with `aisuite`, youll need a [Mistral account](https://console.mistral.ai/). 

After logging in, go to [Workspace billing](https://console.mistral.ai/billing) and choose a plan
- **Experiment** *(Free, 1 request per second); or*
- **Scale** *(Pay per use).*

Visit the [API Keys](https://console.mistral.ai/api-keys/) section in your account settings and generate a new key. Once you have your key, add it to your environment as follows:

```shell
export MISTRAL="your-mistralai-api-key"
```
## Create a Chat Completion

Install the `mistralai` Python client:

Example with pip:
```shell
pip install mistralai
```

Example with poetry:
```shell
poetry add mistralai
```

In your code:
```python
import aisuite as ai
client = ai.Client()

provider = "mistral"
model_id = "mistral-large-latest"

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Whats the weather like in Montral?"},
]

response = client.chat.completions.create(
    model=f"{provider}:{model_id}",
    messages=messages,
)

print(response.choices[0].message.content)
```

Happy coding! If youd like to contribute, please read our [Contributing Guide](../CONTRIBUTING.md).
</file>

<file path="guides/nebius.md">
# Nebius AI Studio

To use Nebius AI Studio with `aisuite`, you need an AI Studio account. Go to [AI Studio](https://studio.nebius.ai/) and press "Log in to AI Studio" in the right top corner. After logging in, go to the [API Keys](https://studio.nebius.ai/settings/api-keys) section and generate a new key. Once you have a key, add it to your environment as follows:

```shell
export NEBIUS_API_KEY="your-nebius-api-key"
```

## Create a Chat Completion

Install the `openai` Python client:

Example with pip:
```shell
pip install openai
```

Example with poetry:
```shell
poetry add openai
```

In your code:
```python
import aisuite as ai
client = ai.Client()

provider = "nebius"
model_id = "meta-llama/Llama-3.3-70B-Instruct"

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "How many times has Jurgen Klopp won the Champions League?"},
]

response = client.chat.completions.create(
    model=f"{provider}:{model_id}",
    messages=messages,
)

print(response.choices[0].message.content)
```

Happy coding! If youd like to contribute, please read our [Contributing Guide](CONTRIBUTING.md).
</file>

<file path="guides/openai.md">
# OpenAI

To use OpenAI with `aisuite`, youll need an [OpenAI account](https://platform.openai.com/). After logging in, go to the [API Keys](https://platform.openai.com/account/api-keys) section in your account settings and generate a new key. Once you have your key, add it to your environment as follows:

```shell
export OPENAI_API_KEY="your-openai-api-key"
```

## Create a Chat Completion

Install the `openai` Python client:

Example with pip:
```shell
pip install openai
```

Example with poetry:
```shell
poetry add openai
```

In your code:
```python
import aisuite as ai
client = ai.Client()

provider = "openai"
model_id = "gpt-4-turbo"

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Whats the weather like in San Francisco?"},
]

response = client.chat.completions.create(
    model=f"{provider}:{model_id}",
    messages=messages,
)

print(response.choices[0].message.content)
```

Happy coding! If youd like to contribute, please read our [Contributing Guide](../CONTRIBUTING.md).
</file>

<file path="guides/README.md">
# Provider guides 

These guides give directions for obtaining API keys from different providers. 

Here are the instructions for:
- [Anthropic](anthropic.md) 
- [AWS](aws.md)
- [Azure](azure.md) 
- [Cohere](cohere.md)
- [Google](google.md)
- [Hugging Face](huggingface.md)
- [Mistral](mistral.md)
- [OpenAI](openai.md)
- [SambaNova](sambanova.md)
- [xAI](xai.md)
- [DeepSeek](deepseek.md)

Unless otherwise stated, these guides have not been endorsed by the providers. 

We also welcome additional [contributions](../CONTRIBUTING.md).
</file>

<file path="guides/sambanova.md">
# Sambanova

To use Sambanova with `aisuite`, youll need a [Sambanova Cloud](https://cloud.sambanova.ai/) account. After logging in, go to the [API](https://cloud.sambanova.ai/apis) section and generate a new key. Once you have your key, add it to your environment as follows:

```shell
export SAMBANOVA_API_KEY="your-sambanova-api-key"
```

## Create a Chat Completion

Install the `openai` Python client:

Example with pip:
```shell
pip install openai
```

Example with poetry:
```shell
poetry add openai
```

In your code:
```python
import aisuite as ai
client = ai.Client()

provider = "sambanova"
model_id = "Meta-Llama-3.1-405B-Instruct"

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Whats the weather like in San Francisco?"},
]

response = client.chat.completions.create(
    model=f"{provider}:{model_id}",
    messages=messages,
)

print(response.choices[0].message.content)
```

Happy coding! If youd like to contribute, please read our [Contributing Guide](CONTRIBUTING.md).
</file>

<file path="guides/watsonx.md">
# Watsonx with `aisuite`

A a step-by-step guide to set up Watsonx with the `aisuite` library, enabling you to use IBM Watsonx's powerful AI models for various tasks.

## Setup Instructions

### Step 1: Create a Watsonx Account

1. Visit [IBM Watsonx](https://www.ibm.com/watsonx).
2. Sign up for a new account or log in with your existing IBM credentials.
3. Once logged in, navigate to the **Watsonx Dashboard** (<https://dataplatform.cloud.ibm.com>)

---

### Step 2: Obtain API Credentials

1. **Generate an API Key**:
   - Go to IAM > API keys and create a new API key (<https://cloud.ibm.com/iam/overview>)
   - Copy the API key. This is your `WATSONX_API_KEY`.

2. **Locate the Service URL**:
   - Your service URL is based on the region where your service is hosted.
   - Pick one from the list here <https://cloud.ibm.com/apidocs/watsonx-ai#endpoint-url>
   - Copy the service URL. This is your `WATSONX_SERVICE_URL`.

3. **Get the Project ID**:
   - Go to the **Watsonx Dashboard** (<https://dataplatform.cloud.ibm.com>)
   - Under the **Projects** section, If you don't have a sandbox project, create a new project.
   - Navigate to the **Manage** tab and find the **Project ID**.
   - Copy the **Project ID**. This will serve as your `WATSONX_PROJECT_ID`.

---

### Step 3: Set Environment Variables

To simplify authentication, set the following environment variables:

Run the following commands in your terminal:

```bash
export WATSONX_API_KEY="your-watsonx-api-key"
export WATSONX_SERVICE_URL="your-watsonx-service-url"
export WATSONX_PROJECT_ID="your-watsonx-project-id"
```


## Create a Chat Completion

Install the `ibm-watsonx-ai` Python client:

Example with pip:

```shell
pip install ibm-watsonx-ai
```

Example with poetry:

```shell
poetry add ibm-watsonx-ai
```

In your code:

```python
import aisuite as ai
client = ai.Client()

provider = "watsonx"
model_id = "meta-llama/llama-3-70b-instruct"

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Tell me a joke."},
]

response = client.chat.completions.create(
    model=f"{provider}:{model_id}",
    messages=messages,
)

print(response.choices[0].message.content)
```
</file>

<file path="guides/xai.md">
# xAI

To use xAI with `aisuite`, youll need an [API key](https://console.x.ai/). Generate a new key and once you have your key, add it to your environment as follows:

```shell
export XAI_API_KEY="your-xai-api-key"
```

## Create a Chat Completion

Sample code:
```python
import aisuite as ai
client = ai.Client()

models = ["xai:grok-beta"]

messages = [
    {"role": "system", "content": "Respond in Pirate English."},
    {"role": "user", "content": "Tell me a joke."},
]

for model in models:
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.75
    )
    print(response.choices[0].message.content)

```

Happy coding! If youd like to contribute, please read our [Contributing Guide](CONTRIBUTING.md).
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2024 Andrew Ng

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and
associated documentation files (the "Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the
following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial
portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT
LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
</file>

<file path="pyproject.toml">
[tool.poetry]
name = "aisuite"
version = "0.1.11"
description = "Uniform access layer for LLMs"
authors = ["Andrew Ng, Rohit P"]
readme = "README.md"

[tool.poetry.dependencies]
python = "^3.10"
anthropic = { version = "^0.30.1", optional = true }
boto3 = { version = "^1.34.144", optional = true }
cohere = { version = "^5.12.0", optional = true }
vertexai = { version = "^1.63.0", optional = true }
groq = { version = "^0.9.0", optional = true }
mistralai = { version = "^1.0.3", optional = true }
openai = { version = "^1.35.8", optional = true }
ibm-watsonx-ai = { version = "^1.1.16", optional = true }
docstring-parser = { version = "^0.14.0", optional = true }
cerebras_cloud_sdk = { version = "^1.19.0", optional = true }

# Optional dependencies for different providers
httpx = "~0.27.0"
[tool.poetry.extras]
anthropic = ["anthropic"]
aws = ["boto3"]
azure = []
cerebras = ["cerebras_cloud_sdk"]
cohere = ["cohere"]
deepseek = ["openai"]
google = ["vertexai"]
groq = ["groq"]
huggingface = []
mistral = ["mistralai"]
ollama = []
openai = ["openai"]
watsonx = ["ibm-watsonx-ai"]
all = ["anthropic", "aws", "cerebras_cloud_sdk", "google", "groq", "mistral", "openai", "cohere", "watsonx"]  # To install all providers

[tool.poetry.group.dev.dependencies]
pre-commit = "^3.7.1"
black = "^24.4.2"
python-dotenv = "^1.0.1"
openai = "^1.35.8"
groq = "^0.9.0"
anthropic = "^0.30.1"
notebook = "^7.2.1"
ollama = "^0.2.1"
mistralai = "^1.0.3"
boto3 = "^1.34.144"
fireworks-ai = "^0.14.0"
chromadb = "^0.5.4"
sentence-transformers = "^3.0.1"
datasets = "^2.20.0"
vertexai = "^1.63.0"
ibm-watsonx-ai = "^1.1.16"
cerebras_cloud_sdk = "^1.19.0"

[tool.poetry.group.test]
optional = true

[tool.poetry.group.test.dependencies]
pytest = "^8.2.2"
pytest-cov = "^6.0.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.pytest.ini_options]
testpaths="tests"
markers = [
    "integration: marks tests as integration tests that interact with external services",
]
</file>

<file path="README.md">
# aisuite

[![PyPI](https://img.shields.io/pypi/v/aisuite)](https://pypi.org/project/aisuite/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

Simple, unified interface to multiple Generative AI providers.

`aisuite` makes it easy for developers to use multiple LLM through a standardized interface. Using an interface similar to OpenAI's, `aisuite` makes it easy to interact with the most popular LLMs and compare the results. It is a thin wrapper around python client libraries, and allows creators to seamlessly swap out and test responses from different LLM providers without changing their code. Today, the library is primarily focussed on chat completions. We will expand it cover more use cases in near future.

Currently supported providers are:
- Anthropic
- AWS
- Azure
- Cerebras
- Google
- Groq
- HuggingFace Ollama
- Mistral
- OpenAI
- Sambanova
- Watsonx

To maximize stability, `aisuite` uses either the HTTP endpoint or the SDK for making calls to the provider.

## Installation

You can install just the base `aisuite` package, or install a provider's package along with `aisuite`.

This installs just the base package without installing any provider's SDK.

```shell
pip install aisuite
```

This installs aisuite along with anthropic's library.

```shell
pip install 'aisuite[anthropic]'
```

This installs all the provider-specific libraries

```shell
pip install 'aisuite[all]'
```

## Set up

To get started, you will need API Keys for the providers you intend to use. You'll need to
install the provider-specific library either separately or when installing aisuite.

The API Keys can be set as environment variables, or can be passed as config to the aisuite Client constructor.
You can use tools like [`python-dotenv`](https://pypi.org/project/python-dotenv/) or [`direnv`](https://direnv.net/) to set the environment variables manually. Please take a look at the `examples` folder to see usage.

Here is a short example of using `aisuite` to generate chat completion responses from gpt-4o and claude-3-5-sonnet.

Set the API keys.

```shell
export OPENAI_API_KEY="your-openai-api-key"
export ANTHROPIC_API_KEY="your-anthropic-api-key"
```

Use the python client.

```python
import aisuite as ai
client = ai.Client()

models = ["openai:gpt-4o", "anthropic:claude-3-5-sonnet-20240620"]

messages = [
    {"role": "system", "content": "Respond in Pirate English."},
    {"role": "user", "content": "Tell me a joke."},
]

for model in models:
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.75
    )
    print(response.choices[0].message.content)

```

Note that the model name in the create() call uses the format - `<provider>:<model-name>`.
`aisuite` will call the appropriate provider with the right parameters based on the provider value.
For a list of provider values, you can look at the directory - `aisuite/providers/`. The list of supported providers are of the format - `<provider>_provider.py` in that directory. We welcome  providers adding support to this library by adding an implementation file in this directory. Please see section below for how to contribute.

For more examples, check out the `examples` directory where you will find several notebooks that you can run to experiment with the interface.

## Adding support for a provider

We have made easy for a provider or volunteer to add support for a new platform.

### Naming Convention for Provider Modules

We follow a convention-based approach for loading providers, which relies on strict naming conventions for both the module name and the class name. The format is based on the model identifier in the form `provider:model`.

- The provider's module file must be named in the format `<provider>_provider.py`.
- The class inside this module must follow the format: the provider name with the first letter capitalized, followed by the suffix `Provider`.

#### Examples

- **Hugging Face**:
  The provider class should be defined as:

  ```python
  class HuggingfaceProvider(BaseProvider)
  ```

  in providers/huggingface_provider.py.
  
- **OpenAI**:
  The provider class should be defined as:

  ```python
  class OpenaiProvider(BaseProvider)
  ```

  in providers/openai_provider.py

This convention simplifies the addition of new providers and ensures consistency across provider implementations.

## Tool Calling

`aisuite` provides a simple abstraction for tool/function calling that works across supported providers. This is in addition to the regular abstraction of passing JSON spec of the tool to the `tools` parameter. The tool calling abstraction makes it easy to use tools with different LLMs without changing your code.

There are two ways to use tools with `aisuite`:

### 1. Manual Tool Handling

This is the default behavior when `max_turns` is not specified.
You can pass tools in the OpenAI tool format:

```python
def will_it_rain(location: str, time_of_day: str):
    """Check if it will rain in a location at a given time today.
    
    Args:
        location (str): Name of the city
        time_of_day (str): Time of the day in HH:MM format.
    """
    return "YES"

tools = [{
    "type": "function",
    "function": {
        "name": "will_it_rain",
        "description": "Check if it will rain in a location at a given time today",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "Name of the city"
                },
                "time_of_day": {
                    "type": "string",
                    "description": "Time of the day in HH:MM format."
                }
            },
            "required": ["location", "time_of_day"]
        }
    }
}]

response = client.chat.completions.create(
    model="openai:gpt-4o",
    messages=messages,
    tools=tools
)
```

### 2. Automatic Tool Execution

When `max_turns` is specified, you can pass a list of callable Python functions as the `tools` parameter. `aisuite` will automatically handle the tool calling flow:

```python
def will_it_rain(location: str, time_of_day: str):
    """Check if it will rain in a location at a given time today.
    
    Args:
        location (str): Name of the city
        time_of_day (str): Time of the day in HH:MM format.
    """
    return "YES"

client = ai.Client()
messages = [{
    "role": "user",
    "content": "I live in San Francisco. Can you check for weather "
               "and plan an outdoor picnic for me at 2pm?"
}]

# Automatic tool execution with max_turns
response = client.chat.completions.create(
    model="openai:gpt-4o",
    messages=messages,
    tools=[will_it_rain],
    max_turns=2  # Maximum number of back-and-forth tool calls
)
print(response.choices[0].message.content)
```

When `max_turns` is specified, `aisuite` will:
1. Send your message to the LLM
2. Execute any tool calls the LLM requests
3. Send the tool results back to the LLM
4. Repeat until the conversation is complete or max_turns is reached

In addition to `response.choices[0].message`, there is an additional field `response.choices[0].intermediate_messages`: which contains the list of all messages including tool interactions used. This can be used to continue the conversation with the model.
For more detailed examples of tool calling, check out the `examples/tool_calling_abstraction.ipynb` notebook.

## License

aisuite is released under the MIT License. You are free to use, modify, and distribute the code for both commercial and non-commercial purposes.

## Contributing

If you would like to contribute, please read our [Contributing Guide](https://github.com/andrewyng/aisuite/blob/main/CONTRIBUTING.md) and join our [Discord](https://discord.gg/T6Nvn8ExSb) server!
</file>

<file path="tests/client/test_client.py">
from unittest.mock import Mock, patch

import pytest

from aisuite import Client


@pytest.fixture(scope="module")
def provider_configs():
    return {
        "openai": {"api_key": "test_openai_api_key"},
        "aws": {
            "aws_access_key": "test_aws_access_key",
            "aws_secret_key": "test_aws_secret_key",
            "aws_session_token": "test_aws_session_token",
            "aws_region": "us-west-2",
        },
        "azure": {
            "api_key": "azure-api-key",
            "base_url": "https://model.ai.azure.com",
        },
        "groq": {
            "api_key": "groq-api-key",
        },
        "mistral": {
            "api_key": "mistral-api-key",
        },
        "google": {
            "project_id": "test_google_project_id",
            "region": "us-west4",
            "application_credentials": "test_google_application_credentials",
        },
        "fireworks": {
            "api_key": "fireworks-api-key",
        },
        "nebius": {
            "api_key": "nebius-api-key",
        },
    }


@pytest.mark.parametrize(
    argnames=("patch_target", "provider", "model"),
    argvalues=[
        (
            "aisuite.providers.openai_provider.OpenaiProvider.chat_completions_create",
            "openai",
            "gpt-4o",
        ),
        (
            "aisuite.providers.mistral_provider.MistralProvider.chat_completions_create",
            "mistral",
            "mistral-model",
        ),
        (
            "aisuite.providers.groq_provider.GroqProvider.chat_completions_create",
            "groq",
            "groq-model",
        ),
        (
            "aisuite.providers.aws_provider.AwsProvider.chat_completions_create",
            "aws",
            "claude-v3",
        ),
        (
            "aisuite.providers.azure_provider.AzureProvider.chat_completions_create",
            "azure",
            "azure-model",
        ),
        (
            "aisuite.providers.anthropic_provider.AnthropicProvider.chat_completions_create",
            "anthropic",
            "anthropic-model",
        ),
        (
            "aisuite.providers.google_provider.GoogleProvider.chat_completions_create",
            "google",
            "google-model",
        ),
        (
            "aisuite.providers.fireworks_provider.FireworksProvider.chat_completions_create",
            "fireworks",
            "fireworks-model",
        ),
        (
            "aisuite.providers.nebius_provider.NebiusProvider.chat_completions_create",
            "nebius",
            "nebius-model",
        ),
    ],
)
def test_client_chat_completions(
    provider_configs: dict, patch_target: str, provider: str, model: str
):
    expected_response = f"{patch_target}_{provider}_{model}"
    with patch(patch_target) as mock_provider:
        mock_provider.return_value = expected_response
        client = Client()
        client.configure(provider_configs)
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Who won the world series in 2020?"},
        ]

        model_str = f"{provider}:{model}"
        model_response = client.chat.completions.create(model_str, messages=messages)
        assert model_response == expected_response


def test_invalid_provider_in_client_config():
    # Testing an invalid provider name in the configuration
    invalid_provider_configs = {
        "invalid_provider": {"api_key": "invalid_api_key"},
    }

    # Expect ValueError when initializing Client with invalid provider and verify message
    with pytest.raises(
        ValueError,
        match=r"Invalid provider key 'invalid_provider'. Supported providers: ",
    ):
        _ = Client(invalid_provider_configs)


def test_invalid_model_format_in_create(monkeypatch):
    from aisuite.providers.openai_provider import OpenaiProvider

    monkeypatch.setattr(
        target=OpenaiProvider,
        name="chat_completions_create",
        value=Mock(),
    )

    # Valid provider configurations
    provider_configs = {
        "openai": {"api_key": "test_openai_api_key"},
    }

    # Initialize the client with valid provider
    client = Client()
    client.configure(provider_configs)

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me a joke."},
    ]

    # Invalid model format
    invalid_model = "invalidmodel"

    # Expect ValueError when calling create with invalid model format and verify message
    with pytest.raises(
        ValueError, match=r"Invalid model format. Expected 'provider:model'"
    ):
        client.chat.completions.create(invalid_model, messages=messages)
</file>

<file path="tests/client/test_prerelease.py">
# Run this test before releasing a new version.
# It will test all the models in the client.

import pytest
import aisuite as ai
from typing import List, Dict
from dotenv import load_dotenv, find_dotenv


def setup_client() -> ai.Client:
    """Initialize the AI client with environment variables."""
    load_dotenv(find_dotenv())
    return ai.Client()


def get_test_models() -> List[str]:
    """Return a list of model identifiers to test."""
    return [
        "anthropic:claude-3-5-sonnet-20240620",
        "aws:meta.llama3-1-8b-instruct-v1:0",
        "huggingface:mistralai/Mistral-7B-Instruct-v0.3",
        "groq:llama3-8b-8192",
        "mistral:open-mistral-7b",
        "openai:gpt-3.5-turbo",
        "cohere:command-r-plus-08-2024",
    ]


def get_test_messages() -> List[Dict[str, str]]:
    """Return the test messages to send to each model."""
    return [
        {
            "role": "system",
            "content": "Respond in Pirate English. Always try to include the phrase - No rum No fun.",
        },
        {"role": "user", "content": "Tell me a joke about Captain Jack Sparrow"},
    ]


@pytest.mark.integration
@pytest.mark.parametrize("model_id", get_test_models())
def test_model_pirate_response(model_id: str):
    """
    Test that each model responds appropriately to the pirate prompt.

    Args:
        model_id: The provider:model identifier to test
    """
    client = setup_client()
    messages = get_test_messages()

    try:
        response = client.chat.completions.create(
            model=model_id, messages=messages, temperature=0.75
        )

        content = response.choices[0].message.content.lower()

        # Check if either version of the required phrase is present
        assert any(
            phrase in content for phrase in ["no rum no fun", "no rum, no fun"]
        ), f"Model {model_id} did not include required phrase 'No rum No fun'"

        assert len(content) > 0, f"Model {model_id} returned empty response"
        assert isinstance(
            content, str
        ), f"Model {model_id} returned non-string response"

    except Exception as e:
        pytest.fail(f"Error testing model {model_id}: {str(e)}")


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
</file>

<file path="tests/providers/test_anthropic_converter.py">
import unittest
from unittest.mock import MagicMock
from aisuite.providers.anthropic_provider import AnthropicMessageConverter
from aisuite.framework.message import Message, ChatCompletionMessageToolCall, Function
from aisuite.framework import ChatCompletionResponse


class TestAnthropicMessageConverter(unittest.TestCase):

    def setUp(self):
        self.converter = AnthropicMessageConverter()

    def test_convert_request_single_user_message(self):
        messages = [{"role": "user", "content": "Hello, how are you?"}]
        system_message, converted_messages = self.converter.convert_request(messages)

        self.assertEqual(system_message, [])
        self.assertEqual(
            converted_messages, [{"role": "user", "content": "Hello, how are you?"}]
        )

    def test_convert_request_with_system_message(self):
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is the weather?"},
        ]
        system_message, converted_messages = self.converter.convert_request(messages)

        self.assertEqual(system_message, "You are a helpful assistant.")
        self.assertEqual(
            converted_messages, [{"role": "user", "content": "What is the weather?"}]
        )

    def test_convert_request_with_tool_use_message(self):
        messages = [
            {"role": "tool", "tool_call_id": "tool123", "content": "Weather data here."}
        ]
        system_message, converted_messages = self.converter.convert_request(messages)

        self.assertEqual(system_message, [])
        self.assertEqual(
            converted_messages,
            [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "tool_result",
                            "tool_use_id": "tool123",
                            "content": "Weather data here.",
                        }
                    ],
                }
            ],
        )

    def test_convert_response_normal_message(self):
        response = MagicMock()
        response.stop_reason = "end_turn"
        response.usage.input_tokens = 10
        response.usage.output_tokens = 5
        content_mock = MagicMock()
        content_mock.type = "text"
        content_mock.text = "The weather is sunny."
        response.content = [content_mock]

        normalized_response = self.converter.convert_response(response)

        self.assertIsInstance(normalized_response, ChatCompletionResponse)
        self.assertEqual(normalized_response.choices[0].finish_reason, "stop")
        self.assertEqual(
            normalized_response.usage,
            {"prompt_tokens": 10, "completion_tokens": 5, "total_tokens": 15},
        )
        self.assertEqual(
            normalized_response.choices[0].message.content, "The weather is sunny."
        )

    # Test that - when Anthropic returns a tool use message, it is correctly converted.
    def test_convert_response_with_tool_use(self):
        response = MagicMock()
        response.id = "msg_01Aq9w938a90dw8q"
        response.model = "claude-3-5-sonnet-20241022"
        response.role = "assistant"
        response.stop_reason = "tool_use"
        response.usage.input_tokens = 20
        response.usage.output_tokens = 10
        tool_use_mock = MagicMock()
        tool_use_mock.type = "tool_use"
        tool_use_mock.id = "tool123"
        tool_use_mock.name = "get_weather"
        tool_use_mock.input = {"location": "Paris"}

        text_mock = MagicMock()
        text_mock.type = "text"
        text_mock.text = "<thinking>I need to call the get_weather function</thinking>"

        response.content = [tool_use_mock, text_mock]

        normalized_response = self.converter.convert_response(response)

        self.assertIsInstance(normalized_response, ChatCompletionResponse)
        self.assertEqual(normalized_response.choices[0].finish_reason, "tool_calls")
        self.assertEqual(
            normalized_response.usage,
            {"prompt_tokens": 20, "completion_tokens": 10, "total_tokens": 30},
        )
        self.assertEqual(
            normalized_response.choices[0].message.content,
            "<thinking>I need to call the get_weather function</thinking>",
        )
        self.assertEqual(len(normalized_response.choices[0].message.tool_calls), 1)
        self.assertEqual(
            normalized_response.choices[0].message.tool_calls[0].id, "tool123"
        )
        self.assertEqual(
            normalized_response.choices[0].message.tool_calls[0].function.name,
            "get_weather",
        )

    def test_convert_tool_spec(self):
        openai_tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Get the weather.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {"type": "string", "description": "City name."}
                        },
                        "required": ["location"],
                    },
                },
            }
        ]

        anthropic_tools = self.converter.convert_tool_spec(openai_tools)

        self.assertEqual(len(anthropic_tools), 1)
        self.assertEqual(anthropic_tools[0]["name"], "get_weather")
        self.assertEqual(anthropic_tools[0]["description"], "Get the weather.")
        self.assertEqual(
            anthropic_tools[0]["input_schema"],
            {
                "type": "object",
                "properties": {
                    "location": {"type": "string", "description": "City name."}
                },
                "required": ["location"],
            },
        )

    def test_convert_request_with_tool_call_and_result(self):
        messages = [
            {
                "role": "assistant",
                "content": "Let me check the weather.",
                "tool_calls": [
                    {
                        "id": "tool123",
                        "function": {
                            "name": "get_weather",
                            "arguments": '{"location": "San Francisco"}',
                        },
                    }
                ],
            },
            {"role": "tool", "tool_call_id": "tool123", "content": "65 degrees"},
        ]
        system_message, converted_messages = self.converter.convert_request(messages)

        self.assertEqual(system_message, [])
        self.assertEqual(
            converted_messages,
            [
                {
                    "role": "assistant",
                    "content": [
                        {"type": "text", "text": "Let me check the weather."},
                        {
                            "type": "tool_use",
                            "id": "tool123",
                            "name": "get_weather",
                            "input": {"location": "San Francisco"},
                        },
                    ],
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "tool_result",
                            "tool_use_id": "tool123",
                            "content": "65 degrees",
                        }
                    ],
                },
            ],
        )


if __name__ == "__main__":
    unittest.main()
</file>

<file path="tests/providers/test_aws_converter.py">
import unittest
from unittest.mock import MagicMock
from aisuite.providers.aws_provider import BedrockMessageConverter
from aisuite.framework.message import Message, ChatCompletionMessageToolCall
from aisuite.framework import ChatCompletionResponse


class TestBedrockMessageConverter(unittest.TestCase):

    def setUp(self):
        self.converter = BedrockMessageConverter()

    def test_convert_request_user_message(self):
        messages = [
            {"role": "user", "content": "What is the most popular song on WZPZ?"}
        ]
        system_message, formatted_messages = self.converter.convert_request(messages)

        self.assertEqual(system_message, [])
        self.assertEqual(len(formatted_messages), 1)
        self.assertEqual(formatted_messages[0]["role"], "user")
        self.assertEqual(
            formatted_messages[0]["content"],
            [{"text": "What is the most popular song on WZPZ?"}],
        )

    def test_convert_request_tool_result(self):
        messages = [
            {
                "role": "tool",
                "tool_call_id": "tool123",
                "content": '{"song": "Elemental Hotel", "artist": "8 Storey Hike"}',
            }
        ]
        system_message, formatted_messages = self.converter.convert_request(messages)

        self.assertEqual(system_message, [])
        self.assertEqual(len(formatted_messages), 1)
        self.assertEqual(formatted_messages[0]["role"], "user")
        self.assertEqual(
            formatted_messages[0]["content"],
            [
                {
                    "toolResult": {
                        "toolUseId": "tool123",
                        "content": [
                            {
                                "json": {
                                    "song": "Elemental Hotel",
                                    "artist": "8 Storey Hike",
                                }
                            }
                        ],
                    }
                }
            ],
        )

    def test_convert_response_tool_call(self):
        response = {
            "output": {
                "message": {
                    "role": "assistant",
                    "content": [
                        {
                            "toolUse": {
                                "toolUseId": "tool123",
                                "name": "top_song",
                                "input": {"sign": "WZPZ"},
                            }
                        }
                    ],
                }
            },
            "stopReason": "tool_use",
        }

        normalized_response = self.converter.convert_response(response)

        self.assertIsInstance(normalized_response, ChatCompletionResponse)
        self.assertEqual(normalized_response.choices[0].finish_reason, "tool_calls")
        tool_call = normalized_response.choices[0].message.tool_calls[0]
        self.assertEqual(tool_call.function.name, "top_song")
        self.assertEqual(tool_call.function.arguments, '{"sign": "WZPZ"}')

    def test_convert_response_text(self):
        response = {
            "output": {
                "message": {
                    "role": "assistant",
                    "content": [
                        {
                            "text": "The most popular song on WZPZ is Elemental Hotel by 8 Storey Hike."
                        }
                    ],
                }
            },
            "stopReason": "complete",
        }

        normalized_response = self.converter.convert_response(response)

        self.assertIsInstance(normalized_response, ChatCompletionResponse)
        self.assertEqual(normalized_response.choices[0].finish_reason, "stop")
        self.assertEqual(
            normalized_response.choices[0].message.content,
            "The most popular song on WZPZ is Elemental Hotel by 8 Storey Hike.",
        )


if __name__ == "__main__":
    unittest.main()
</file>

<file path="tests/providers/test_azure_provider.py">
import unittest
from aisuite.providers.azure_provider import AzureMessageConverter
from aisuite.framework.message import Message, ChatCompletionMessageToolCall
from aisuite.framework import ChatCompletionResponse


class TestAzureMessageConverter(unittest.TestCase):
    def setUp(self):
        self.converter = AzureMessageConverter()

    def test_convert_request_dict_message(self):
        messages = [{"role": "user", "content": "Hello, how are you?"}]
        converted_messages = self.converter.convert_request(messages)

        self.assertEqual(
            converted_messages, [{"role": "user", "content": "Hello, how are you?"}]
        )

    def test_convert_request_message_object(self):
        message = Message(role="user", content="Hello", tool_calls=None, refusal=None)
        messages = [message]
        converted_messages = self.converter.convert_request(messages)

        expected_message = {
            "role": "user",
            "content": "Hello",
            "reasoning_content": None,
            "tool_calls": None,
            "refusal": None,
        }
        self.assertEqual(converted_messages, [expected_message])

    def test_convert_response_basic(self):
        azure_response = {
            "choices": [
                {
                    "message": {
                        "role": "assistant",
                        "content": "Hello! How can I help you?",
                    }
                }
            ]
        }

        response = self.converter.convert_response(azure_response)

        self.assertIsInstance(response, ChatCompletionResponse)
        self.assertEqual(
            response.choices[0].message.content, "Hello! How can I help you?"
        )
        self.assertEqual(response.choices[0].message.role, "assistant")
        self.assertIsNone(response.choices[0].message.tool_calls)

    def test_convert_response_with_tool_calls(self):
        azure_response = {
            "choices": [
                {
                    "message": {
                        "role": "assistant",
                        "content": "Let me check the weather.",
                        "tool_calls": [
                            {
                                "id": "tool123",
                                "type": "function",
                                "function": {
                                    "name": "get_weather",
                                    "arguments": '{"location": "London"}',
                                },
                            }
                        ],
                    }
                }
            ]
        }

        response = self.converter.convert_response(azure_response)

        self.assertIsInstance(response, ChatCompletionResponse)
        self.assertEqual(
            response.choices[0].message.content, "Let me check the weather."
        )
        self.assertEqual(len(response.choices[0].message.tool_calls), 1)

        tool_call = response.choices[0].message.tool_calls[0]
        self.assertEqual(tool_call.id, "tool123")
        self.assertEqual(tool_call.type, "function")
        self.assertEqual(tool_call.function.name, "get_weather")
        self.assertEqual(tool_call.function.arguments, '{"location": "London"}')


if __name__ == "__main__":
    unittest.main()
</file>

<file path="tests/providers/test_cerebras_provider.py">
from unittest.mock import MagicMock, patch

import pytest

from aisuite.providers.cerebras_provider import CerebrasProvider


@pytest.fixture(autouse=True)
def set_api_key_env_var(monkeypatch):
    """Fixture to set environment variables for tests."""
    monkeypatch.setenv("CEREBRAS_API_KEY", "test-api-key")


def test_cerebras_provider():
    """High-level test that the provider is initialized and chat completions are requested successfully."""

    user_greeting = "Hello!"
    message_history = [{"role": "user", "content": user_greeting}]
    selected_model = "our-favorite-model"
    chosen_temperature = 0.75
    response_text_content = "mocked-text-response-from-model"

    provider = CerebrasProvider()
    mock_response = MagicMock()
    mock_response.model_dump.return_value = {
        "choices": [{"message": {"content": response_text_content}}]
    }

    with patch.object(
        provider.client.chat.completions,
        "create",
        return_value=mock_response,
    ) as mock_create:
        response = provider.chat_completions_create(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        mock_create.assert_called_with(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        assert response.choices[0].message.content == response_text_content
</file>

<file path="tests/providers/test_cohere_provider.py">
from unittest.mock import MagicMock, patch

import pytest

from aisuite.providers.cohere_provider import CohereProvider


@pytest.fixture(autouse=True)
def set_api_key_env_var(monkeypatch):
    """Fixture to set environment variables for tests."""
    monkeypatch.setenv("CO_API_KEY", "test-api-key")


def test_cohere_provider():
    """High-level test that the provider is initialized and chat completions are requested successfully."""

    user_greeting = "Hello!"
    message_history = [{"role": "user", "content": user_greeting}]
    selected_model = "our-favorite-model"
    chosen_temperature = 0.75
    response_text_content = "mocked-text-response-from-model"

    provider = CohereProvider()
    mock_response = MagicMock()
    mock_response.message = MagicMock()
    mock_response.message.content = [MagicMock()]
    mock_response.message.content[0].text = response_text_content

    with patch.object(
        provider.client,
        "chat",
        return_value=mock_response,
    ) as mock_create:
        response = provider.chat_completions_create(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        mock_create.assert_called_with(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        assert response.choices[0].message.content == response_text_content
</file>

<file path="tests/providers/test_deepseek_provider.py">
from unittest.mock import MagicMock, patch

import pytest

from aisuite.providers.deepseek_provider import DeepseekProvider


@pytest.fixture(autouse=True)
def set_api_key_env_var(monkeypatch):
    """Fixture to set environment variables for tests."""
    monkeypatch.setenv("DEEPSEEK_API_KEY", "test-api-key")


def test_groq_provider():
    """High-level test that the provider is initialized and chat completions are requested successfully."""

    user_greeting = "Hello!"
    message_history = [{"role": "user", "content": user_greeting}]
    selected_model = "our-favorite-model"
    chosen_temperature = 0.75
    response_text_content = "mocked-text-response-from-model"

    provider = DeepseekProvider()
    mock_response = MagicMock()
    mock_response.choices = [MagicMock()]
    mock_response.choices[0].message = MagicMock()
    mock_response.choices[0].message.content = response_text_content

    with patch.object(
        provider.client.chat.completions,
        "create",
        return_value=mock_response,
    ) as mock_create:
        response = provider.chat_completions_create(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        mock_create.assert_called_with(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        assert response.choices[0].message.content == response_text_content
</file>

<file path="tests/providers/test_google_converter.py">
import unittest
from unittest.mock import MagicMock
from aisuite.providers.google_provider import GoogleMessageConverter
from aisuite.framework.message import Message, ChatCompletionMessageToolCall, Function
from aisuite.framework import ChatCompletionResponse


class TestGoogleMessageConverter(unittest.TestCase):

    def setUp(self):
        self.converter = GoogleMessageConverter()

    def test_convert_request_user_message(self):
        messages = [{"role": "user", "content": "What is the weather today?"}]
        converted_messages = self.converter.convert_request(messages)

        self.assertEqual(len(converted_messages), 1)
        self.assertEqual(converted_messages[0].role, "user")
        self.assertEqual(
            converted_messages[0].parts[0].text, "What is the weather today?"
        )

    def test_convert_request_tool_result_message(self):
        messages = [
            {
                "role": "tool",
                "name": "get_weather",
                "content": '{"temperature": "15", "unit": "Celsius"}',
            }
        ]
        converted_messages = self.converter.convert_request(messages)

        self.assertEqual(len(converted_messages), 1)
        self.assertEqual(converted_messages[0].function_response.name, "get_weather")
        self.assertEqual(
            converted_messages[0].function_response.response,
            {"temperature": "15", "unit": "Celsius"},
        )

    def test_convert_request_assistant_message(self):
        messages = [
            {
                "role": "assistant",
                "content": "The weather is sunny with a temperature of 25 degrees Celsius.",
            }
        ]
        converted_messages = self.converter.convert_request(messages)

        self.assertEqual(len(converted_messages), 1)
        self.assertEqual(converted_messages[0].role, "model")
        self.assertEqual(
            converted_messages[0].parts[0].text,
            "The weather is sunny with a temperature of 25 degrees Celsius.",
        )

    def test_convert_response_with_function_call(self):
        function_call_mock = MagicMock()
        function_call_mock.name = "get_exchange_rate"
        function_call_mock.args = {
            "currency_from": "AUD",
            "currency_to": "SEK",
            "currency_date": "latest",
        }

        response = MagicMock()
        response.candidates = [
            MagicMock(
                content=MagicMock(parts=[MagicMock(function_call=function_call_mock)]),
                finish_reason="function_call",
            )
        ]

        normalized_response = self.converter.convert_response(response)

        self.assertIsInstance(normalized_response, ChatCompletionResponse)
        self.assertEqual(normalized_response.choices[0].finish_reason, "tool_calls")
        self.assertEqual(
            normalized_response.choices[0].message.tool_calls[0].function.name,
            "get_exchange_rate",
        )
        self.assertEqual(
            normalized_response.choices[0].message.tool_calls[0].function.arguments,
            '{"currency_from": "AUD", "currency_to": "SEK", "currency_date": "latest"}',
        )

    def test_convert_response_with_text(self):
        response = MagicMock()
        text_content = "The current exchange rate is 7.50 SEK per AUD."

        mock_part = MagicMock()
        mock_part.text = text_content
        mock_part.function_call = None

        mock_content = MagicMock()
        mock_content.parts = [mock_part]

        mock_candidate = MagicMock()
        mock_candidate.content = mock_content
        mock_candidate.finish_reason = "stop"

        response.candidates = [mock_candidate]

        normalized_response = self.converter.convert_response(response)

        self.assertIsInstance(normalized_response, ChatCompletionResponse)
        self.assertEqual(normalized_response.choices[0].finish_reason, "stop")
        self.assertEqual(normalized_response.choices[0].message.content, text_content)


if __name__ == "__main__":
    unittest.main()
</file>

<file path="tests/providers/test_google_provider.py">
import pytest
from unittest.mock import patch, MagicMock
from aisuite.providers.google_provider import GoogleProvider
from vertexai.generative_models import Content, Part
import json


@pytest.fixture(autouse=True)
def set_api_key_env_var(monkeypatch):
    """Fixture to set environment variables for tests."""
    monkeypatch.setenv("GOOGLE_APPLICATION_CREDENTIALS", "path-to-service-account-json")
    monkeypatch.setenv("GOOGLE_PROJECT_ID", "vertex-project-id")
    monkeypatch.setenv("GOOGLE_REGION", "us-central1")


def test_missing_env_vars():
    """Test that an error is raised if required environment variables are missing."""
    with patch.dict("os.environ", {}, clear=True):
        with pytest.raises(EnvironmentError) as exc_info:
            GoogleProvider()
        assert "Missing one or more required Google environment variables" in str(
            exc_info.value
        )


def test_vertex_interface():
    """High-level test that the interface is initialized and chat completions are requested successfully."""

    # Test case 1: Regular text response
    def test_text_response():
        user_greeting = "Hello!"
        message_history = [{"role": "user", "content": user_greeting}]
        selected_model = "our-favorite-model"
        response_text_content = "mocked-text-response-from-model"

        interface = GoogleProvider()
        mock_response = MagicMock()
        mock_response.candidates = [MagicMock()]
        mock_response.candidates[0].content.parts = [MagicMock()]
        mock_response.candidates[0].content.parts[0].text = response_text_content
        # Ensure function_call attribute doesn't exist
        del mock_response.candidates[0].content.parts[0].function_call

        with patch(
            "aisuite.providers.google_provider.GenerativeModel"
        ) as mock_generative_model:
            mock_model = MagicMock()
            mock_generative_model.return_value = mock_model
            mock_chat = MagicMock()
            mock_model.start_chat.return_value = mock_chat
            mock_chat.send_message.return_value = mock_response

            response = interface.chat_completions_create(
                messages=message_history,
                model=selected_model,
                temperature=0.7,
            )

            # Assert the response is in the correct format
            assert response.choices[0].message.content == response_text_content
            assert response.choices[0].finish_reason == "stop"

    # Test case 2: Function call response
    def test_function_call():
        user_greeting = "What's the weather?"
        message_history = [{"role": "user", "content": user_greeting}]
        selected_model = "our-favorite-model"

        interface = GoogleProvider()
        mock_response = MagicMock()
        mock_response.candidates = [MagicMock()]
        mock_response.candidates[0].content.parts = [MagicMock()]

        # Mock the function call response
        function_call_mock = MagicMock()
        function_call_mock.name = "get_weather"
        function_call_mock.args = {"location": "San Francisco"}
        mock_response.candidates[0].content.parts[0].function_call = function_call_mock
        mock_response.candidates[0].content.parts[0].text = None

        with patch(
            "aisuite.providers.google_provider.GenerativeModel"
        ) as mock_generative_model:
            mock_model = MagicMock()
            mock_generative_model.return_value = mock_model
            mock_chat = MagicMock()
            mock_model.start_chat.return_value = mock_chat
            mock_chat.send_message.return_value = mock_response

            response = interface.chat_completions_create(
                messages=message_history,
                model=selected_model,
                temperature=0.7,
            )

            # Assert the response contains the function call
            assert response.choices[0].message.content is None
            assert response.choices[0].message.tool_calls[0].type == "function"
            assert (
                response.choices[0].message.tool_calls[0].function.name == "get_weather"
            )
            assert json.loads(
                response.choices[0].message.tool_calls[0].function.arguments
            ) == {"location": "San Francisco"}
            assert response.choices[0].finish_reason == "tool_calls"

    # Run both test cases
    test_text_response()
    test_function_call()


def test_convert_openai_to_vertex_ai():
    """Test the message conversion from OpenAI format to Vertex AI format."""
    interface = GoogleProvider()
    message = {"role": "user", "content": "Hello!"}

    # Use the transformer to convert the message
    result = interface.transformer.convert_request([message])

    # Verify the conversion result
    assert len(result) == 1
    assert isinstance(result[0], Content)
    assert result[0].role == "user"
    assert len(result[0].parts) == 1
    assert isinstance(result[0].parts[0], Part)
    assert result[0].parts[0].text == "Hello!"


def test_role_conversions():
    """Test that different message roles are converted correctly."""
    interface = GoogleProvider()

    messages = [
        {"role": "system", "content": "System message"},
        {"role": "user", "content": "User message"},
        {"role": "assistant", "content": "Assistant message"},
    ]

    result = interface.transformer.convert_request(messages)

    # System and user messages should both be converted to "user" role in Vertex AI
    assert len(result) == 3
    assert result[0].role == "user"  # system converted to user
    assert result[0].parts[0].text == "System message"

    assert result[1].role == "user"
    assert result[1].parts[0].text == "User message"

    assert result[2].role == "model"  # assistant converted to model
    assert result[2].parts[0].text == "Assistant message"
</file>

<file path="tests/providers/test_groq_provider.py">
from unittest.mock import MagicMock, patch

import pytest

from aisuite.providers.groq_provider import GroqProvider


@pytest.fixture(autouse=True)
def set_api_key_env_var(monkeypatch):
    """Fixture to set environment variables for tests."""
    monkeypatch.setenv("GROQ_API_KEY", "test-api-key")


def test_groq_provider():
    """High-level test that the provider is initialized and chat completions are requested successfully."""

    user_greeting = "Hello!"
    message_history = [{"role": "user", "content": user_greeting}]
    selected_model = "our-favorite-model"
    chosen_temperature = 0.75
    response_text_content = "mocked-text-response-from-model"

    provider = GroqProvider()
    mock_response = MagicMock()
    mock_response.model_dump.return_value = {
        "choices": [{"message": {"content": response_text_content}}]
    }

    with patch.object(
        provider.client.chat.completions,
        "create",
        return_value=mock_response,
    ) as mock_create:
        response = provider.chat_completions_create(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        mock_create.assert_called_with(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        assert response.choices[0].message.content == response_text_content
</file>

<file path="tests/providers/test_mistral_provider.py">
import pytest
from unittest.mock import patch, MagicMock

from aisuite.providers.mistral_provider import MistralProvider


@pytest.fixture(autouse=True)
def set_api_key_env_var(monkeypatch):
    """Fixture to set environment variables for tests."""
    monkeypatch.setenv("MISTRAL_API_KEY", "test-api-key")


def test_mistral_provider():
    """High-level test that the provider is initialized and chat completions are requested successfully."""

    user_greeting = "Hello!"
    message_history = [{"role": "user", "content": user_greeting}]
    selected_model = "our-favorite-model"
    chosen_temperature = 0.75
    response_text_content = "mocked-text-response-from-model"

    provider = MistralProvider()
    mock_response = MagicMock()
    mock_response.model_dump.return_value = {
        "choices": [{"message": {"content": response_text_content}}]
    }

    with patch.object(
        provider.client.chat, "complete", return_value=mock_response
    ) as mock_create:
        response = provider.chat_completions_create(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        mock_create.assert_called_with(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        assert response.choices[0].message.content == response_text_content
</file>

<file path="tests/providers/test_nebius_provider.py">
import pytest
from unittest.mock import patch, MagicMock

from aisuite.providers.nebius_provider import NebiusProvider


@pytest.fixture(autouse=True)
def set_api_key_env_var(monkeypatch):
    """Fixture to set environment variables for tests."""
    monkeypatch.setenv("NEBIUS_API_KEY", "test-api-key")


def test_nebius_provider():
    """High-level test that the provider is initialized and chat completions are requested successfully."""

    user_greeting = "Hello!"
    message_history = [{"role": "user", "content": user_greeting}]
    selected_model = "our-favorite-model"
    chosen_temperature = 0.75
    response_text_content = "mocked-text-response-from-model"

    provider = NebiusProvider()
    mock_response = MagicMock()
    mock_response.choices = [MagicMock()]
    mock_response.choices[0].message = MagicMock()
    mock_response.choices[0].message.content = response_text_content

    with patch.object(
        provider.client.chat.completions,
        "create",
        return_value=mock_response,
    ) as mock_create:
        response = provider.chat_completions_create(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        mock_create.assert_called_with(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        assert response.choices[0].message.content == response_text_content
</file>

<file path="tests/providers/test_ollama_provider.py">
import pytest
from unittest.mock import patch, MagicMock
from aisuite.providers.ollama_provider import OllamaProvider


@pytest.fixture(autouse=True)
def set_api_url_var(monkeypatch):
    """Fixture to set environment variables for tests."""
    monkeypatch.setenv("OLLAMA_API_URL", "http://localhost:11434")


def test_completion():
    """Test that completions request successfully."""

    user_greeting = "Howdy!"
    message_history = [{"role": "user", "content": user_greeting}]
    selected_model = "best-model-ever"
    chosen_temperature = 0.77
    response_text_content = "mocked-text-response-from-ollama-model"

    ollama = OllamaProvider()
    mock_response = {"message": {"content": response_text_content}}

    with patch(
        "httpx.post",
        return_value=MagicMock(status_code=200, json=lambda: mock_response),
    ) as mock_post:
        response = ollama.chat_completions_create(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        mock_post.assert_called_once_with(
            "http://localhost:11434/api/chat",
            json={
                "model": selected_model,
                "messages": message_history,
                "stream": False,
                "temperature": chosen_temperature,
            },
            timeout=30,
        )

        assert response.choices[0].message.content == response_text_content
</file>

<file path="tests/providers/test_sambanova_provider.py">
from unittest.mock import MagicMock, patch

import pytest

from aisuite.providers.sambanova_provider import SambanovaProvider


@pytest.fixture(autouse=True)
def set_api_key_env_var(monkeypatch):
    """Fixture to set environment variables for tests."""
    monkeypatch.setenv("SAMBANOVA_API_KEY", "test-api-key")


def test_sambanova_provider():
    """High-level test that the provider is initialized and chat completions are requested successfully."""

    user_greeting = "Hello!"
    message_history = [{"role": "user", "content": user_greeting}]
    selected_model = "our-favorite-model"
    chosen_temperature = 0.75
    response_text_content = "mocked-text-response-from-model"

    provider = SambanovaProvider()
    mock_response = MagicMock()
    mock_response.model_dump.return_value = {
        "choices": [
            {"message": {"content": response_text_content, "role": "assistant"}}
        ]
    }

    with patch.object(
        provider.client.chat.completions,
        "create",
        return_value=mock_response,
    ) as mock_create:
        response = provider.chat_completions_create(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        mock_create.assert_called_with(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        assert response.choices[0].message.content == response_text_content
</file>

<file path="tests/providers/test_watsonx_provider.py">
from unittest.mock import MagicMock, patch

import pytest

try:
    from ibm_watsonx_ai.metanames import GenChatParamsMetaNames as GenChatParams
except Exception as e:
    pytest.skip(f"Skipping test due to import error: {e}", allow_module_level=True)

from aisuite.providers.watsonx_provider import WatsonxProvider


@pytest.fixture(autouse=True)
def set_api_key_env_var(monkeypatch):
    """Fixture to set environment variables for tests."""
    monkeypatch.setenv("WATSONX_SERVICE_URL", "https://watsonx-service-url.com")
    monkeypatch.setenv("WATSONX_API_KEY", "test-api-key")
    monkeypatch.setenv("WATSONX_PROJECT_ID", "test-project-id")


@pytest.mark.skip(reason="Skipping due to version compatibility issue on python 3.11")
def test_watsonx_provider():
    """High-level test that the provider is initialized and chat completions are requested successfully."""

    user_greeting = "Hello!"
    message_history = [{"role": "user", "content": user_greeting}]
    selected_model = "our-favorite-model"
    chosen_temperature = 0.7
    response_text_content = "mocked-text-response-from-model"

    provider = WatsonxProvider()
    mock_response = {"choices": [{"message": {"content": response_text_content}}]}

    with patch(
        "aisuite.providers.watsonx_provider.ModelInference"
    ) as mock_model_inference:
        mock_model = MagicMock()
        mock_model_inference.return_value = mock_model
        mock_model.chat.return_value = mock_response

        response = provider.chat_completions_create(
            messages=message_history,
            model=selected_model,
            temperature=chosen_temperature,
        )

        # Assert that ModelInference was called with correct arguments.
        mock_model_inference.assert_called_once()
        args, kwargs = mock_model_inference.call_args
        assert kwargs["model_id"] == selected_model
        assert kwargs["project_id"] == provider.project_id

        # Assert that the credentials have the correct API key and service URL.
        credentials = kwargs["credentials"]
        assert credentials.api_key == provider.api_key
        assert credentials.url == provider.service_url

        # Assert that chat was called with correct history and params
        mock_model.chat.assert_called_once_with(
            messages=message_history,
            params={GenChatParams.TEMPERATURE: chosen_temperature},
        )

        assert response.choices[0].message.content == response_text_content
</file>

<file path="tests/utils/test_tool_manager.py">
import unittest
from pydantic import BaseModel
from typing import Dict
from aisuite.utils.tools import Tools  # Import your ToolManager class
from enum import Enum


# Define a sample tool function and Pydantic model for testing
class TemperatureUnit(str, Enum):
    CELSIUS = "Celsius"
    FAHRENHEIT = "Fahrenheit"


class TemperatureParamsV2(BaseModel):
    location: str
    unit: TemperatureUnit = TemperatureUnit.CELSIUS


class TemperatureParams(BaseModel):
    location: str
    unit: str = "Celsius"


def get_current_temperature(location: str, unit: str = "Celsius") -> Dict[str, str]:
    """Gets the current temperature for a specific location and unit."""
    return {"location": location, "unit": unit, "temperature": "72"}


def missing_annotation_tool(location, unit="Celsius"):
    """Tool function without type annotations."""
    return {"location": location, "unit": unit, "temperature": "72"}


def get_current_temperature_v2(
    location: str, unit: TemperatureUnit = TemperatureUnit.CELSIUS
) -> Dict[str, str]:
    """Gets the current temperature for a specific location and unit (with enum support)."""
    return {"location": location, "unit": unit, "temperature": "72"}


class TestToolManager(unittest.TestCase):
    def setUp(self):
        self.tool_manager = Tools()

    def test_add_tool_with_pydantic_model(self):
        """Test adding a tool with an explicit Pydantic model."""
        self.tool_manager._add_tool(get_current_temperature, TemperatureParams)

        expected_tool_spec = [
            {
                "type": "function",
                "function": {
                    "name": "get_current_temperature",
                    "description": "Gets the current temperature for a specific location and unit.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {
                                "type": "string",
                                "description": "",
                            },
                            "unit": {
                                "type": "string",
                                "description": "",
                                "default": "Celsius",
                            },
                        },
                        "required": ["location"],
                    },
                },
            }
        ]

        tools = self.tool_manager.tools()
        self.assertIn(
            "get_current_temperature", [tool["function"]["name"] for tool in tools]
        )
        assert (
            tools == expected_tool_spec
        ), f"Expected {expected_tool_spec}, but got {tools}"

    def test_add_tool_with_signature_inference(self):
        """Test adding a tool and inferring parameters from the function signature."""
        self.tool_manager._add_tool(get_current_temperature)
        # Expected output from tool_manager.tools() when called with OpenAI format
        expected_tool_spec = [
            {
                "type": "function",
                "function": {
                    "name": "get_current_temperature",
                    "description": "Gets the current temperature for a specific location and unit.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {
                                "type": "string",
                                "description": "",  # No description provided in function signature
                            },
                            "unit": {
                                "type": "string",
                                "description": "",
                                "default": "Celsius",
                            },
                        },
                        "required": ["location"],
                    },
                },
            }
        ]
        tools = self.tool_manager.tools()
        print(tools)
        self.assertIn(
            "get_current_temperature", [tool["function"]["name"] for tool in tools]
        )
        assert (
            tools == expected_tool_spec
        ), f"Expected {expected_tool_spec}, but got {tools}"

    def test_add_tool_missing_annotation_raises_exception(self):
        """Test that adding a tool with missing type annotations raises a TypeError."""
        with self.assertRaises(TypeError):
            self.tool_manager._add_tool(missing_annotation_tool)

    def test_execute_tool_valid_parameters(self):
        """Test executing a registered tool with valid parameters."""
        self.tool_manager._add_tool(get_current_temperature, TemperatureParams)
        tool_call = {
            "id": "call_1",
            "function": {
                "name": "get_current_temperature",
                "arguments": {"location": "San Francisco", "unit": "Celsius"},
            },
        }
        result, result_message = self.tool_manager.execute_tool(tool_call)

        # Assuming result is returned as a list with a single dictionary
        result_dict = result[0] if isinstance(result, list) else result

        # Check that the result matches expected output
        self.assertEqual(result_dict["location"], "San Francisco")
        self.assertEqual(result_dict["unit"], "Celsius")
        self.assertEqual(result_dict["temperature"], "72")

    def test_execute_tool_invalid_parameters(self):
        """Test that executing a tool with invalid parameters raises a ValueError."""
        self.tool_manager._add_tool(get_current_temperature, TemperatureParams)
        tool_call = {
            "id": "call_1",
            "function": {
                "name": "get_current_temperature",
                "arguments": {"location": 123},  # Invalid type for location
            },
        }

        with self.assertRaises(ValueError) as context:
            self.tool_manager.execute_tool(tool_call)

        # Verify the error message contains information about the validation error
        self.assertIn(
            "Error in tool 'get_current_temperature' parameters", str(context.exception)
        )

    def test_add_tool_with_enum(self):
        """Test adding a tool with an enum parameter."""
        self.tool_manager._add_tool(get_current_temperature_v2, TemperatureParamsV2)

        expected_tool_spec = [
            {
                "type": "function",
                "function": {
                    "name": "get_current_temperature_v2",
                    "description": "Gets the current temperature for a specific location and unit (with enum support).",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {
                                "type": "string",
                                "description": "",
                            },
                            "unit": {
                                "type": "string",
                                "enum": ["Celsius", "Fahrenheit"],
                                "description": "",
                                "default": "Celsius",
                            },
                        },
                        "required": ["location"],
                    },
                },
            }
        ]

        tools = self.tool_manager.tools()
        assert (
            tools == expected_tool_spec
        ), f"Expected {expected_tool_spec}, but got {tools}"


if __name__ == "__main__":
    unittest.main()
</file>

</files>
